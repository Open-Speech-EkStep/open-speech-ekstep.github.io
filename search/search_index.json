{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Vakyansh \u00b6 Recipies to build Speech Recognition models Introduction \u00b6 Vakyansh aims to host the key essentials of Automatic Speech Recognition (ASR) technology, focusing on Indian languages. It is a resource that allows people to build applications that leverage speech recognition. The site will host open data for training ASR models, open source utilities and pipelines to train ASR models and open ASR models themselves. Vakyansh also hosts tools to contribute your voices to create a diverse open data repository of Indian voices to this end. This data will be available in an open manner for all to use. It is a resource that allows people to build applications that leverage speech recognition. Open Source \u00b6 Open sourcing the speech recognition technology empowers us to bring our languages to the core of our fundamentals. Vakyansh aims to open source the speech recognition models in various languages, the datasets collected through various channels and the linguistic utilities developed to process and clean the data and make it usable by speech recognition tools. The open source strategy will enable the various language communities, individuals and technologists, who are passionate about their language, to develop speech recognition applications, and integrate them in various domains for the good of the community. Getting started \u00b6 Vakyansh's developer documentation is meant for its adopters, developers and contributors. It would enable people to innovate and improve and build Speech Recognition Models in any language. Modeling Process \u00b6 Vakyansh Components \u00b6 1. Data Collection Pipeline \u00b6 Pipelines to collect data in automated way for the language you want 2. Crowdsourcing Platform \u00b6 Platform to record and validate voice data from various speakers. 3. Intelligent Data Pipeline \u00b6 Pipelines to transform raw data and prepare data for model training. They clean, process and balance data for model training 4. Model Training Pipeline \u00b6 Pipeline to build state of the art Speech Recognition Model using the data provided by Intelligent Data Pipeline Vakyansh Technical Overview \u00b6 The Developer documentation provides you with a complete set of guidelines which you need to: Install Vakyansh Configure Vakyansh Customize Vakyansh Extend Vakyansh Contribute to Vakyansh Discussion \u00b6 Connect with community on Gitter","title":"Home"},{"location":"#welcome-to-vakyansh","text":"Recipies to build Speech Recognition models","title":"Welcome to Vakyansh"},{"location":"#introduction","text":"Vakyansh aims to host the key essentials of Automatic Speech Recognition (ASR) technology, focusing on Indian languages. It is a resource that allows people to build applications that leverage speech recognition. The site will host open data for training ASR models, open source utilities and pipelines to train ASR models and open ASR models themselves. Vakyansh also hosts tools to contribute your voices to create a diverse open data repository of Indian voices to this end. This data will be available in an open manner for all to use. It is a resource that allows people to build applications that leverage speech recognition.","title":"Introduction"},{"location":"#open-source","text":"Open sourcing the speech recognition technology empowers us to bring our languages to the core of our fundamentals. Vakyansh aims to open source the speech recognition models in various languages, the datasets collected through various channels and the linguistic utilities developed to process and clean the data and make it usable by speech recognition tools. The open source strategy will enable the various language communities, individuals and technologists, who are passionate about their language, to develop speech recognition applications, and integrate them in various domains for the good of the community.","title":"Open Source"},{"location":"#getting-started","text":"Vakyansh's developer documentation is meant for its adopters, developers and contributors. It would enable people to innovate and improve and build Speech Recognition Models in any language.","title":"Getting started"},{"location":"#modeling-process","text":"","title":"Modeling Process"},{"location":"#vakyansh-components","text":"","title":"Vakyansh Components"},{"location":"#1-data-collection-pipeline","text":"Pipelines to collect data in automated way for the language you want","title":"1. Data Collection Pipeline"},{"location":"#2-crowdsourcing-platform","text":"Platform to record and validate voice data from various speakers.","title":"2. Crowdsourcing Platform"},{"location":"#3-intelligent-data-pipeline","text":"Pipelines to transform raw data and prepare data for model training. They clean, process and balance data for model training","title":"3. Intelligent Data Pipeline"},{"location":"#4-model-training-pipeline","text":"Pipeline to build state of the art Speech Recognition Model using the data provided by Intelligent Data Pipeline","title":"4. Model Training Pipeline"},{"location":"#vakyansh-technical-overview","text":"The Developer documentation provides you with a complete set of guidelines which you need to: Install Vakyansh Configure Vakyansh Customize Vakyansh Extend Vakyansh Contribute to Vakyansh","title":"Vakyansh Technical Overview"},{"location":"#discussion","text":"Connect with community on Gitter","title":"Discussion"},{"location":"CONTRIBUTING/","text":"Contributing to this project \u00b6 Contributors Agreement Introduction I don't want to read this whole thing I just have a question!!! How Can I Contribute? Reporting Bugs Suggesting Enhancements Creating Pull Requests Contributors Agreement \u00b6 By submitting patches to this project you agree to allow them to be redistributed under the project's license, according to the normal forms and usages of the open-source community. Introduction \u00b6 First off, thank you for considering contributing to this project. It's people like you that make it such a great tool. Following these guidelines helps to communicate that you respect the time of the developers managing and developing this open source project. In return, they should reciprocate that respect in addressing your issue, assessing changes, and helping you finalize your pull requests. This is an open source project and we love to receive contributions from our community \u2014 you! There are many ways to contribute, from writing tutorials or blog posts, improving the documentation, submitting bug reports and feature requests or writing code which can be incorporated into the main project itself. I don't want to read this whole thing I just have a question!!! \u00b6 We currently allow our users to use the issue tracker for support questions. But please be wary that maintaining an open source project can take a lot of time from the maintainers. If asking for a support question, state it clearly and take the time to explain your problem properly. Also, if your problem is not strictly related to this project we recommend you to use Stack Overlow instead. How Can I Contribute? \u00b6 Reporting Bugs \u00b6 Before creating bug reports, please check the existing bug reports as you might find out that you don't need to create one. When you are creating a bug report, please include as many details as possible. How Do I Submit A (Good) Bug Report? \u00b6 Bugs are tracked as GitHub issues . Create an issue on the project's repository and provide the following information. Explain the problem and include additional details to help maintainers reproduce the problem: Use a clear and descriptive title for the issue to identify the problem. Describe the exact steps which reproduce the problem in as many details as possible. For example, start by explaining how you used the project. When listing steps, don't just say what you did, but explain how you did it . Provide specific examples to demonstrate the steps . It's always better to get more information. You can include links to files or GitHub projects, copy/pasteable snippets or even print screens or animated GIFS. If you're providing snippets in the issue, use Markdown code blocks . Describe the behavior you observed after following the steps and point out what exactly is the problem with that behavior. Explain which behavior you expected to see instead and why. If the problem wasn't triggered by a specific action , describe what you were doing before the problem happened and share more information using the guidelines below. Provide more context by answering these questions: Did the problem start happening recently (e.g. after updating to a new version) or was this always a problem? If the problem started happening recently, can you reproduce the problem in an older version? What's the most recent version in which the problem doesn't happen? Can you reliably reproduce the issue? If not, provide details about how often the problem happens and under which conditions it normally happens. Include details about your configuration and environment: Which version of the project are you using? What's the name and version of the OS you're using ? Any other information that could be useful about you environment Suggesting Enhancements \u00b6 This section guides you through submitting an enhancement suggestion for this project, including completely new features and minor improvements to existing functionality. Following these guidelines helps maintainers and the community understand your suggestion and find related suggestions. Before creating enhancement suggestions, please check the list of enhancements suggestions in the issue tracker as you might find out that you don't need to create one. When you are creating an enhancement suggestion, please include as many details as possible. How Do I Submit A (Good) Enhancement Suggestion? \u00b6 Enhancement suggestions are tracked as GitHub issues . Create an issue on the project's repository and provide the following information: Use a clear and descriptive title for the issue to identify the suggestion. Provide a step-by-step description of the suggested enhancement in as many details as possible. Provide specific examples to demonstrate the steps . It's always better to get more information. You can include links to files or GitHub projects, copy/pasteable snippets or even print screens or animated GIFS. If you're providing snippets in the issue, use Markdown code blocks . Describe the current behavior and explain which behavior you expected to see instead and why. List some other similar projects where this enhancement exists. Specify which version of the project you're using. Specify the current environment you're using. if this is a useful information. Creating Pull Requests \u00b6 How Do I Submit A (Good) Pull Request? \u00b6 Be warned that the contributors agreement displayed on top of this document is applicable as soon as you create a pull request . Use a clear and descriptive title for the pull request to state the improvement you made to the code or the bug you solved. Provide a link to the related issue if the pull request is a follow up of an existing bug report or enhancement suggestion. Comment why this pull request represents an enhancement and give a rationale explaining why you did it that way and not another way. Use the same coding style than the one used in this project . Welcome suggestions from the maintainers to improve your pull request .","title":"Contributions"},{"location":"CONTRIBUTING/#contributing-to-this-project","text":"Contributors Agreement Introduction I don't want to read this whole thing I just have a question!!! How Can I Contribute? Reporting Bugs Suggesting Enhancements Creating Pull Requests","title":"Contributing to this project"},{"location":"CONTRIBUTING/#contributors-agreement","text":"By submitting patches to this project you agree to allow them to be redistributed under the project's license, according to the normal forms and usages of the open-source community.","title":"Contributors Agreement"},{"location":"CONTRIBUTING/#introduction","text":"First off, thank you for considering contributing to this project. It's people like you that make it such a great tool. Following these guidelines helps to communicate that you respect the time of the developers managing and developing this open source project. In return, they should reciprocate that respect in addressing your issue, assessing changes, and helping you finalize your pull requests. This is an open source project and we love to receive contributions from our community \u2014 you! There are many ways to contribute, from writing tutorials or blog posts, improving the documentation, submitting bug reports and feature requests or writing code which can be incorporated into the main project itself.","title":"Introduction"},{"location":"CONTRIBUTING/#i-dont-want-to-read-this-whole-thing-i-just-have-a-question","text":"We currently allow our users to use the issue tracker for support questions. But please be wary that maintaining an open source project can take a lot of time from the maintainers. If asking for a support question, state it clearly and take the time to explain your problem properly. Also, if your problem is not strictly related to this project we recommend you to use Stack Overlow instead.","title":"I don't want to read this whole thing I just have a question!!!"},{"location":"CONTRIBUTING/#how-can-i-contribute","text":"","title":"How Can I Contribute?"},{"location":"CONTRIBUTING/#reporting-bugs","text":"Before creating bug reports, please check the existing bug reports as you might find out that you don't need to create one. When you are creating a bug report, please include as many details as possible.","title":"Reporting Bugs"},{"location":"CONTRIBUTING/#how-do-i-submit-a-good-bug-report","text":"Bugs are tracked as GitHub issues . Create an issue on the project's repository and provide the following information. Explain the problem and include additional details to help maintainers reproduce the problem: Use a clear and descriptive title for the issue to identify the problem. Describe the exact steps which reproduce the problem in as many details as possible. For example, start by explaining how you used the project. When listing steps, don't just say what you did, but explain how you did it . Provide specific examples to demonstrate the steps . It's always better to get more information. You can include links to files or GitHub projects, copy/pasteable snippets or even print screens or animated GIFS. If you're providing snippets in the issue, use Markdown code blocks . Describe the behavior you observed after following the steps and point out what exactly is the problem with that behavior. Explain which behavior you expected to see instead and why. If the problem wasn't triggered by a specific action , describe what you were doing before the problem happened and share more information using the guidelines below. Provide more context by answering these questions: Did the problem start happening recently (e.g. after updating to a new version) or was this always a problem? If the problem started happening recently, can you reproduce the problem in an older version? What's the most recent version in which the problem doesn't happen? Can you reliably reproduce the issue? If not, provide details about how often the problem happens and under which conditions it normally happens. Include details about your configuration and environment: Which version of the project are you using? What's the name and version of the OS you're using ? Any other information that could be useful about you environment","title":"How Do I Submit A (Good) Bug Report?"},{"location":"CONTRIBUTING/#suggesting-enhancements","text":"This section guides you through submitting an enhancement suggestion for this project, including completely new features and minor improvements to existing functionality. Following these guidelines helps maintainers and the community understand your suggestion and find related suggestions. Before creating enhancement suggestions, please check the list of enhancements suggestions in the issue tracker as you might find out that you don't need to create one. When you are creating an enhancement suggestion, please include as many details as possible.","title":"Suggesting Enhancements"},{"location":"CONTRIBUTING/#how-do-i-submit-a-good-enhancement-suggestion","text":"Enhancement suggestions are tracked as GitHub issues . Create an issue on the project's repository and provide the following information: Use a clear and descriptive title for the issue to identify the suggestion. Provide a step-by-step description of the suggested enhancement in as many details as possible. Provide specific examples to demonstrate the steps . It's always better to get more information. You can include links to files or GitHub projects, copy/pasteable snippets or even print screens or animated GIFS. If you're providing snippets in the issue, use Markdown code blocks . Describe the current behavior and explain which behavior you expected to see instead and why. List some other similar projects where this enhancement exists. Specify which version of the project you're using. Specify the current environment you're using. if this is a useful information.","title":"How Do I Submit A (Good) Enhancement Suggestion?"},{"location":"CONTRIBUTING/#creating-pull-requests","text":"","title":"Creating Pull Requests"},{"location":"CONTRIBUTING/#how-do-i-submit-a-good-pull-request","text":"Be warned that the contributors agreement displayed on top of this document is applicable as soon as you create a pull request . Use a clear and descriptive title for the pull request to state the improvement you made to the code or the bug you solved. Provide a link to the related issue if the pull request is a follow up of an existing bug report or enhancement suggestion. Comment why this pull request represents an enhancement and give a rationale explaining why you did it that way and not another way. Use the same coding style than the one used in this project . Welcome suggestions from the maintainers to improve your pull request .","title":"How Do I Submit A (Good) Pull Request?"},{"location":"RELEASE_NOTES/","text":"","title":"Release notes"},{"location":"about/","text":"","title":"About"},{"location":"adr/","text":"Architecture Decision Records \u00b6 Crowdsourcing: Eliminate direct RDBMS access: IN_PROGRESS Cache above RDMS: PROPOSED Messaging Queue System: PROPOSED Continous Intelligent Data Pipelines: PROPOSED UI Framework: PROPOSED Discuss \u00b6 Crowdsourcing platform: Eliminate direct RDBMS access \u00b6 Status: APPROVED Driver(s): Rajat Singhal Approver(s): Pramod Verma, Vivek Raghavan Impact Area: High Level Architecture Date created: 2021-03-24 Date closed: 2021-03-26 Technical Story: Eliminate direct RDBMS access Context and Problem Statement \u00b6 Currently, the application service directly talks with RDBMS to fetch the sentences shown while doing Contributions and to fetch contributions while doing the Validations. When the application will scale and more features are added, there may be some contention at RDBMS layer. See current architecture Solutions \u00b6 Add cache layer over RDMS Write to topics asynchronously Tech Choices \u00b6 For cache : AWS ElasticCache - Redis, Apache Ignite : ADR For queue: AWS Kafka, AWS Kinesis : ADR Proposed Architecture \u00b6 Caching and messaging queue layer can be added with small imcremental changes Positive Consequences \u00b6 RDBMS will be off loaded from frequent reads and writes Users will experience lower latency System will be able to scale easily at data layer Audio processing like 'automatic validation', 'SNR' etc can be done in 'Contributions processor' Contribution processor can be in any other langauge like Python which is more suited for audio processing If in future the analytics need to be moved to separate data store, rich application domain events can be used rather than crude CDC events. Additional domains can be on boarded easily like 'Likho India' by adding new domain topics, processors and services Can be easily evolved into event driven microservices architecture assuming more features will come onto this platform Negative Consequences \u00b6 Overhead of maintaining cache and queue system The user experience may change (may be for go) due to async behaviour of the system Decision Outcome \u00b6 WIP Choose Cache above RDBMS \u00b6 Status: PROPOSED Deciders: Rajat Singhal , Heera Ballabh, Soujyo Sen Impact Area: Tools and frameworks Date: 2021-03-24 Technical Story: [] Context and Problem Statement \u00b6 Options \u00b6 AWS ElasticCache(Redis), Apache Ignite Solutions \u00b6 Decision Outcome \u00b6 not yet decided Choose messaging queue system \u00b6 Status: PROPOSED Deciders: Rajat Singhal , Heera Ballabh, Soujyo Sen Impact Area: Tools and frameworks Date: 2021-03-24 Technical Story: [] Context and Problem Statement \u00b6 Options \u00b6 Apache Kafka, AWS Kinesis Solutions \u00b6 Decision Outcome \u00b6 not yet decided Continuous Intelligent Data Pipeline \u00b6 Status: PROPOSED Deciders: Rajat Singhal , Pramod Verma, Vivek Raghavan Impact Area: High Level Architecture Date: 2021-03-24 Technical Story: Continous Intelligent Data Pipelines Context and Problem Statement \u00b6 Solutions \u00b6 Decision Outcome \u00b6 Choose a UI framework \u00b6 Status: PROPOSED Deciders: Rajat Singhal , Sunny Goel Impact Area: Tools and frameworks Date: 2021-04-17 Technical Story: [] Context and Problem Statement \u00b6 Options \u00b6 Vue, React, Custom framework Solutions \u00b6 Decision Outcome \u00b6 not yet decided","title":"Architecture Decision Records"},{"location":"adr/#architecture-decision-records","text":"Crowdsourcing: Eliminate direct RDBMS access: IN_PROGRESS Cache above RDMS: PROPOSED Messaging Queue System: PROPOSED Continous Intelligent Data Pipelines: PROPOSED UI Framework: PROPOSED","title":"Architecture Decision Records"},{"location":"adr/#discuss","text":"","title":"Discuss"},{"location":"adr/#crowdsourcing-platform-eliminate-direct-rdbms-access","text":"Status: APPROVED Driver(s): Rajat Singhal Approver(s): Pramod Verma, Vivek Raghavan Impact Area: High Level Architecture Date created: 2021-03-24 Date closed: 2021-03-26 Technical Story: Eliminate direct RDBMS access","title":"Crowdsourcing platform: Eliminate direct RDBMS access"},{"location":"adr/#context-and-problem-statement","text":"Currently, the application service directly talks with RDBMS to fetch the sentences shown while doing Contributions and to fetch contributions while doing the Validations. When the application will scale and more features are added, there may be some contention at RDBMS layer. See current architecture","title":"Context and Problem Statement"},{"location":"adr/#solutions","text":"Add cache layer over RDMS Write to topics asynchronously","title":"Solutions"},{"location":"adr/#tech-choices","text":"For cache : AWS ElasticCache - Redis, Apache Ignite : ADR For queue: AWS Kafka, AWS Kinesis : ADR","title":"Tech Choices"},{"location":"adr/#proposed-architecture","text":"Caching and messaging queue layer can be added with small imcremental changes","title":"Proposed Architecture"},{"location":"adr/#positive-consequences","text":"RDBMS will be off loaded from frequent reads and writes Users will experience lower latency System will be able to scale easily at data layer Audio processing like 'automatic validation', 'SNR' etc can be done in 'Contributions processor' Contribution processor can be in any other langauge like Python which is more suited for audio processing If in future the analytics need to be moved to separate data store, rich application domain events can be used rather than crude CDC events. Additional domains can be on boarded easily like 'Likho India' by adding new domain topics, processors and services Can be easily evolved into event driven microservices architecture assuming more features will come onto this platform","title":"Positive Consequences"},{"location":"adr/#negative-consequences","text":"Overhead of maintaining cache and queue system The user experience may change (may be for go) due to async behaviour of the system","title":"Negative Consequences"},{"location":"adr/#decision-outcome","text":"WIP","title":"Decision Outcome"},{"location":"adr/#choose-cache-above-rdbms","text":"Status: PROPOSED Deciders: Rajat Singhal , Heera Ballabh, Soujyo Sen Impact Area: Tools and frameworks Date: 2021-03-24 Technical Story: []","title":"Choose Cache above RDBMS"},{"location":"adr/#context-and-problem-statement_1","text":"","title":"Context and Problem Statement"},{"location":"adr/#options","text":"AWS ElasticCache(Redis), Apache Ignite","title":"Options"},{"location":"adr/#solutions_1","text":"","title":"Solutions"},{"location":"adr/#decision-outcome_1","text":"not yet decided","title":"Decision Outcome"},{"location":"adr/#choose-messaging-queue-system","text":"Status: PROPOSED Deciders: Rajat Singhal , Heera Ballabh, Soujyo Sen Impact Area: Tools and frameworks Date: 2021-03-24 Technical Story: []","title":"Choose messaging queue system"},{"location":"adr/#context-and-problem-statement_2","text":"","title":"Context and Problem Statement"},{"location":"adr/#options_1","text":"Apache Kafka, AWS Kinesis","title":"Options"},{"location":"adr/#solutions_2","text":"","title":"Solutions"},{"location":"adr/#decision-outcome_2","text":"not yet decided","title":"Decision Outcome"},{"location":"adr/#continuous-intelligent-data-pipeline","text":"Status: PROPOSED Deciders: Rajat Singhal , Pramod Verma, Vivek Raghavan Impact Area: High Level Architecture Date: 2021-03-24 Technical Story: Continous Intelligent Data Pipelines","title":"Continuous Intelligent Data Pipeline"},{"location":"adr/#context-and-problem-statement_3","text":"","title":"Context and Problem Statement"},{"location":"adr/#solutions_3","text":"","title":"Solutions"},{"location":"adr/#decision-outcome_3","text":"","title":"Decision Outcome"},{"location":"adr/#choose-a-ui-framework","text":"Status: PROPOSED Deciders: Rajat Singhal , Sunny Goel Impact Area: Tools and frameworks Date: 2021-04-17 Technical Story: []","title":"Choose a UI framework"},{"location":"adr/#context-and-problem-statement_4","text":"","title":"Context and Problem Statement"},{"location":"adr/#options_2","text":"Vue, React, Custom framework","title":"Options"},{"location":"adr/#solutions_4","text":"","title":"Solutions"},{"location":"adr/#decision-outcome_4","text":"not yet decided","title":"Decision Outcome"},{"location":"adr_template/","text":"[short title of solved problem and solution] \u00b6 Status: [proposed | rejected | accepted | deprecated | \u2026 | superseded by ADR-0005 ] Deciders: [list everyone involved in the decision] Date: [YYYY-MM-DD when the decision was last updated] Technical Story: [description | ticket/issue URL] Context and Problem Statement \u00b6 [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] Decision Drivers \u00b6 [driver 1, e.g., a force, facing concern, \u2026] [driver 2, e.g., a force, facing concern, \u2026] \u2026 Considered Options \u00b6 [option 1] [option 2] [option 3] \u2026 Decision Outcome \u00b6 Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. Positive Consequences \u00b6 [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] \u2026 Negative Consequences \u00b6 [e.g., compromising quality attribute, follow-up decisions required, \u2026] \u2026 Pros and Cons of the Options \u00b6 [option 1] \u00b6 [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 [option 2] \u00b6 [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 [option 3] \u00b6 [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 Links \u00b6 [Link type] [Link to ADR] \u2026","title":"[short title of solved problem and solution]"},{"location":"adr_template/#short-title-of-solved-problem-and-solution","text":"Status: [proposed | rejected | accepted | deprecated | \u2026 | superseded by ADR-0005 ] Deciders: [list everyone involved in the decision] Date: [YYYY-MM-DD when the decision was last updated] Technical Story: [description | ticket/issue URL]","title":"[short title of solved problem and solution]"},{"location":"adr_template/#context-and-problem-statement","text":"[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]","title":"Context and Problem Statement"},{"location":"adr_template/#decision-drivers","text":"[driver 1, e.g., a force, facing concern, \u2026] [driver 2, e.g., a force, facing concern, \u2026] \u2026","title":"Decision Drivers "},{"location":"adr_template/#considered-options","text":"[option 1] [option 2] [option 3] \u2026","title":"Considered Options"},{"location":"adr_template/#decision-outcome","text":"Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)].","title":"Decision Outcome"},{"location":"adr_template/#positive-consequences","text":"[e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] \u2026","title":"Positive Consequences "},{"location":"adr_template/#negative-consequences","text":"[e.g., compromising quality attribute, follow-up decisions required, \u2026] \u2026","title":"Negative Consequences "},{"location":"adr_template/#pros-and-cons-of-the-options","text":"","title":"Pros and Cons of the Options "},{"location":"adr_template/#option-1","text":"[example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026","title":"[option 1]"},{"location":"adr_template/#option-2","text":"[example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026","title":"[option 2]"},{"location":"adr_template/#option-3","text":"[example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026","title":"[option 3]"},{"location":"adr_template/#links","text":"[Link type] [Link to ADR] \u2026","title":"Links "},{"location":"crowdsource_platform/","text":"Crowdsourcing Platform \u00b6 Table of Contents \u00b6 Crowdsourcing Platform Table of Contents About The Project Built With Architecture Logical Architecture Cloud Agnostic Architecture: Kubernetes Deployment Architecture: AWS Architecture Improvements Languages and Tools Dashboard Design CI/CD Infrastructure as Code Getting Started Prerequisites Installation Usage Common configuration steps: Setting credentials for Google cloud bucket Setting credentials for AWS cloud bucket Bucket configuration Environment file configurations Running services Database migrations Testing Unit Tests Functional Test Scalabiity Test Load Test Security Running cost estimates Architecture Decision Records Contributing License Git repository Contact About The Project \u00b6 This is a web application which can be used to crowdsource audio and validate them for various languages. The application makes use of NodeJs, Postgres for Database. It can be hosted on any cloud platform. The current application has code to support AWS and GCP as providers to store the recorded information. Crowdsourcing Platform\u2019s developer documentation is meant for its adopters, developers and contributors. The developer documentation helps you to get familiar with the bare necessities, giving you a quick and clean approach to get you up and running. If you are looking for ways to customize the workflow, or just breaking things down to build them back up, head to the reference section to dig into the mechanics of Crowdsourcing Platform. Data Collection Pipeline is based on an open platform, you are free to use any programming language to extend or customize it but we prefer to use python to perform smart scraping. The Developer documentation provides you with a complete set of guidelines which you need to: Install dependencies for the Crowdsourcing Platform Configure Crowdsourcing Platform Customize Crowdsourcing Platform Extend Crowdsourcing Platform Contribute to Crowdsourcing Platform Built With \u00b6 We have used Node.js to build this platform. * Node Architecture \u00b6 Logical Architecture \u00b6 The logical architecture is 3 layered with UI layer implemented using EJS templates, Service layer is implemented in Javascript which run on Nodejs server and storage layer which has RDBS and Object Storage Cloud Agnostic Architecture: Kubernetes \u00b6 This the architecture for deployment on Kubernetes, wherein the service layer is scalable leveraging K8s capabilities. Deployment Architecture: AWS \u00b6 This is the deployement architecure for running portal on AWS infra. It leverages AWS manages services like EKS and Fargate for K8s cluster and AWS RDS for managed database. It also uses AWS managed Load Balancer Architecture Improvements \u00b6 We believe in continously improving the architecture. Here are some ADR opened : Architecture Decision Records Proposed Architecture: Languages and Tools \u00b6 Dashboard Design \u00b6 The transactional tables and view tables are kept separate. Materialized views are used which holds the data as well. This avoids on the fly computations for aggregation for each query. The materizaled view are refreshed every 4 hours As a part of the refresh job, the aggregated data is dumped as json that is be served directly via CDN. Advantages: Faster reads: Separate view with only 365 aggregated data points per year. Less overhead on DB as data queried is on a very small data set and served from S3 buckets Transactional tables are optimized for faster writes as we have separate views for reads Simplified read queries as complexity is abstracted in views AWS RDS managed DB. Can be scaled horizontally and vertically easily if required in future. CI/CD \u00b6 CircleCI is used for CI/CD. Unit tests are run continously for each commit Functional Tests are run continously for each commit and act as one if the quality gates before Production deployment Automated deployment to K8s for multiple environments Database schema changes are done continously and automatically Trunk based developement is followed Infrastructure as Code \u00b6 Infrastructure defined in code with Terraform and shell scripts Easily migrate to another AWS account Spin up new env easily Getting Started \u00b6 To get started install the prerequisites and clone the repo to machine on which you wish to run the application. Prerequisites \u00b6 Install node library using commands mentioned below. For any linux based operating system (preferred Ubuntu): sudo apt-get install nodejs For Mac-os: brew install node Windows user can follow installation steps on https://nodejs.org/en/#home-downloadhead Install or connect to a postgres database Get credentials from google developer console for google cloud storage access/ or aws cli for amazon s3 storage access. Installation \u00b6 Clone the repo using git clone https://github.com/Open-Speech-EkStep/crowdsource-dataplatform.git Go inside the directory cd crowdsource-dataplatform Install node requirements npm install Usage \u00b6 Common configuration steps: \u00b6 Setting credentials for Google cloud bucket \u00b6 You can set credentials for Google cloud bucket by running the following command gcloud auth application-default login Setting credentials for AWS cloud bucket \u00b6 You can set credentials for AWS cloud bucket by running the following command aws configure Bucket configuration \u00b6 You can create a specific bucket to store the recorded samples on aws or gcp. And mention those in the environment variables. Environment file configurations \u00b6 The following are the variables required to run the application, for running on local these can be added to a .env file DB_HOST: The host url where your postgres instance is running DB_USER: The username to access the db DB_NAME: The database name DEV_DB_NAME: The database name specific to dev environment DB_PASS: The database password BUCKET_NAME: The bucket name configured on aws or gcp ENCRYPTION_KEY: Key to run unit tests PORT: Port to run the application on Running services \u00b6 Make sure the google credentials are present in project root folder in credentials.json file. You can run the project using the command npm run To run application using a Google cloud bucket npm run gcp To run application using a AWS cloud bucket npm run aws Database migrations \u00b6 This package is used to do migrations. To create the current database structure in your postgres instance, run the following command: db-migrate up It would read the configurations from the path migations/config/migration_config.json Once can also run the migrate up command by setting an environment variable DATABASE_URL=postgresql://${DB_USER}:${DB_PASS}@${DB_HOST}/${DB_NAME} To add a new migration db-migrate create add-new-table Using the above command with the --sqlFile flag would create corresponding .sql files in which one can write sql commands to do the operation. To rollback the last migration, one can db-migrate down Documentation for the package can be found here Testing \u00b6 Multiple types of tests are continously performed to make sure the application is in healthy state. Pyramid approach is followed with Unit tests at the base and Exploratory tests on top. Unit Tests \u00b6 Unit tests can be run using below command npm test Functional Test \u00b6 Functional tests can be run using below command npm run functional_test -- --env ( test | dev ) Scalabiity Test \u00b6 Scalabiity tests performed to verify that the system is elastically scalable Below tests were performed Test Objective: Scalability Test - Validate elastic scalability Resource Configuration: Environment: Dev Pod resources: 0.25 CPU/ 250M RAM Horizontal Pod Autoscaler : Scaling Threshold - 10% CPU Utilization Min pods: 1 Max Pods: 10 Test configuration: Number of concurrent users: 1000 Total Requests : 15000 Expected: Pods should scale if load increases and CPU utilization goes beyond 10% and should scale down after 5 mins Actual : Pods were scaled up after the CPU utilization went past 10%. Time to scale to desired state was around 2-3 mins Outcome: PASSED As surge started, pods started spinning up Load Test \u00b6 Load testing is performed to verify the system is able to handle 5K concurrent users without much impact on latency Test Objective: Load Test - Validate if application can handle 5K concurrent users Date: 04/03/2021 Resource Configuration: Environment: Test Initial Pods: 3 Pod resources: 2 CPU/ 2GB RAM Horizontal Pod Autoscaler : Scaling Threshold - 40% CPU Utilization Min pods: 3 , Max Pods: 10 Database CPU : 4 Test configuration: Number of concurrent users: 20000 Requests per user : 3 Ramp up time: 10 sec Iterations: 3 Outcome: PASSED ELB stats: Database stats: Jmeter stats: Summary: - This test had 20000 users ramped up within 1 min (3 times). - The test was performed from a single machine so 20K concurrent users could scale in 1 min. - All the requests were served within initial resources, no scaling was triggered. - All three endpoints served response in around 2 sec on an average. - The system was able to handle upto 12K concurrent users. - There were some errors thrown by AWS Load balancer may be due to single IP requests. - Database could handle the load and no connection leak is observed Security \u00b6 Security first approach is taken while building this application. The OWASP top 10 are ingrained in the application security DNA. Please reach out to srajat@thoughtworks or heerabal@thoughtworks.com for more information around Security Running cost estimates \u00b6 Cloud : AWS Amazon RDS (4 CPU): $400 WAF: $30 EKS + Fargate: $75 + $225 = $300 ELB: $150 Others: $200 Total: ~ $1100-1200 per month Architecture Decision Records \u00b6 Decision records are maintained HERE Cache above RDBMS Contributing \u00b6 Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request License \u00b6 Distributed under the [MIT] License. See LICENSE for more information. Git repository \u00b6 https://github.com/Open-Speech-EkStep/crowdsource-dataplatform/ Contact \u00b6 Connect with community on Gitter Project Link: https://github.com/Open-Speech-EkStep/crowdsource-dataplatform/","title":"Crowdsourcing Platform"},{"location":"crowdsource_platform/#crowdsourcing-platform","text":"","title":"Crowdsourcing Platform"},{"location":"crowdsource_platform/#table-of-contents","text":"Crowdsourcing Platform Table of Contents About The Project Built With Architecture Logical Architecture Cloud Agnostic Architecture: Kubernetes Deployment Architecture: AWS Architecture Improvements Languages and Tools Dashboard Design CI/CD Infrastructure as Code Getting Started Prerequisites Installation Usage Common configuration steps: Setting credentials for Google cloud bucket Setting credentials for AWS cloud bucket Bucket configuration Environment file configurations Running services Database migrations Testing Unit Tests Functional Test Scalabiity Test Load Test Security Running cost estimates Architecture Decision Records Contributing License Git repository Contact","title":"Table of Contents"},{"location":"crowdsource_platform/#about-the-project","text":"This is a web application which can be used to crowdsource audio and validate them for various languages. The application makes use of NodeJs, Postgres for Database. It can be hosted on any cloud platform. The current application has code to support AWS and GCP as providers to store the recorded information. Crowdsourcing Platform\u2019s developer documentation is meant for its adopters, developers and contributors. The developer documentation helps you to get familiar with the bare necessities, giving you a quick and clean approach to get you up and running. If you are looking for ways to customize the workflow, or just breaking things down to build them back up, head to the reference section to dig into the mechanics of Crowdsourcing Platform. Data Collection Pipeline is based on an open platform, you are free to use any programming language to extend or customize it but we prefer to use python to perform smart scraping. The Developer documentation provides you with a complete set of guidelines which you need to: Install dependencies for the Crowdsourcing Platform Configure Crowdsourcing Platform Customize Crowdsourcing Platform Extend Crowdsourcing Platform Contribute to Crowdsourcing Platform","title":"About The Project"},{"location":"crowdsource_platform/#built-with","text":"We have used Node.js to build this platform. * Node","title":"Built With"},{"location":"crowdsource_platform/#architecture","text":"","title":"Architecture"},{"location":"crowdsource_platform/#logical-architecture","text":"The logical architecture is 3 layered with UI layer implemented using EJS templates, Service layer is implemented in Javascript which run on Nodejs server and storage layer which has RDBS and Object Storage","title":"Logical Architecture"},{"location":"crowdsource_platform/#cloud-agnostic-architecture-kubernetes","text":"This the architecture for deployment on Kubernetes, wherein the service layer is scalable leveraging K8s capabilities.","title":"Cloud Agnostic Architecture: Kubernetes"},{"location":"crowdsource_platform/#deployment-architecture-aws","text":"This is the deployement architecure for running portal on AWS infra. It leverages AWS manages services like EKS and Fargate for K8s cluster and AWS RDS for managed database. It also uses AWS managed Load Balancer","title":"Deployment Architecture: AWS"},{"location":"crowdsource_platform/#architecture-improvements","text":"We believe in continously improving the architecture. Here are some ADR opened : Architecture Decision Records Proposed Architecture:","title":"Architecture Improvements"},{"location":"crowdsource_platform/#languages-and-tools","text":"","title":"Languages and Tools"},{"location":"crowdsource_platform/#dashboard-design","text":"The transactional tables and view tables are kept separate. Materialized views are used which holds the data as well. This avoids on the fly computations for aggregation for each query. The materizaled view are refreshed every 4 hours As a part of the refresh job, the aggregated data is dumped as json that is be served directly via CDN. Advantages: Faster reads: Separate view with only 365 aggregated data points per year. Less overhead on DB as data queried is on a very small data set and served from S3 buckets Transactional tables are optimized for faster writes as we have separate views for reads Simplified read queries as complexity is abstracted in views AWS RDS managed DB. Can be scaled horizontally and vertically easily if required in future.","title":"Dashboard Design"},{"location":"crowdsource_platform/#cicd","text":"CircleCI is used for CI/CD. Unit tests are run continously for each commit Functional Tests are run continously for each commit and act as one if the quality gates before Production deployment Automated deployment to K8s for multiple environments Database schema changes are done continously and automatically Trunk based developement is followed","title":"CI/CD"},{"location":"crowdsource_platform/#infrastructure-as-code","text":"Infrastructure defined in code with Terraform and shell scripts Easily migrate to another AWS account Spin up new env easily","title":"Infrastructure as Code"},{"location":"crowdsource_platform/#getting-started","text":"To get started install the prerequisites and clone the repo to machine on which you wish to run the application.","title":"Getting Started"},{"location":"crowdsource_platform/#prerequisites","text":"Install node library using commands mentioned below. For any linux based operating system (preferred Ubuntu): sudo apt-get install nodejs For Mac-os: brew install node Windows user can follow installation steps on https://nodejs.org/en/#home-downloadhead Install or connect to a postgres database Get credentials from google developer console for google cloud storage access/ or aws cli for amazon s3 storage access.","title":"Prerequisites"},{"location":"crowdsource_platform/#installation","text":"Clone the repo using git clone https://github.com/Open-Speech-EkStep/crowdsource-dataplatform.git Go inside the directory cd crowdsource-dataplatform Install node requirements npm install","title":"Installation"},{"location":"crowdsource_platform/#usage","text":"","title":"Usage"},{"location":"crowdsource_platform/#common-configuration-steps","text":"","title":"Common configuration steps:"},{"location":"crowdsource_platform/#setting-credentials-for-google-cloud-bucket","text":"You can set credentials for Google cloud bucket by running the following command gcloud auth application-default login","title":"Setting credentials for Google cloud bucket"},{"location":"crowdsource_platform/#setting-credentials-for-aws-cloud-bucket","text":"You can set credentials for AWS cloud bucket by running the following command aws configure","title":"Setting credentials for AWS cloud bucket"},{"location":"crowdsource_platform/#bucket-configuration","text":"You can create a specific bucket to store the recorded samples on aws or gcp. And mention those in the environment variables.","title":"Bucket configuration"},{"location":"crowdsource_platform/#environment-file-configurations","text":"The following are the variables required to run the application, for running on local these can be added to a .env file DB_HOST: The host url where your postgres instance is running DB_USER: The username to access the db DB_NAME: The database name DEV_DB_NAME: The database name specific to dev environment DB_PASS: The database password BUCKET_NAME: The bucket name configured on aws or gcp ENCRYPTION_KEY: Key to run unit tests PORT: Port to run the application on","title":"Environment file configurations"},{"location":"crowdsource_platform/#running-services","text":"Make sure the google credentials are present in project root folder in credentials.json file. You can run the project using the command npm run To run application using a Google cloud bucket npm run gcp To run application using a AWS cloud bucket npm run aws","title":"Running services"},{"location":"crowdsource_platform/#database-migrations","text":"This package is used to do migrations. To create the current database structure in your postgres instance, run the following command: db-migrate up It would read the configurations from the path migations/config/migration_config.json Once can also run the migrate up command by setting an environment variable DATABASE_URL=postgresql://${DB_USER}:${DB_PASS}@${DB_HOST}/${DB_NAME} To add a new migration db-migrate create add-new-table Using the above command with the --sqlFile flag would create corresponding .sql files in which one can write sql commands to do the operation. To rollback the last migration, one can db-migrate down Documentation for the package can be found here","title":"Database migrations"},{"location":"crowdsource_platform/#testing","text":"Multiple types of tests are continously performed to make sure the application is in healthy state. Pyramid approach is followed with Unit tests at the base and Exploratory tests on top.","title":"Testing"},{"location":"crowdsource_platform/#unit-tests","text":"Unit tests can be run using below command npm test","title":"Unit Tests"},{"location":"crowdsource_platform/#functional-test","text":"Functional tests can be run using below command npm run functional_test -- --env ( test | dev )","title":"Functional Test"},{"location":"crowdsource_platform/#scalabiity-test","text":"Scalabiity tests performed to verify that the system is elastically scalable Below tests were performed Test Objective: Scalability Test - Validate elastic scalability Resource Configuration: Environment: Dev Pod resources: 0.25 CPU/ 250M RAM Horizontal Pod Autoscaler : Scaling Threshold - 10% CPU Utilization Min pods: 1 Max Pods: 10 Test configuration: Number of concurrent users: 1000 Total Requests : 15000 Expected: Pods should scale if load increases and CPU utilization goes beyond 10% and should scale down after 5 mins Actual : Pods were scaled up after the CPU utilization went past 10%. Time to scale to desired state was around 2-3 mins Outcome: PASSED As surge started, pods started spinning up","title":"Scalabiity Test"},{"location":"crowdsource_platform/#load-test","text":"Load testing is performed to verify the system is able to handle 5K concurrent users without much impact on latency Test Objective: Load Test - Validate if application can handle 5K concurrent users Date: 04/03/2021 Resource Configuration: Environment: Test Initial Pods: 3 Pod resources: 2 CPU/ 2GB RAM Horizontal Pod Autoscaler : Scaling Threshold - 40% CPU Utilization Min pods: 3 , Max Pods: 10 Database CPU : 4 Test configuration: Number of concurrent users: 20000 Requests per user : 3 Ramp up time: 10 sec Iterations: 3 Outcome: PASSED ELB stats: Database stats: Jmeter stats: Summary: - This test had 20000 users ramped up within 1 min (3 times). - The test was performed from a single machine so 20K concurrent users could scale in 1 min. - All the requests were served within initial resources, no scaling was triggered. - All three endpoints served response in around 2 sec on an average. - The system was able to handle upto 12K concurrent users. - There were some errors thrown by AWS Load balancer may be due to single IP requests. - Database could handle the load and no connection leak is observed","title":"Load Test"},{"location":"crowdsource_platform/#security","text":"Security first approach is taken while building this application. The OWASP top 10 are ingrained in the application security DNA. Please reach out to srajat@thoughtworks or heerabal@thoughtworks.com for more information around Security","title":"Security"},{"location":"crowdsource_platform/#running-cost-estimates","text":"Cloud : AWS Amazon RDS (4 CPU): $400 WAF: $30 EKS + Fargate: $75 + $225 = $300 ELB: $150 Others: $200 Total: ~ $1100-1200 per month","title":"Running cost estimates"},{"location":"crowdsource_platform/#architecture-decision-records","text":"Decision records are maintained HERE Cache above RDBMS","title":"Architecture Decision Records"},{"location":"crowdsource_platform/#contributing","text":"Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request","title":"Contributing"},{"location":"crowdsource_platform/#license","text":"Distributed under the [MIT] License. See LICENSE for more information.","title":"License"},{"location":"crowdsource_platform/#git-repository","text":"https://github.com/Open-Speech-EkStep/crowdsource-dataplatform/","title":"Git repository"},{"location":"crowdsource_platform/#contact","text":"Connect with community on Gitter Project Link: https://github.com/Open-Speech-EkStep/crowdsource-dataplatform/","title":"Contact"},{"location":"data_collection/","text":"Data Collection Pipeline \u00b6 Table of Contents \u00b6 Data Collection Pipeline Table of Contents About The Project Built With Summary Getting Started Prerequisites Installation Usage Common configuration steps: Setting credentials for Google cloud bucket Bucket configuration Metadata file configurations Youtube download configurations Youtube API configuration Web Crawl Configuration Adding new spider Running services Youtube spider in channel mode: Youtube spider in file mode: Bing Spider Urls Spider Selenium google crawler Selenium youtube crawler for file mode and api mode Contributing License Git Repository Contact Acknowledgements About The Project \u00b6 This is downloading framework that is extensible and allows the user to add new source without much code changes. For each new source user need to write a scrapy spider script and rest of downloading and meta file creation is handled by repective pipelines. And if required user can add their custom pipelines. This framework automatically transfer the downloaded data to a Google cloud bucket automatically. For more info on writing scrapy spider and pipeline one can refer to the documentation . Data Collection Pipeline\u2019s developer documentation is meant for its adopters, developers and contributors. The developer documentation helps you to get familiar with the bare necessities, giving you a quick and clean approach to get you up and running. If you are looking for ways to customize the workflow, or just breaking things down to build them back up, head to the reference section to dig into the mechanics of Data Collection Pipeline. Data Collection Pipeline is based on an open platform, you are free to use any programming language to extend or customize it but we prefer to use python to perform smart scraping. The Developer documentation provides you with a complete set of guidelines which you need to: Install Data Collection Pipeline Configure Data Collection Pipeline Customize Data Collection Pipeline Extend Data Collection Pipeline Contribute to Data Collection Pipeline Built With \u00b6 We have used scrapy as the base of this framework. * Scrapy Summary \u00b6 This summary mentions the key advantages and limitations of this smart crawler service. Youtube Crawler Key Points and Advantages: Get language relevant channels from YouTube and download videos from them.(70%-80% relevancy with language - based on Manual Analysis) Can fetch channels with Creative Commons video and download the videos in them as well.(70% relevancy with language) Can download using file mode(manually filled with video Ids) or channel mode. Youtube-dl can fetch N number of videos from a channel and download them. YouTube crawler downloads files at a rate of maximum of 2000 hours per day and minimum of 800 hours per day. Youtube crawler is more convenient and it\u2019s a main source of Creative Commons data that can be accessed easily. It can be deployed in cloud service called zyte used for scraping/crawling. License information of videos are available in metadata. Limitations: Youtube-api cannot return more than 500 videos per channel.(when using YOUTUBE_API mode in configuration) Youtube-api is restricted to 10000 tokens per day in free mode. 10000 tokens can be used to get license info of 10000 videos.(in any mode) 10000 tokens can be used to get 5000 channels.(in YOUTUBE_API mode) Youtube-dl can be used to get all videos freely.(in YOUTUBE_DL mode) Cannot fetch data from specific playlist. (Solution: Fetch videos Ids of a playlist using YouTube-dl and put them in a file and download in file mode.) Rare cases in which you might get Too many requests error from Youtube-DL. (Solution: Rerun the application with same sources.) Cannot download videos which require user information and private videos. Web Crawler Key Points and Advantages: Web crawler can download specific language audio but with around 50 - 60% relevance. Web crawler downloads files at a rate of at least 2000 hours per day. It is a faster means of downloading data. Creative Commons license of videos can be identified if available while crawling websites. Limitations: Web crawler is not finely tuned yet, so downloaded content might have low language relevance. It cannot be deployed in zyte service free accounts and can be only deployed in zyte service paid accounts where docker container creation can be customised. License information of videos in web crawler cannot be automatically identified but requires some manual intervention. Getting Started \u00b6 To get started install the prerequisites and clone the repo to machine on which you wish to run the framework. Prerequisites \u00b6 Install ffmpeg library using commands mentioned below. For any linux based operating system (preferred Ubuntu): sudo apt-get install ffmpeg For Mac-os: brew install ffmpeg Windows user can follow installation steps on https://www.ffmpeg.org Install Python Version = 3.6 Get credentials from google developer console for google cloud storage access. Installation \u00b6 Clone the repo using git clone https://github.com/Open-Speech-EkStep/data-acquisition-pipeline.git Go inside the directory cd data-acquisition-pipeline Install python requirements ``` pip install -r requirements.txt Install gcloud utils Download from: https://cloud.google.com/sdk/docs/install#linux > gcloud init Usage \u00b6 This framework allows the user to download the media file from a websource(youtube, xyz.com, etc) and creates the respective metadata file from the data that is extracted from the file.For using any added source or to add new source refer to steps below.It can also crawl internet for media of a specific language. For web crawling, refer to the web crawl configuration below. Common configuration steps: \u00b6 Setting credentials for Google cloud bucket \u00b6 You can set credentials for Google cloud bucket in the credentials.json add the credentials in given manner It can be found in the project root folder. { \"Credentials\" : { YOUR ACCOUNT CREDENTIAL KEYS }} Note: All configuration files can be found in the following path data-acquisition-pipeline/data_acquisition_framework/configs/ Bucket configuration \u00b6 Bucket configurations for data transfer in storage_config.json \"bucket\" : \"ekstepspeechrecognition-dev\" , Your bucket name \"channel_blob_path\" : \"scrapydump/refactor_test\" , Path to directory where downloaded files is to be stored \"archive_blob_path\" : \"archive\" , Folder name in which history of download is to be maintained \"channels_file_blob_path\" : \"channels\" , Folder name in which channels and its videos are saved \"scraped_data_blob_path\" : \"data_to_be_scraped\" Folder name in which CSV for youtube file mode is stored Note: 1 . The scraped_data_blob_path folder should be present inside the channel_blob_path folder. 2 . The CSV file used in file mode of youtube and its name must be same as source_name given above. 3 . ( only for datacollector_urls and datacollector_bing spiders ) To autoconfigure language parameter to channel_blob_path from web_crawler_config.json, use <language> in channel_blob_path. \"eg: for tamil : data/download/<language>/audio - this will replace <language> with tamil.\" 4 . The archive_blob_path and channels_file_blob_path are folders that will be autogenerated in bucket with the given name. Metadata file configurations \u00b6 Metadata file configurations in config.json mode: 'complete' This should not be changed audio_id: null If you want to give a custom audio id add here cleaned_duration: null If you know the cleaned duration of audio add here num_of_speakers: null Number of speaker present in audio language: Hindi Language of audio has_other_audio_signature: False If audio has multiple speaker in same file ( True/False ) type: 'audio' Type of media ( audio or video ) source: 'Demo_Source' Source name experiment_use: False If its for experimental use ( True/False ) utterances_files_list: null source_website: '' Source website url experiment_name: null Name of experiment if experiment_use is True mother_tongue: null Accent of language ( Bengali, Marathi, etc... ) age_group: null Age group of speaker in audio recorded_state: null State in which audio is recorded recorded_district: null District of state in which audio is recorded recorded_place: null Recording location recorded_date: null Recording date purpose: null Purpose of recording speaker_gender: null Gender of speaker speaker_name: null Name of speaker Note: 1 . If any of the field info is not available keep its value to null 2 . If speaker_name or speaker_gender is given then that same will be used for all the files in given source Youtube download configurations \u00b6 You can set download mode [file/channel] in youtube_pipeline_config.py mode = 'file' # [channel,file] In file mode you will store a csv file whose name must be same as source name in scraped_data_blob_path. csv must contain urls of youtube videos, speaker name and gender as three different columns. Urls is a must field. You can leave speaker name and gender blank if data is not available. Given below is the structure of csv. video_url,speaker_name,speaker_gender https://www.youtube.com/watch?v = K1vW_ZikA5o,Ram_Singh,male https://www.youtube.com/watch?v = o82HIOgozi8,John_Doe,male ... Common configurations in youtube_pipeline_config.py # Common configurations \"source_name\" : \"DEMO\" , This is the name of source you are downloading batch_num = 1 Number of videos to be downloaded as batches youtube_service_to_use = YoutubeService.YOUTUBE_DL This field is to choose which service to use for getting video information only_creative_commons = False Should Download only creative commons ( True, False ) Possible values for youtube_service_to_use: (YoutubeService.YOUTUBE_DL, YoutubeService.YOUTUBE_API) File mode configurations in youtube_pipeline_config.py # File Mode configurations file_speaker_gender_column = 'speaker_gender' Gender column name in csv file file_speaker_name_column = \"speaker_name\" Speaker name column name in csv file file_url_name_column = \"video_url\" Video url column name in csv file license_column = \"license\" Video license column name in csv file channel mode configuration in youtube_pipeline_config.py # Channel mode configurations channel_url_dict = {} Channel url dictionary ( This will download all the videos from the given channels with corresponding source names ) Note: 1 . In channel_url_dict, the keys must be the urls and values must be their channel names 2 . To get list of channels from youtube API, channel_url_dict must be empty Youtube API configuration \u00b6 Automated Youtube fetching configuration in youtube_api_config.json # Youtube API configurations \"language\" : \"hindi\" , Type of language for which search results are required. \"language_code\" : \"hi\" , Language code for the specified language. \"keywords\" : [ The search keywords to be given in youtube API query \"audio\" , \"speech\" , \"talk\" ] , \"words_to_ignore\" : [ The words that are to be ignored in youtube API query \"song\" , \"music\" ] , \"max_results\" : 20 Maximum number of channels or results that is required. Web Crawl Configuration \u00b6 web crawl configuration in web_crawl_config.json (Use this only for datacollector_bing and datacollector_urls spider) \"language\" : \"gujarati\" , Language to be crawled \"language_code\" : \"gu\" , Language code for the specified language. \"keywords\" : [ Keywords to query \"talks audio\" , \"audiobooks\" , \"speeches\" , ] , \"word_to_ignore\" : [ Words to ignore while crawling \"ieeexplore.ieee.org\" , \"dl.acm.org\" , \"www.microsoft.com\" ] , \"extensions_to_ignore\" : [ Formats/extensions to ignore while crawling \".jpeg\" , \"xlsx\" , \".xml\" ] , \"extensions_to_include\" : [ Formats/extensions to include while crawling \".mp3\" , \".wav\" , \".mp4\" , ] , \"pages\" : 1 , Number of pages to crawl \"depth\" : 1 , Nesting depth for each website \"continue_page\" : \"NO\" , Field to continue /resume crawling \"last_visited\" : 200 , Last visited results count \"enable_hours_restriction\" : \"YES\" , Restrict crawling based on hours of data collected \"max_hours\" : 1 Maximum hours to crawl Adding new spider \u00b6 As we already mentioned our framework is extensible for any new source. To add a new source user just need to write a spider for that source. To add a spider you can follow the scrapy documentation or you can check our sample spider. Running services \u00b6 Make sure the google credentials are present in project root folder in credentials.json file. Youtube spider in channel mode: \u00b6 In data_acqusition_framework/configs , do the following: Open config.json and change language and type to your respective use case. Open storage_config.json and change bucket and channel_blob_path to your respective gcp paths.(For more info on these fields, scroll above to Bucket configuration) Open youtube_pipeline_config.py and change mode to channel (eg: mode='channel') There are two ways to download videos of youtube channels: You can hardcode the channel url and channel name. You can use youtube-utils service(youtube-dl/youtube data api) to fetch channels and its respective videos information. To download by hardcoding the channel urls, do the following: Open data_acqusition_framework/configs/youtube_pipeline_config.py and do the following: Add the channel_urls and its names in channel_url_dict variable. eg. channel_url_dict = { \"https://www.youtube.com/channel/1\": \"channel_name_a\", \"https://www.youtube.com/channel/2\":\"channel_name_b\" } Set youtube_service_to_use variable value to either YoutubeService.YOUTUBE_DL or YoutubeService.YOUTUBE_API for collecting video info. If YoutubeService.YOUTUBE_API is chosen, then get APIKEY for youtube data api from google developer console and store it in a file called .youtube_api_key in project root folder. Generate .youtube_api_key from From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos from youtube for the given channels and download them to bucket. To download by using youtube-utils service, do the following: Open data_acqusition_framework/configs/youtube_pipeline_config.py and do the following: Assign channel_url_dict = {} (If not empty, will not work). Set youtube_service_to_use variable value to either YoutubeService.YOUTUBE_DL or YoutubeService.YOUTUBE_API for collecting video info. If YoutubeService.YOUTUBE_API is chosen, then get APIKEY for youtube data api from google developer console and store it in a file called .youtube_api_key in project root folder. Open data_acqusition_framework/configs/youtube_api_config.json and change the fields to your requirements.(For more info: check above in Youtube api configuration) From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos from youtube for the given channels and download them to bucket. Youtube spider in file mode: \u00b6 In data_acqusition_framework/configs , do the following: Open config.json and change language and type to your respective use case. Open storage_config.json and change bucket and channel_blob_path to your respective gcp paths.(For more info on these fields, scroll above to Bucket configuration) Open youtube_pipeline_config.py and do the following: change mode to file (eg: mode='file'). change source_name to your requirement so that videos get downloaded to that folder in google storage bucket. Next Steps: Create a file in the following format: eg. source_name.csv with content (license column is optional): Here source_name in source_name.csv is the name you gave in youtube_pipeline_config.py file. It should be the same. video_url,speaker_name,speaker_gender,license https://www.youtube.com/watch?v=K1vW_ZikA5o,Ram_Singh,male,Creative Commons https://www.youtube.com/watch?v=o82HIOgozi8,John_Doe,male,Standard Youtube ... Now to upload this file to google cloud storage do the following: Open the channel_blob_path folder that you gave in storage_config.json and create a folder there named data_to_be_scraped . Upload the file that you created with previous step to this folder. From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos mentioned in the file from youtube and download them to bucket. Bing Spider \u00b6 Configure data_acquisition_framework/configs/web_crawl_config.json for your requirements. Starting datacollector_bing spider with audio pipeline. From project root folder, run the following: scrapy crawl datacollector_bing Urls Spider \u00b6 Configure data_acquisition_framework/configs/web_crawl_config.json for your requirements. Starting datacollector_urls spider with audio pipeline. Make sure to put the urls to crawl in the data_acquisition_framework/urls.txt . From project root folder, run the following: scrapy crawl datacollector_urls Selenium google crawler \u00b6 It is capable of crawling search results of google for a given language and exporting them to urls.txt file. This urls.txt file can be used with datacollector_urls spider to crawl all the search results website and download the media along with their metadata. A specified Readme can be found in selenium_google_crawler folder. Readme for selenium google crawler Selenium youtube crawler for file mode and api mode \u00b6 It is capable of crawling youtube videos using youtube api or from a list of files with youtube video ids provided with channel name as filename. A specified Readme can be found in selenium_youtube_crawler folder. Readme for selenium youtube crawler Contributing \u00b6 Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request License \u00b6 Distributed under the [XYZ] License. See LICENSE for more information. Git Repository \u00b6 https://github.com/Open-Speech-EkStep/data-acquisition-pipeline Contact \u00b6 Connect with community on Gitter Acknowledgements \u00b6 Scrapy YouTube-dl TinyTag","title":"Data Collection Pipeine"},{"location":"data_collection/#data-collection-pipeline","text":"","title":"Data Collection Pipeline"},{"location":"data_collection/#table-of-contents","text":"Data Collection Pipeline Table of Contents About The Project Built With Summary Getting Started Prerequisites Installation Usage Common configuration steps: Setting credentials for Google cloud bucket Bucket configuration Metadata file configurations Youtube download configurations Youtube API configuration Web Crawl Configuration Adding new spider Running services Youtube spider in channel mode: Youtube spider in file mode: Bing Spider Urls Spider Selenium google crawler Selenium youtube crawler for file mode and api mode Contributing License Git Repository Contact Acknowledgements","title":"Table of Contents"},{"location":"data_collection/#about-the-project","text":"This is downloading framework that is extensible and allows the user to add new source without much code changes. For each new source user need to write a scrapy spider script and rest of downloading and meta file creation is handled by repective pipelines. And if required user can add their custom pipelines. This framework automatically transfer the downloaded data to a Google cloud bucket automatically. For more info on writing scrapy spider and pipeline one can refer to the documentation . Data Collection Pipeline\u2019s developer documentation is meant for its adopters, developers and contributors. The developer documentation helps you to get familiar with the bare necessities, giving you a quick and clean approach to get you up and running. If you are looking for ways to customize the workflow, or just breaking things down to build them back up, head to the reference section to dig into the mechanics of Data Collection Pipeline. Data Collection Pipeline is based on an open platform, you are free to use any programming language to extend or customize it but we prefer to use python to perform smart scraping. The Developer documentation provides you with a complete set of guidelines which you need to: Install Data Collection Pipeline Configure Data Collection Pipeline Customize Data Collection Pipeline Extend Data Collection Pipeline Contribute to Data Collection Pipeline","title":"About The Project"},{"location":"data_collection/#built-with","text":"We have used scrapy as the base of this framework. * Scrapy","title":"Built With"},{"location":"data_collection/#summary","text":"This summary mentions the key advantages and limitations of this smart crawler service. Youtube Crawler Key Points and Advantages: Get language relevant channels from YouTube and download videos from them.(70%-80% relevancy with language - based on Manual Analysis) Can fetch channels with Creative Commons video and download the videos in them as well.(70% relevancy with language) Can download using file mode(manually filled with video Ids) or channel mode. Youtube-dl can fetch N number of videos from a channel and download them. YouTube crawler downloads files at a rate of maximum of 2000 hours per day and minimum of 800 hours per day. Youtube crawler is more convenient and it\u2019s a main source of Creative Commons data that can be accessed easily. It can be deployed in cloud service called zyte used for scraping/crawling. License information of videos are available in metadata. Limitations: Youtube-api cannot return more than 500 videos per channel.(when using YOUTUBE_API mode in configuration) Youtube-api is restricted to 10000 tokens per day in free mode. 10000 tokens can be used to get license info of 10000 videos.(in any mode) 10000 tokens can be used to get 5000 channels.(in YOUTUBE_API mode) Youtube-dl can be used to get all videos freely.(in YOUTUBE_DL mode) Cannot fetch data from specific playlist. (Solution: Fetch videos Ids of a playlist using YouTube-dl and put them in a file and download in file mode.) Rare cases in which you might get Too many requests error from Youtube-DL. (Solution: Rerun the application with same sources.) Cannot download videos which require user information and private videos. Web Crawler Key Points and Advantages: Web crawler can download specific language audio but with around 50 - 60% relevance. Web crawler downloads files at a rate of at least 2000 hours per day. It is a faster means of downloading data. Creative Commons license of videos can be identified if available while crawling websites. Limitations: Web crawler is not finely tuned yet, so downloaded content might have low language relevance. It cannot be deployed in zyte service free accounts and can be only deployed in zyte service paid accounts where docker container creation can be customised. License information of videos in web crawler cannot be automatically identified but requires some manual intervention.","title":"Summary"},{"location":"data_collection/#getting-started","text":"To get started install the prerequisites and clone the repo to machine on which you wish to run the framework.","title":"Getting Started"},{"location":"data_collection/#prerequisites","text":"Install ffmpeg library using commands mentioned below. For any linux based operating system (preferred Ubuntu): sudo apt-get install ffmpeg For Mac-os: brew install ffmpeg Windows user can follow installation steps on https://www.ffmpeg.org Install Python Version = 3.6 Get credentials from google developer console for google cloud storage access.","title":"Prerequisites"},{"location":"data_collection/#installation","text":"Clone the repo using git clone https://github.com/Open-Speech-EkStep/data-acquisition-pipeline.git Go inside the directory cd data-acquisition-pipeline Install python requirements ``` pip install -r requirements.txt Install gcloud utils Download from: https://cloud.google.com/sdk/docs/install#linux > gcloud init","title":"Installation"},{"location":"data_collection/#usage","text":"This framework allows the user to download the media file from a websource(youtube, xyz.com, etc) and creates the respective metadata file from the data that is extracted from the file.For using any added source or to add new source refer to steps below.It can also crawl internet for media of a specific language. For web crawling, refer to the web crawl configuration below.","title":"Usage"},{"location":"data_collection/#common-configuration-steps","text":"","title":"Common configuration steps:"},{"location":"data_collection/#setting-credentials-for-google-cloud-bucket","text":"You can set credentials for Google cloud bucket in the credentials.json add the credentials in given manner It can be found in the project root folder. { \"Credentials\" : { YOUR ACCOUNT CREDENTIAL KEYS }} Note: All configuration files can be found in the following path data-acquisition-pipeline/data_acquisition_framework/configs/","title":"Setting credentials for Google cloud bucket"},{"location":"data_collection/#bucket-configuration","text":"Bucket configurations for data transfer in storage_config.json \"bucket\" : \"ekstepspeechrecognition-dev\" , Your bucket name \"channel_blob_path\" : \"scrapydump/refactor_test\" , Path to directory where downloaded files is to be stored \"archive_blob_path\" : \"archive\" , Folder name in which history of download is to be maintained \"channels_file_blob_path\" : \"channels\" , Folder name in which channels and its videos are saved \"scraped_data_blob_path\" : \"data_to_be_scraped\" Folder name in which CSV for youtube file mode is stored Note: 1 . The scraped_data_blob_path folder should be present inside the channel_blob_path folder. 2 . The CSV file used in file mode of youtube and its name must be same as source_name given above. 3 . ( only for datacollector_urls and datacollector_bing spiders ) To autoconfigure language parameter to channel_blob_path from web_crawler_config.json, use <language> in channel_blob_path. \"eg: for tamil : data/download/<language>/audio - this will replace <language> with tamil.\" 4 . The archive_blob_path and channels_file_blob_path are folders that will be autogenerated in bucket with the given name.","title":"Bucket configuration"},{"location":"data_collection/#metadata-file-configurations","text":"Metadata file configurations in config.json mode: 'complete' This should not be changed audio_id: null If you want to give a custom audio id add here cleaned_duration: null If you know the cleaned duration of audio add here num_of_speakers: null Number of speaker present in audio language: Hindi Language of audio has_other_audio_signature: False If audio has multiple speaker in same file ( True/False ) type: 'audio' Type of media ( audio or video ) source: 'Demo_Source' Source name experiment_use: False If its for experimental use ( True/False ) utterances_files_list: null source_website: '' Source website url experiment_name: null Name of experiment if experiment_use is True mother_tongue: null Accent of language ( Bengali, Marathi, etc... ) age_group: null Age group of speaker in audio recorded_state: null State in which audio is recorded recorded_district: null District of state in which audio is recorded recorded_place: null Recording location recorded_date: null Recording date purpose: null Purpose of recording speaker_gender: null Gender of speaker speaker_name: null Name of speaker Note: 1 . If any of the field info is not available keep its value to null 2 . If speaker_name or speaker_gender is given then that same will be used for all the files in given source","title":"Metadata file configurations"},{"location":"data_collection/#youtube-download-configurations","text":"You can set download mode [file/channel] in youtube_pipeline_config.py mode = 'file' # [channel,file] In file mode you will store a csv file whose name must be same as source name in scraped_data_blob_path. csv must contain urls of youtube videos, speaker name and gender as three different columns. Urls is a must field. You can leave speaker name and gender blank if data is not available. Given below is the structure of csv. video_url,speaker_name,speaker_gender https://www.youtube.com/watch?v = K1vW_ZikA5o,Ram_Singh,male https://www.youtube.com/watch?v = o82HIOgozi8,John_Doe,male ... Common configurations in youtube_pipeline_config.py # Common configurations \"source_name\" : \"DEMO\" , This is the name of source you are downloading batch_num = 1 Number of videos to be downloaded as batches youtube_service_to_use = YoutubeService.YOUTUBE_DL This field is to choose which service to use for getting video information only_creative_commons = False Should Download only creative commons ( True, False ) Possible values for youtube_service_to_use: (YoutubeService.YOUTUBE_DL, YoutubeService.YOUTUBE_API) File mode configurations in youtube_pipeline_config.py # File Mode configurations file_speaker_gender_column = 'speaker_gender' Gender column name in csv file file_speaker_name_column = \"speaker_name\" Speaker name column name in csv file file_url_name_column = \"video_url\" Video url column name in csv file license_column = \"license\" Video license column name in csv file channel mode configuration in youtube_pipeline_config.py # Channel mode configurations channel_url_dict = {} Channel url dictionary ( This will download all the videos from the given channels with corresponding source names ) Note: 1 . In channel_url_dict, the keys must be the urls and values must be their channel names 2 . To get list of channels from youtube API, channel_url_dict must be empty","title":"Youtube download configurations"},{"location":"data_collection/#youtube-api-configuration","text":"Automated Youtube fetching configuration in youtube_api_config.json # Youtube API configurations \"language\" : \"hindi\" , Type of language for which search results are required. \"language_code\" : \"hi\" , Language code for the specified language. \"keywords\" : [ The search keywords to be given in youtube API query \"audio\" , \"speech\" , \"talk\" ] , \"words_to_ignore\" : [ The words that are to be ignored in youtube API query \"song\" , \"music\" ] , \"max_results\" : 20 Maximum number of channels or results that is required.","title":"Youtube API configuration"},{"location":"data_collection/#web-crawl-configuration","text":"web crawl configuration in web_crawl_config.json (Use this only for datacollector_bing and datacollector_urls spider) \"language\" : \"gujarati\" , Language to be crawled \"language_code\" : \"gu\" , Language code for the specified language. \"keywords\" : [ Keywords to query \"talks audio\" , \"audiobooks\" , \"speeches\" , ] , \"word_to_ignore\" : [ Words to ignore while crawling \"ieeexplore.ieee.org\" , \"dl.acm.org\" , \"www.microsoft.com\" ] , \"extensions_to_ignore\" : [ Formats/extensions to ignore while crawling \".jpeg\" , \"xlsx\" , \".xml\" ] , \"extensions_to_include\" : [ Formats/extensions to include while crawling \".mp3\" , \".wav\" , \".mp4\" , ] , \"pages\" : 1 , Number of pages to crawl \"depth\" : 1 , Nesting depth for each website \"continue_page\" : \"NO\" , Field to continue /resume crawling \"last_visited\" : 200 , Last visited results count \"enable_hours_restriction\" : \"YES\" , Restrict crawling based on hours of data collected \"max_hours\" : 1 Maximum hours to crawl","title":"Web Crawl Configuration"},{"location":"data_collection/#adding-new-spider","text":"As we already mentioned our framework is extensible for any new source. To add a new source user just need to write a spider for that source. To add a spider you can follow the scrapy documentation or you can check our sample spider.","title":"Adding new spider"},{"location":"data_collection/#running-services","text":"Make sure the google credentials are present in project root folder in credentials.json file.","title":"Running services"},{"location":"data_collection/#youtube-spider-in-channel-mode","text":"In data_acqusition_framework/configs , do the following: Open config.json and change language and type to your respective use case. Open storage_config.json and change bucket and channel_blob_path to your respective gcp paths.(For more info on these fields, scroll above to Bucket configuration) Open youtube_pipeline_config.py and change mode to channel (eg: mode='channel') There are two ways to download videos of youtube channels: You can hardcode the channel url and channel name. You can use youtube-utils service(youtube-dl/youtube data api) to fetch channels and its respective videos information. To download by hardcoding the channel urls, do the following: Open data_acqusition_framework/configs/youtube_pipeline_config.py and do the following: Add the channel_urls and its names in channel_url_dict variable. eg. channel_url_dict = { \"https://www.youtube.com/channel/1\": \"channel_name_a\", \"https://www.youtube.com/channel/2\":\"channel_name_b\" } Set youtube_service_to_use variable value to either YoutubeService.YOUTUBE_DL or YoutubeService.YOUTUBE_API for collecting video info. If YoutubeService.YOUTUBE_API is chosen, then get APIKEY for youtube data api from google developer console and store it in a file called .youtube_api_key in project root folder. Generate .youtube_api_key from From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos from youtube for the given channels and download them to bucket. To download by using youtube-utils service, do the following: Open data_acqusition_framework/configs/youtube_pipeline_config.py and do the following: Assign channel_url_dict = {} (If not empty, will not work). Set youtube_service_to_use variable value to either YoutubeService.YOUTUBE_DL or YoutubeService.YOUTUBE_API for collecting video info. If YoutubeService.YOUTUBE_API is chosen, then get APIKEY for youtube data api from google developer console and store it in a file called .youtube_api_key in project root folder. Open data_acqusition_framework/configs/youtube_api_config.json and change the fields to your requirements.(For more info: check above in Youtube api configuration) From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos from youtube for the given channels and download them to bucket.","title":"Youtube spider in channel mode:"},{"location":"data_collection/#youtube-spider-in-file-mode","text":"In data_acqusition_framework/configs , do the following: Open config.json and change language and type to your respective use case. Open storage_config.json and change bucket and channel_blob_path to your respective gcp paths.(For more info on these fields, scroll above to Bucket configuration) Open youtube_pipeline_config.py and do the following: change mode to file (eg: mode='file'). change source_name to your requirement so that videos get downloaded to that folder in google storage bucket. Next Steps: Create a file in the following format: eg. source_name.csv with content (license column is optional): Here source_name in source_name.csv is the name you gave in youtube_pipeline_config.py file. It should be the same. video_url,speaker_name,speaker_gender,license https://www.youtube.com/watch?v=K1vW_ZikA5o,Ram_Singh,male,Creative Commons https://www.youtube.com/watch?v=o82HIOgozi8,John_Doe,male,Standard Youtube ... Now to upload this file to google cloud storage do the following: Open the channel_blob_path folder that you gave in storage_config.json and create a folder there named data_to_be_scraped . Upload the file that you created with previous step to this folder. From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos mentioned in the file from youtube and download them to bucket.","title":"Youtube spider in file mode:"},{"location":"data_collection/#bing-spider","text":"Configure data_acquisition_framework/configs/web_crawl_config.json for your requirements. Starting datacollector_bing spider with audio pipeline. From project root folder, run the following: scrapy crawl datacollector_bing","title":"Bing Spider"},{"location":"data_collection/#urls-spider","text":"Configure data_acquisition_framework/configs/web_crawl_config.json for your requirements. Starting datacollector_urls spider with audio pipeline. Make sure to put the urls to crawl in the data_acquisition_framework/urls.txt . From project root folder, run the following: scrapy crawl datacollector_urls","title":"Urls Spider"},{"location":"data_collection/#selenium-google-crawler","text":"It is capable of crawling search results of google for a given language and exporting them to urls.txt file. This urls.txt file can be used with datacollector_urls spider to crawl all the search results website and download the media along with their metadata. A specified Readme can be found in selenium_google_crawler folder. Readme for selenium google crawler","title":"Selenium google crawler"},{"location":"data_collection/#selenium-youtube-crawler-for-file-mode-and-api-mode","text":"It is capable of crawling youtube videos using youtube api or from a list of files with youtube video ids provided with channel name as filename. A specified Readme can be found in selenium_youtube_crawler folder. Readme for selenium youtube crawler","title":"Selenium youtube crawler for file mode and api mode"},{"location":"data_collection/#contributing","text":"Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request","title":"Contributing"},{"location":"data_collection/#license","text":"Distributed under the [XYZ] License. See LICENSE for more information.","title":"License"},{"location":"data_collection/#git-repository","text":"https://github.com/Open-Speech-EkStep/data-acquisition-pipeline","title":"Git Repository"},{"location":"data_collection/#contact","text":"Connect with community on Gitter","title":"Contact"},{"location":"data_collection/#acknowledgements","text":"Scrapy YouTube-dl TinyTag","title":"Acknowledgements"},{"location":"gender_identification/","text":"Gender Identification \u00b6 Table of Contents \u00b6 Gender Identification Table of Contents About The Project Working Embeddings Classification algorithm Hyperparameters Train data Test data Usage Parameters to change Commands for Inference About The Project \u00b6 Gender Identification is the task of classifying an audio into gender labels (we used 'male' and 'female'). We needed to identify whether the voice present in each utterance was male or female since it is a very important step while trying to balance your data among gender classes, for any downstream task. This also helps in maintaining diversity in data. Our intelligent data pipelines split each audio, based on Voice Activity Detection, as one of the starting steps. These shorter audio utterances can then used to train deep learning models. We assume that the utterances are short enough to have one speaker per utterance, since the splitting logic is using unvoiced segments as points to split an audio. We use embeddings (explained in the next section) to train a machine learning based classifier and achieve an accurracy of 96% on our test set. (Datasets and training explained later.) Working \u00b6 Embeddings \u00b6 To convert our audio utternaces into fixed length summary vectors, we use the Voice Encoder model proposed here , and implemented here for converting our audio utterances into fixed length embeddings. Voice Encoder is a speaker-discriminative model trained on a text-independent speaker verification task. Since these embeddings are able to summarise the characteristics of the voice spoken, they have been used for Gender Classification here , as shown in the diagram below. The above plot shows a clear boundary for gender between embeddings from distinct speakers present in LibriSpeech dataset, when projected in 2D using umap. Classification algorithm \u00b6 We use the Support vector machine classifier defined here to train for gender classification task, with embeddings from previous step and input, and their correct gender labels as target classes. Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection. The advantages of support vector machines are: Effective in high dimensional spaces. Our space is 256 dimensional. Still effective in cases where number of dimensions is greater than the number of samples. Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient. Hyperparameters \u00b6 Below is the description of each hyperparameter we used, with the value we used it it. kernel: 'rbf' RBF Kernel is popular because of its similarity to K-Nearest Neighborhood Algorithm. It has the advantages of K-NN and overcomes the space complexity problem as RBF Kernel Support Vector Machines just needs to store the support vectors during training and not the entire dataset. When training an SVM with the Radial Basis Function (RBF) kernel, two parameters must be considered: C and gamma. The parameter C, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly. gamma defines how much influence a single training example has. The larger gamma is, the closer other examples must be to be affected. We used the following values for C and gamma: - gamma: 0.01 - C: 100 Train data \u00b6 We used data for training as a combined set of audios in the following languages: Hindi, Tamil, Telugu, Kannada. Final train dataset had the following distribution: - male: 17.4 hours - female: 16.2 hours Test data \u00b6 We used data for training as a combined set of audios in the following languages: Hindi, Tamil, Telugu, Kannada, Marathi and Bengali. Test data had two additional languages, as compared to the train data: Marathi and Bengali. Final test dataset (balanced) had the following distribution: - male: 3.6 hours - female: 3.6 hours Usage \u00b6 Parameters to change \u00b6 model-path : set the path where you want to save the trained model. eg: 'path/to/trained/model/model.sav' file-mode : default=False; set True for single file inference file-path : path to .wav file if --file-mode = True csv-path : path to csv containing multiple audio file paths save-dir : default= current directory; else give path to save predictions.csv Commands for Inference \u00b6 For single file inference: python scripts/inference.py --model-path model/clf_svc.sav --file-mode True --file-path <filename>.wav For CSV mode inference: Create a csv containing multiple file paths python scripts/inference.py --model-path model/clf_svc.sav --csv-path <file_paths>.csv --save-dir <destination path>","title":"Gender Identification"},{"location":"gender_identification/#gender-identification","text":"","title":"Gender Identification"},{"location":"gender_identification/#table-of-contents","text":"Gender Identification Table of Contents About The Project Working Embeddings Classification algorithm Hyperparameters Train data Test data Usage Parameters to change Commands for Inference","title":"Table of Contents"},{"location":"gender_identification/#about-the-project","text":"Gender Identification is the task of classifying an audio into gender labels (we used 'male' and 'female'). We needed to identify whether the voice present in each utterance was male or female since it is a very important step while trying to balance your data among gender classes, for any downstream task. This also helps in maintaining diversity in data. Our intelligent data pipelines split each audio, based on Voice Activity Detection, as one of the starting steps. These shorter audio utterances can then used to train deep learning models. We assume that the utterances are short enough to have one speaker per utterance, since the splitting logic is using unvoiced segments as points to split an audio. We use embeddings (explained in the next section) to train a machine learning based classifier and achieve an accurracy of 96% on our test set. (Datasets and training explained later.)","title":"About The Project"},{"location":"gender_identification/#working","text":"","title":"Working"},{"location":"gender_identification/#embeddings","text":"To convert our audio utternaces into fixed length summary vectors, we use the Voice Encoder model proposed here , and implemented here for converting our audio utterances into fixed length embeddings. Voice Encoder is a speaker-discriminative model trained on a text-independent speaker verification task. Since these embeddings are able to summarise the characteristics of the voice spoken, they have been used for Gender Classification here , as shown in the diagram below. The above plot shows a clear boundary for gender between embeddings from distinct speakers present in LibriSpeech dataset, when projected in 2D using umap.","title":"Embeddings"},{"location":"gender_identification/#classification-algorithm","text":"We use the Support vector machine classifier defined here to train for gender classification task, with embeddings from previous step and input, and their correct gender labels as target classes. Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection. The advantages of support vector machines are: Effective in high dimensional spaces. Our space is 256 dimensional. Still effective in cases where number of dimensions is greater than the number of samples. Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.","title":"Classification algorithm"},{"location":"gender_identification/#hyperparameters","text":"Below is the description of each hyperparameter we used, with the value we used it it. kernel: 'rbf' RBF Kernel is popular because of its similarity to K-Nearest Neighborhood Algorithm. It has the advantages of K-NN and overcomes the space complexity problem as RBF Kernel Support Vector Machines just needs to store the support vectors during training and not the entire dataset. When training an SVM with the Radial Basis Function (RBF) kernel, two parameters must be considered: C and gamma. The parameter C, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly. gamma defines how much influence a single training example has. The larger gamma is, the closer other examples must be to be affected. We used the following values for C and gamma: - gamma: 0.01 - C: 100","title":"Hyperparameters"},{"location":"gender_identification/#train-data","text":"We used data for training as a combined set of audios in the following languages: Hindi, Tamil, Telugu, Kannada. Final train dataset had the following distribution: - male: 17.4 hours - female: 16.2 hours","title":"Train data"},{"location":"gender_identification/#test-data","text":"We used data for training as a combined set of audios in the following languages: Hindi, Tamil, Telugu, Kannada, Marathi and Bengali. Test data had two additional languages, as compared to the train data: Marathi and Bengali. Final test dataset (balanced) had the following distribution: - male: 3.6 hours - female: 3.6 hours","title":"Test data"},{"location":"gender_identification/#usage","text":"","title":"Usage"},{"location":"gender_identification/#parameters-to-change","text":"model-path : set the path where you want to save the trained model. eg: 'path/to/trained/model/model.sav' file-mode : default=False; set True for single file inference file-path : path to .wav file if --file-mode = True csv-path : path to csv containing multiple audio file paths save-dir : default= current directory; else give path to save predictions.csv","title":"Parameters to change"},{"location":"gender_identification/#commands-for-inference","text":"For single file inference: python scripts/inference.py --model-path model/clf_svc.sav --file-mode True --file-path <filename>.wav For CSV mode inference: Create a csv containing multiple file paths python scripts/inference.py --model-path model/clf_svc.sav --csv-path <file_paths>.csv --save-dir <destination path>","title":"Commands for Inference"},{"location":"intelligent_data_pipelines/","text":"Intelligent Data Pipeline \u00b6 Table of Contents \u00b6 Intelligent Data Pipeline Table of Contents About The Project Getting Started Architecture Intelligent Data Pipeline - Jobs Audio Processor Audio Analysis Language identification Speaker identification Gender identification Audio Data Balancing Audio Validation Audio Transcription Installation Run on Kubernetes Using Composer Requirements Infra Setup CI/CD setup Audio Processing Description Config Steps to run Audio Analysis Config Steps to run Data Balancing config steps to run: Audio Transcription (with config): config: steps to run: Contributing License Git Repository Contact About The Project \u00b6 Intelligent Data Pipelines are built to create the audio data set that can be used for Speech Recognition deeplearning models. The aim is to allow easy, quick and fast dataset generation without doing manual work. It splits data into smallers utterences which are understood well by deeplearning models. The data is then cleansed based on 'Signal to Noise ratio'. The audio analysis is performed using pre trained models and clustering based on audio features (see Resymbleyr for more details). It leverages Kubernetes for parallel computing and below are the metrics we have acheived so far: Some stats for a language with 1000 hrs raw data Raw data 1000 hrs Time taken: 2-3 days Final Usable Data of Pretraining: 600 Final Usable Data of Fine Tuning: 400 Getting Started \u00b6 The developer documentation helps you to get familiar with the bare necessities, giving you a quick and clean approach to get you up and running. If you are looking for ways to customize the workflow, or just breaking things down to build them back up, head to the reference section to dig into the mechanics of Data Pipelines. To get started install the prerequisites and clone the repo to machine on which you wish to run the framework. Here is the code Architecture \u00b6 Intelligent Data Pipeline - Jobs \u00b6 Audio Processor \u00b6 Audio Processor job takes raw data generted from Data Collection Pipeline or it also consumes data that is generated by any other means. It splits data into smaller utterances and then cleanses based on 'Signal to Noise Ratio (SNR)'. The threshold can be changed through configuration. It then adds audio metadata to the catalogue (PostgresDB). Audio Analysis \u00b6 Audio Analysis job takes cleaned and processed data from Audio Processor job and performs three type of analysys: Language identification \u00b6 It predicts the language of each utterance in a source using a pre trained model for a language. It gives the confidence score of the language for each utterance. Please see this for more details. Speaker identification \u00b6 It estimates the total number of speakers in a source. It also maps the utterances to the speaker. That metadata is required for data balancing. Please see this for more details. Gender identification \u00b6 It estimates the gender of each utterance in a source using a pre trained model. Please see this for more details. ### Audio Data Balancing The model training data requires data with right ratio of gender. Also the data should be balanced based on speaker duration. It also provides capability to filter and choose data based on certain metadata filter criteria. Audio Validation \u00b6 The data that goes into model training should be of good quality. This job validates data that is not adhereing to quiality standards required by the model. It generates csv reports that can be analysed by data scientists to further filter out the best data for model training. Audio Transcription \u00b6 For model fine-tuning, the paired audio data is required (audio with labeled text). This job generates text for the utterances using Google or Azure API's. The texts generated are further sanitized based on the rules defined for the language. Installation \u00b6 Clone the repo git clone git@github.com:Open-Speech-EkStep/audio-to-speech-pipeline.git Install python requirements pip install -r requirements.txt Run on Kubernetes \u00b6 Using Composer \u00b6 Requirements \u00b6 Terraform https://www.terraform.io/downloads.html gcloud https://cloud.google.com/sdk/docs/install Infra Setup \u00b6 Clone the repo: sh git clone git@github.com:Open-Speech-EkStep/audio-to-speech-pipeline.git Initialize terraform modules terraform init Select a workspace as per the environments(dev,test,prod). terraform workspace select <env_name> eg: terraform workspace select prod Configure variable \"project\" { descrip t io n = \"The name of the Google Cloud Project.\" de fault = \"<project-name>\" } variable \"composer_env_name\" { descrip t io n = \"The name of the Google composer_env_name.\" de fault = \"ekstepcomposer\" } variable \"script_path\" { descrip t io n = \"The path of the working dir.\" de fault = \"./modules/gcp-composer/\" } variable \"bucket_name\" { descrip t io n = \"The name of the gcp bucket\" de fault = \"<bucket-name>\" } variable \"database_version\" { descrip t io n = \"The name of the database_version.\" t ype = s tr i n g de fault = \"POSTGRES_11\" } variable \"database_instance_name\" { descrip t io n = \"The name of the database_instance.\" t ype = s tr i n g de fault = \"<db-instance-name>\" } variable \"db_region\" { descrip t io n = \"The name of the db region.\" t ype = s tr i n g de fault = \"us-central1\" } variable \"database1\" { descrip t io n = \"The name of the database1.\" t ype = s tr i n g de fault = \"speech_recognition_data_catalog-1\" } variable \"database2\" { descrip t io n = \"The name of the database2.\" t ype = s tr i n g de fault = \"speech_recognition_data_catalog-2\" } variable \"speechrecognition_service_account\" { descrip t io n = \"The name of the speechrecognition_service_account.\" t ype = s tr i n g de fault = \"service-account-1\" } variable \"circleci_service_account\" { descrip t io n = \"The name of the circleci_service_account.\" t ype = s tr i n g de fault = \"servacct-circleci\" } variable \"sql_instance_size\" { de fault = \"db-custom-2-7680\" t ype = s tr i n g descrip t io n = \"Size of Cloud SQL instances\" } variable \"sql_disk_type\" { de fault = \"PD_HDD\" t ype = s tr i n g descrip t io n = \"Cloud SQL instance disk type\" } variable \"sql_disk_size\" { de fault = \"20\" t ype = s tr i n g descrip t io n = \"Storage size in GB\" } Create Service account : terraform apply -target=module.service-accounts Create keys from console.cloud.google.com Set env variable export GOOGLE_APPLICATION_CREDENTIAL_SERVICE_ACC= </path/to/key.json> Run specific modules as per requirements. terraform apply -target = module.<module-name> eg: terraform apply -target = module.sql-database Run all modules at once. terraform apply Connect to DB from local: Setup proxy ./cloud_sql_proxy -dir = ./cloudsql -instances = <project-id>:<zone>:<db-instance-name> = tcp:5432 Create username and password from console. Then connect to localhost Whitelist composer worker IP in DB Network CI/CD setup \u00b6 Once you pull code you have to configure some variable in your circle-ci . So that while deploying code image should easily push into google container registry. 1. GCP_PROJECT # Name of your GCP project 2. GOOGLE_AUTH # Service account key that is created using terraform 3. POSTGRES_DB # Database host ip that is created using terraform 4. POSTGRES_PASSWORD # Database password 5. POSTGRES_USER # Database user name 6. DB_INSTANCE # Database instance name Audio Processing \u00b6 Description \u00b6 Config \u00b6 config : common : db_configuration : db_name : '' db_pass : '' db_user : '' cloud_sql_connection_name : '<DB Host>' gcs_config : # master data bucket master_bucket : '<Name of the bucket>' audio_processor_config : # feat_language_identification should true if you want run language identification for a source feat_language_identification : False # language of the audio language : '' # path of the files on gcs which need to be processed # path eg: <bucket-name/data/audiotospeech/raw/download/downloaded/{language}/audio> remote_raw_audio_file_path : '' # after processing where we want to move raw data snr_done_folder_path : '' # <bucket-name/data/audiotospeech/raw/download/snr_done/{language}/audio> # path where the processed files need to be uploaded remote_processed_audio_file_path : '' # <bucket-name/data/audiotospeech/raw/download/catalogue/{language}/audio> # path where Duplicate files need to be uploaded based on checksum duplicate_audio_file_path : '' # <bucket-name/data/audiotospeech/raw/download/duplicate/{language}/audio> chunking_conversion_configuration : aggressiveness : '' # using for vad by default it's value is 2 the more the value that aggressive vad for chunking audio max_duration : '' # max duration is second if chunk is more than that vad will retry chunking with inc aggressiveness # SNR specific configurations snr_configuration : max_snr_threshold : '' # less than max_snr_threshold utterance will move to rejected folder. local_input_file_path : '' local_output_file_path : '' Steps to run \u00b6 We have to configure sourcepathforsnr in airflow variable where our raw data stored. Other variable is snrcatalogue in that we update our source which we want to run and count how many file should run in one trigger.and format is what raw audio file format in bucket and language and parallelism is how many pod will up in one run if parallelism is not define number of pod = count ex: ```json \"snrcatalogue\": { \" \": { \"count\": 5, \"format\": \"mp3\", \"language\": \"telugu\", \"parallelism\":2 } * We have to also set **audiofilelist** with whatever source we want to run with empty array that will store our file path ex: ```json \"audiofilelist\": { \"<source_name>\": [] } ``` * That will create a dag with the source_name now we can trigger that dag that will process given number(count) of file. and upload processed file to **remote_processed_audio_file_path** that we mentioned in config file. and move raw data from **remote_raw_audio_file_path** to **snr_done_folder_path**. and update DB also with the metadata which we created using circle-ci. ### Audio Analysis #### Config ```yaml audio_analysis_config: analysis_options: gender_analysis: 1 # It should be 1 or 0 if you want to run gender_analysis it should be 1 else 0 speaker_analysis: 0 # It should be 1 or 0 if you want to run speaker_analysis it should be 1 else 0 # path where the processed files need to be uploaded remote_processed_audio_file_path: '<bucket_name>/data/audiotospeech/raw/download/catalogued/{language}/audio' # speaker_analysis_config it's for gender_analysis module speaker_analysis_config: min_cluster_size: 4 # min_cluster_size is least number of cluster for one speaker partial_set_size: 8000 # number of utterances for create embeddings for a given source fit_noise_on_similarity: 0.77 min_samples: 2 ``` #### Steps to run * We have to configure **audio_analysis_config** in airflow variable in this json we have to mention source name and language. ```json \"audio_analysis_config\" : { \"<source name>\" : { \"language\" : \"hindi\" } } That will create a dag audio_analysis now we can trigger that dag that will process given sources. and upload processed file to remote_processed_audio_file_path that we mentioned in config file. and update DB also with the metadata which we created using circle-ci. Data Balancing \u00b6 config \u00b6 data_tagger_config : # path of to the folder in the master bucket where the data tagger will move the data to landing_directory_path : '' #'<bucket_name>/data/audiotospeech/raw/download/catalogued/{language}/audio' # path of to the folder in the master bucket from where the data tagger will pick up the data that needs to be moved source_directory_path : '' #'<bucket_name>/data/audiotospeech/raw/landing/{language}/audio' steps to run: \u00b6 We need to configure data_filter_config airflow variable for each source. we have multiple filters by_snr # filter based on SNR value by_duration # total duration from a given source. by_speaker # we can configure how much data per speaker we want. by_utterance_duration # we can required duration of utterance. exclude_audio_ids # we can pass a list of audio_ids that we want to skip. exclude_speaker_ids # we can pass a list of speaker_ids that we want to skip. with_randomness # It is a boolean value if it's it will pickup random data from DB. \"data_filter_config\" : { \"test_source1\" : { \"language\" : \"hindi\" , \"filter\" : { \"by_snr\" : { \"lte\" : 75 , \"gte\" : 15 }, \"by_duration\" : 2 , \"with_randomness\" : \"true\" } }, \"test_source2\" : { \"language\" : \"hindi\" , \"filter\" : { \"by_speaker\" : { \"lte_per_speaker_duration\" : 60 , \"gte_per_speaker_duration\" : 0 , \"with_threshold\" : 0 }, \"by_duration\" : 2 } } After configure all value one dag will created data_marker_pipeline we can trigger that dag. this dag filter out all data from given criteria It will pick data from source_directory_path and after filtering move data to landing_directory_path . Audio Transcription (with config): \u00b6 config: \u00b6 config: common: db_configuration: db_name: '' db_pass: '' db_user: '' cloud_sql_connection_name: '<DB host>' gcs_config: # master data bucket master_bucket: '<bucket name>' azure_transcription_client: speech_key: '<key of the api>' service_region: 'centralindia' # service region google_transcription_client: bucket: '<bucket name>' language: 'hi-IN' # It is BCP-47 language tag with this we call STT api. sample_rate: 16000 # Sample rate of audio utterance audio_channel_count: 1 #The number of channels in the input audio data audio_transcription_config: # defaults to hi-IN language: 'hi-IN' # language # audio_language it's used for sanitization rule whichever language you choose you need to add a rule class for the same. # You can use reference of hindi sanitization # sanitization rule eg: empty transcription, strip, char etc audio_language: 'kannada' # Bucket bath of wav file remote_clean_audio_file_path: '<bucketname>/data/audiotospeech/raw/landing/{language}/audio' # path where the processed files need to be uploaded remote_stt_audio_file_path: '<bucketname>/data/audiotospeech/integration/processed/{language}/audio' steps to run: \u00b6 We have to configure sttsourcepath in airflow variable where our raw data stored. Other variable is sourceinfo in that we update our source which we want to run for STT and count how many file should run in one trigger.stt is whatever api we want to call for STT for google and azure we have all rapper for other API you can add rapper as well. language and parallelism is how many pod will up in one run if parallelism is not define number of pod = count ex: \"snrcatalogue\" : { \"<source_name>\" : { \"count\" : 5 , \"stt\" : \"google\" \"language\" : \"telugu\" , \"parallelism\" : 2 } We have to also set audioidsforstt and integrationprocessedpath with whatever source we want to run with empty array that will store audio_id ex: \"audioidsforstt\" : { \"<source_name>\" : [] } integrationprocessedpath:\"\" # path of folder where we want move transcribed data. That will create a dag with the source_name now we can trigger that dag that will process given number(count) of file. and upload processed file to remote_stt_audio_file_path that we mentioned in config file. and move raw data from remote_clean_audio_file_path to integrationprocessedpath . and update DB also with the metadata which we created using circle-ci. Contributing \u00b6 Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request We follow conventional commits License \u00b6 Distributed under the [MIT] License. See LICENSE for more information. Git Repository \u00b6 https://github.com/Open-Speech-EkStep/audio-to-speech-pipeline Contact \u00b6 Connect with community on Gitter","title":"Intelligent Data Pipeline"},{"location":"intelligent_data_pipelines/#intelligent-data-pipeline","text":"","title":"Intelligent Data Pipeline"},{"location":"intelligent_data_pipelines/#table-of-contents","text":"Intelligent Data Pipeline Table of Contents About The Project Getting Started Architecture Intelligent Data Pipeline - Jobs Audio Processor Audio Analysis Language identification Speaker identification Gender identification Audio Data Balancing Audio Validation Audio Transcription Installation Run on Kubernetes Using Composer Requirements Infra Setup CI/CD setup Audio Processing Description Config Steps to run Audio Analysis Config Steps to run Data Balancing config steps to run: Audio Transcription (with config): config: steps to run: Contributing License Git Repository Contact","title":"Table of Contents"},{"location":"intelligent_data_pipelines/#about-the-project","text":"Intelligent Data Pipelines are built to create the audio data set that can be used for Speech Recognition deeplearning models. The aim is to allow easy, quick and fast dataset generation without doing manual work. It splits data into smallers utterences which are understood well by deeplearning models. The data is then cleansed based on 'Signal to Noise ratio'. The audio analysis is performed using pre trained models and clustering based on audio features (see Resymbleyr for more details). It leverages Kubernetes for parallel computing and below are the metrics we have acheived so far: Some stats for a language with 1000 hrs raw data Raw data 1000 hrs Time taken: 2-3 days Final Usable Data of Pretraining: 600 Final Usable Data of Fine Tuning: 400","title":"About The Project"},{"location":"intelligent_data_pipelines/#getting-started","text":"The developer documentation helps you to get familiar with the bare necessities, giving you a quick and clean approach to get you up and running. If you are looking for ways to customize the workflow, or just breaking things down to build them back up, head to the reference section to dig into the mechanics of Data Pipelines. To get started install the prerequisites and clone the repo to machine on which you wish to run the framework. Here is the code","title":"Getting Started"},{"location":"intelligent_data_pipelines/#architecture","text":"","title":"Architecture"},{"location":"intelligent_data_pipelines/#intelligent-data-pipeline-jobs","text":"","title":"Intelligent Data Pipeline - Jobs"},{"location":"intelligent_data_pipelines/#audio-processor","text":"Audio Processor job takes raw data generted from Data Collection Pipeline or it also consumes data that is generated by any other means. It splits data into smaller utterances and then cleanses based on 'Signal to Noise Ratio (SNR)'. The threshold can be changed through configuration. It then adds audio metadata to the catalogue (PostgresDB).","title":"Audio Processor"},{"location":"intelligent_data_pipelines/#audio-analysis","text":"Audio Analysis job takes cleaned and processed data from Audio Processor job and performs three type of analysys:","title":"Audio Analysis"},{"location":"intelligent_data_pipelines/#language-identification","text":"It predicts the language of each utterance in a source using a pre trained model for a language. It gives the confidence score of the language for each utterance. Please see this for more details.","title":"Language identification"},{"location":"intelligent_data_pipelines/#speaker-identification","text":"It estimates the total number of speakers in a source. It also maps the utterances to the speaker. That metadata is required for data balancing. Please see this for more details.","title":"Speaker identification"},{"location":"intelligent_data_pipelines/#gender-identification","text":"It estimates the gender of each utterance in a source using a pre trained model. Please see this for more details. ### Audio Data Balancing The model training data requires data with right ratio of gender. Also the data should be balanced based on speaker duration. It also provides capability to filter and choose data based on certain metadata filter criteria.","title":"Gender identification"},{"location":"intelligent_data_pipelines/#audio-validation","text":"The data that goes into model training should be of good quality. This job validates data that is not adhereing to quiality standards required by the model. It generates csv reports that can be analysed by data scientists to further filter out the best data for model training.","title":"Audio Validation"},{"location":"intelligent_data_pipelines/#audio-transcription","text":"For model fine-tuning, the paired audio data is required (audio with labeled text). This job generates text for the utterances using Google or Azure API's. The texts generated are further sanitized based on the rules defined for the language.","title":"Audio Transcription"},{"location":"intelligent_data_pipelines/#installation","text":"Clone the repo git clone git@github.com:Open-Speech-EkStep/audio-to-speech-pipeline.git Install python requirements pip install -r requirements.txt","title":"Installation"},{"location":"intelligent_data_pipelines/#run-on-kubernetes","text":"","title":"Run on Kubernetes"},{"location":"intelligent_data_pipelines/#using-composer","text":"","title":"Using Composer"},{"location":"intelligent_data_pipelines/#requirements","text":"Terraform https://www.terraform.io/downloads.html gcloud https://cloud.google.com/sdk/docs/install","title":"Requirements"},{"location":"intelligent_data_pipelines/#infra-setup","text":"Clone the repo: sh git clone git@github.com:Open-Speech-EkStep/audio-to-speech-pipeline.git Initialize terraform modules terraform init Select a workspace as per the environments(dev,test,prod). terraform workspace select <env_name> eg: terraform workspace select prod Configure variable \"project\" { descrip t io n = \"The name of the Google Cloud Project.\" de fault = \"<project-name>\" } variable \"composer_env_name\" { descrip t io n = \"The name of the Google composer_env_name.\" de fault = \"ekstepcomposer\" } variable \"script_path\" { descrip t io n = \"The path of the working dir.\" de fault = \"./modules/gcp-composer/\" } variable \"bucket_name\" { descrip t io n = \"The name of the gcp bucket\" de fault = \"<bucket-name>\" } variable \"database_version\" { descrip t io n = \"The name of the database_version.\" t ype = s tr i n g de fault = \"POSTGRES_11\" } variable \"database_instance_name\" { descrip t io n = \"The name of the database_instance.\" t ype = s tr i n g de fault = \"<db-instance-name>\" } variable \"db_region\" { descrip t io n = \"The name of the db region.\" t ype = s tr i n g de fault = \"us-central1\" } variable \"database1\" { descrip t io n = \"The name of the database1.\" t ype = s tr i n g de fault = \"speech_recognition_data_catalog-1\" } variable \"database2\" { descrip t io n = \"The name of the database2.\" t ype = s tr i n g de fault = \"speech_recognition_data_catalog-2\" } variable \"speechrecognition_service_account\" { descrip t io n = \"The name of the speechrecognition_service_account.\" t ype = s tr i n g de fault = \"service-account-1\" } variable \"circleci_service_account\" { descrip t io n = \"The name of the circleci_service_account.\" t ype = s tr i n g de fault = \"servacct-circleci\" } variable \"sql_instance_size\" { de fault = \"db-custom-2-7680\" t ype = s tr i n g descrip t io n = \"Size of Cloud SQL instances\" } variable \"sql_disk_type\" { de fault = \"PD_HDD\" t ype = s tr i n g descrip t io n = \"Cloud SQL instance disk type\" } variable \"sql_disk_size\" { de fault = \"20\" t ype = s tr i n g descrip t io n = \"Storage size in GB\" } Create Service account : terraform apply -target=module.service-accounts Create keys from console.cloud.google.com Set env variable export GOOGLE_APPLICATION_CREDENTIAL_SERVICE_ACC= </path/to/key.json> Run specific modules as per requirements. terraform apply -target = module.<module-name> eg: terraform apply -target = module.sql-database Run all modules at once. terraform apply Connect to DB from local: Setup proxy ./cloud_sql_proxy -dir = ./cloudsql -instances = <project-id>:<zone>:<db-instance-name> = tcp:5432 Create username and password from console. Then connect to localhost Whitelist composer worker IP in DB Network","title":"Infra Setup"},{"location":"intelligent_data_pipelines/#cicd-setup","text":"Once you pull code you have to configure some variable in your circle-ci . So that while deploying code image should easily push into google container registry. 1. GCP_PROJECT # Name of your GCP project 2. GOOGLE_AUTH # Service account key that is created using terraform 3. POSTGRES_DB # Database host ip that is created using terraform 4. POSTGRES_PASSWORD # Database password 5. POSTGRES_USER # Database user name 6. DB_INSTANCE # Database instance name","title":"CI/CD setup"},{"location":"intelligent_data_pipelines/#audio-processing","text":"","title":"Audio Processing"},{"location":"intelligent_data_pipelines/#description","text":"","title":"Description"},{"location":"intelligent_data_pipelines/#config","text":"config : common : db_configuration : db_name : '' db_pass : '' db_user : '' cloud_sql_connection_name : '<DB Host>' gcs_config : # master data bucket master_bucket : '<Name of the bucket>' audio_processor_config : # feat_language_identification should true if you want run language identification for a source feat_language_identification : False # language of the audio language : '' # path of the files on gcs which need to be processed # path eg: <bucket-name/data/audiotospeech/raw/download/downloaded/{language}/audio> remote_raw_audio_file_path : '' # after processing where we want to move raw data snr_done_folder_path : '' # <bucket-name/data/audiotospeech/raw/download/snr_done/{language}/audio> # path where the processed files need to be uploaded remote_processed_audio_file_path : '' # <bucket-name/data/audiotospeech/raw/download/catalogue/{language}/audio> # path where Duplicate files need to be uploaded based on checksum duplicate_audio_file_path : '' # <bucket-name/data/audiotospeech/raw/download/duplicate/{language}/audio> chunking_conversion_configuration : aggressiveness : '' # using for vad by default it's value is 2 the more the value that aggressive vad for chunking audio max_duration : '' # max duration is second if chunk is more than that vad will retry chunking with inc aggressiveness # SNR specific configurations snr_configuration : max_snr_threshold : '' # less than max_snr_threshold utterance will move to rejected folder. local_input_file_path : '' local_output_file_path : ''","title":"Config"},{"location":"intelligent_data_pipelines/#steps-to-run","text":"We have to configure sourcepathforsnr in airflow variable where our raw data stored. Other variable is snrcatalogue in that we update our source which we want to run and count how many file should run in one trigger.and format is what raw audio file format in bucket and language and parallelism is how many pod will up in one run if parallelism is not define number of pod = count ex: ```json \"snrcatalogue\": { \" \": { \"count\": 5, \"format\": \"mp3\", \"language\": \"telugu\", \"parallelism\":2 } * We have to also set **audiofilelist** with whatever source we want to run with empty array that will store our file path ex: ```json \"audiofilelist\": { \"<source_name>\": [] } ``` * That will create a dag with the source_name now we can trigger that dag that will process given number(count) of file. and upload processed file to **remote_processed_audio_file_path** that we mentioned in config file. and move raw data from **remote_raw_audio_file_path** to **snr_done_folder_path**. and update DB also with the metadata which we created using circle-ci. ### Audio Analysis #### Config ```yaml audio_analysis_config: analysis_options: gender_analysis: 1 # It should be 1 or 0 if you want to run gender_analysis it should be 1 else 0 speaker_analysis: 0 # It should be 1 or 0 if you want to run speaker_analysis it should be 1 else 0 # path where the processed files need to be uploaded remote_processed_audio_file_path: '<bucket_name>/data/audiotospeech/raw/download/catalogued/{language}/audio' # speaker_analysis_config it's for gender_analysis module speaker_analysis_config: min_cluster_size: 4 # min_cluster_size is least number of cluster for one speaker partial_set_size: 8000 # number of utterances for create embeddings for a given source fit_noise_on_similarity: 0.77 min_samples: 2 ``` #### Steps to run * We have to configure **audio_analysis_config** in airflow variable in this json we have to mention source name and language. ```json \"audio_analysis_config\" : { \"<source name>\" : { \"language\" : \"hindi\" } } That will create a dag audio_analysis now we can trigger that dag that will process given sources. and upload processed file to remote_processed_audio_file_path that we mentioned in config file. and update DB also with the metadata which we created using circle-ci.","title":"Steps to run"},{"location":"intelligent_data_pipelines/#data-balancing","text":"","title":"Data Balancing"},{"location":"intelligent_data_pipelines/#config_1","text":"data_tagger_config : # path of to the folder in the master bucket where the data tagger will move the data to landing_directory_path : '' #'<bucket_name>/data/audiotospeech/raw/download/catalogued/{language}/audio' # path of to the folder in the master bucket from where the data tagger will pick up the data that needs to be moved source_directory_path : '' #'<bucket_name>/data/audiotospeech/raw/landing/{language}/audio'","title":"config"},{"location":"intelligent_data_pipelines/#steps-to-run_1","text":"We need to configure data_filter_config airflow variable for each source. we have multiple filters by_snr # filter based on SNR value by_duration # total duration from a given source. by_speaker # we can configure how much data per speaker we want. by_utterance_duration # we can required duration of utterance. exclude_audio_ids # we can pass a list of audio_ids that we want to skip. exclude_speaker_ids # we can pass a list of speaker_ids that we want to skip. with_randomness # It is a boolean value if it's it will pickup random data from DB. \"data_filter_config\" : { \"test_source1\" : { \"language\" : \"hindi\" , \"filter\" : { \"by_snr\" : { \"lte\" : 75 , \"gte\" : 15 }, \"by_duration\" : 2 , \"with_randomness\" : \"true\" } }, \"test_source2\" : { \"language\" : \"hindi\" , \"filter\" : { \"by_speaker\" : { \"lte_per_speaker_duration\" : 60 , \"gte_per_speaker_duration\" : 0 , \"with_threshold\" : 0 }, \"by_duration\" : 2 } } After configure all value one dag will created data_marker_pipeline we can trigger that dag. this dag filter out all data from given criteria It will pick data from source_directory_path and after filtering move data to landing_directory_path .","title":"steps to run:"},{"location":"intelligent_data_pipelines/#audio-transcription-with-config","text":"","title":"Audio Transcription (with config):"},{"location":"intelligent_data_pipelines/#config_2","text":"config: common: db_configuration: db_name: '' db_pass: '' db_user: '' cloud_sql_connection_name: '<DB host>' gcs_config: # master data bucket master_bucket: '<bucket name>' azure_transcription_client: speech_key: '<key of the api>' service_region: 'centralindia' # service region google_transcription_client: bucket: '<bucket name>' language: 'hi-IN' # It is BCP-47 language tag with this we call STT api. sample_rate: 16000 # Sample rate of audio utterance audio_channel_count: 1 #The number of channels in the input audio data audio_transcription_config: # defaults to hi-IN language: 'hi-IN' # language # audio_language it's used for sanitization rule whichever language you choose you need to add a rule class for the same. # You can use reference of hindi sanitization # sanitization rule eg: empty transcription, strip, char etc audio_language: 'kannada' # Bucket bath of wav file remote_clean_audio_file_path: '<bucketname>/data/audiotospeech/raw/landing/{language}/audio' # path where the processed files need to be uploaded remote_stt_audio_file_path: '<bucketname>/data/audiotospeech/integration/processed/{language}/audio'","title":"config:"},{"location":"intelligent_data_pipelines/#steps-to-run_2","text":"We have to configure sttsourcepath in airflow variable where our raw data stored. Other variable is sourceinfo in that we update our source which we want to run for STT and count how many file should run in one trigger.stt is whatever api we want to call for STT for google and azure we have all rapper for other API you can add rapper as well. language and parallelism is how many pod will up in one run if parallelism is not define number of pod = count ex: \"snrcatalogue\" : { \"<source_name>\" : { \"count\" : 5 , \"stt\" : \"google\" \"language\" : \"telugu\" , \"parallelism\" : 2 } We have to also set audioidsforstt and integrationprocessedpath with whatever source we want to run with empty array that will store audio_id ex: \"audioidsforstt\" : { \"<source_name>\" : [] } integrationprocessedpath:\"\" # path of folder where we want move transcribed data. That will create a dag with the source_name now we can trigger that dag that will process given number(count) of file. and upload processed file to remote_stt_audio_file_path that we mentioned in config file. and move raw data from remote_clean_audio_file_path to integrationprocessedpath . and update DB also with the metadata which we created using circle-ci.","title":"steps to run:"},{"location":"intelligent_data_pipelines/#contributing","text":"Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request We follow conventional commits","title":"Contributing"},{"location":"intelligent_data_pipelines/#license","text":"Distributed under the [MIT] License. See LICENSE for more information.","title":"License"},{"location":"intelligent_data_pipelines/#git-repository","text":"https://github.com/Open-Speech-EkStep/audio-to-speech-pipeline","title":"Git Repository"},{"location":"intelligent_data_pipelines/#contact","text":"Connect with community on Gitter","title":"Contact"},{"location":"language_identification/","text":"Language Identification \u00b6 Table of Contents \u00b6 Language Identification Table of Contents About The Project Working Training Train data Results Usage Preparing the Data Training the Model Inference About The Project \u00b6 Language Identification works for classifying the audio utterances into different classes. This repository can work for 2 or more classes depending on the requirement. This utility is a part of our Intelligent Data Pipelines. For all the data crawled, we can't always be a 100% sure that it only belongs to the language we want to train our Speech Recognition model on. Thus we train a language identification model on a different, limited set of data, to aid us in finding (or eliminating) files that have high content of foreign language. Since we also pass the datasets used for training LID through our pipelines, we can assume that VAD has split all original audios into small chunks with minimum overlap between languages, for training.Another assumption is that the training data comes from a wide variety of environments. We train a ResNet18 model on the spectrograms extracted from audio utterances. During inference, we return a confidence score value for each class (provided by user during training) for an input audio. Working \u00b6 Training \u00b6 We extract spectrograms from audio utterances and use them to train a CNN based classifier: ResNet18, which is 18 layers deep. We don't use the pretrained version, and treat Language Identification as an image classification problem. Users can decide upon the number of classes they want to train on. We follow a one vs other approach where main language is the one we want to classify against. Outputs for each audio are confidence scores for different labels. Train data \u00b6 We trained a Tamil vs others classification model, with 30-40 hours data for each of the classes- Tamil and Other. Validation data consisted of roughly 14 hours of data, balanced between the classes. Usage \u00b6 Preparing the Data \u00b6 Keep separate audio folders for different classes as well as the train and valid sets of each. The audio files should be present in .wav format. To prepare the data, edit the data paths in file data/create_manifest.py. To run the file: python create_manifest . py This creates the train and valid csv files in the data/ directory. Training the Model \u00b6 Edit the train_config.yml file for the training parameters. Give the file path for train and valid csv's created while preparing the data. To start the training, run: python train . py Inference \u00b6 Edit the language_map.yml to map the labels(0,1, etc) with the languege names or codes('hi','en', etc) To infer, edit inference.py file and provide the best_checkpoint path and audio file name. Parameters: model_path : Path to best_checkpoint.pt audio_path : Audio file path Run the file: python inference . py This runs on a single audio file.","title":"Language Identification"},{"location":"language_identification/#language-identification","text":"","title":"Language Identification"},{"location":"language_identification/#table-of-contents","text":"Language Identification Table of Contents About The Project Working Training Train data Results Usage Preparing the Data Training the Model Inference","title":"Table of Contents"},{"location":"language_identification/#about-the-project","text":"Language Identification works for classifying the audio utterances into different classes. This repository can work for 2 or more classes depending on the requirement. This utility is a part of our Intelligent Data Pipelines. For all the data crawled, we can't always be a 100% sure that it only belongs to the language we want to train our Speech Recognition model on. Thus we train a language identification model on a different, limited set of data, to aid us in finding (or eliminating) files that have high content of foreign language. Since we also pass the datasets used for training LID through our pipelines, we can assume that VAD has split all original audios into small chunks with minimum overlap between languages, for training.Another assumption is that the training data comes from a wide variety of environments. We train a ResNet18 model on the spectrograms extracted from audio utterances. During inference, we return a confidence score value for each class (provided by user during training) for an input audio.","title":"About The Project"},{"location":"language_identification/#working","text":"","title":"Working"},{"location":"language_identification/#training","text":"We extract spectrograms from audio utterances and use them to train a CNN based classifier: ResNet18, which is 18 layers deep. We don't use the pretrained version, and treat Language Identification as an image classification problem. Users can decide upon the number of classes they want to train on. We follow a one vs other approach where main language is the one we want to classify against. Outputs for each audio are confidence scores for different labels.","title":"Training"},{"location":"language_identification/#train-data","text":"We trained a Tamil vs others classification model, with 30-40 hours data for each of the classes- Tamil and Other. Validation data consisted of roughly 14 hours of data, balanced between the classes.","title":"Train data"},{"location":"language_identification/#usage","text":"","title":"Usage"},{"location":"language_identification/#preparing-the-data","text":"Keep separate audio folders for different classes as well as the train and valid sets of each. The audio files should be present in .wav format. To prepare the data, edit the data paths in file data/create_manifest.py. To run the file: python create_manifest . py This creates the train and valid csv files in the data/ directory.","title":"Preparing the Data"},{"location":"language_identification/#training-the-model","text":"Edit the train_config.yml file for the training parameters. Give the file path for train and valid csv's created while preparing the data. To start the training, run: python train . py","title":"Training the Model"},{"location":"language_identification/#inference","text":"Edit the language_map.yml to map the labels(0,1, etc) with the languege names or codes('hi','en', etc) To infer, edit inference.py file and provide the best_checkpoint path and audio file name. Parameters: model_path : Path to best_checkpoint.pt audio_path : Audio file path Run the file: python inference . py This runs on a single audio file.","title":"Inference"},{"location":"model_api/","text":"Speech Recognition model API \u00b6 About The Project \u00b6 Our speech to text interface enables you to accurately convert speech into text using an API powered by deep learning neural network algorithms for automatic speech recognition (ASR). This is enabled to provide the following features: Speech to text transcription support for a growing list of indic languages. Transcribe your content in real time from stored files or audio bytes. Generate subtitle or transcript for your audios as per your choice of output. Support for various audio formats like WAV,MP3,PCM. [beta]Enables transcription optimized for domain-specific quality requirements associating domain models in backend. [beta]Speech-to-Text accurately punctuates transcriptions (e.g., commas, question marks, and periods). The Developer documentation provides you with a complete set of guidelines which you need to get started with: Architecture overview API reference Client Code reference Setup and getting started guide Extend this project Contribute to this project Architecture Overview \u00b6 The logical architecture here is built with a grpc server hosting our speech recognition models and dependencies, which cna be run in any environment or docker. With gRPC we can define our service once in a .proto file and generate clients and servers in any of gRPC\u2019s supported languages, which in turn can be run in environments ranging from servers inside a large data center to your own tablet \u2014 all the complexity of communication between different languages and environments is handled for you by gRPC. We also get all the advantages of working with protocol buffers, including efficient serialization, a simple IDL, and easy interface updating. In gRPC, a client application can directly call a method on a server application on a different machine as if it were a local object, making it easier for you to create distributed applications and services. Here we can use the grpc generated stubs from client code in any language and make requests using gRPC calls and receive the responses from the server.On the server side, the server implements this interface and runs a gRPC server to handle client calls. By default, gRPC uses Protocol Buffers, Google\u2019s mature open source mechanism for serializing structured data (although it can be used with other data formats such as JSON). gRPC uses protoc with a special gRPC plugin to generate code from your proto file: you get generated gRPC client and server code, as well as the regular protocol buffer code for populating, serializing, and retrieving your message types. Apart from using gRPC stubs, we have added the support for REST calls to the gRPC server via an api-gateway. With API Gateway for gRPC, you can use the API management capabilities of API Gateway to add monitoring, hosting, tracing, authentication, and more to your gRPC services on Cloud Run. In addition, once you specify special mapping rules, API Gateway translates RESTful JSON over HTTP into gRPC requests. This means that you can deploy a gRPC server managed by API Gateway and call its API using a gRPC or JSON/HTTP client, giving you much more flexibility and ease of integration with other systems. API reference \u00b6 Our API has predictable resource-oriented URLs, accepts form-encoded request bodies, returns JSON-encoded responses, and uses standard HTTP response codes, authentication, and verbs. Base URL https://<gateway-url>/v1/recognize/ Authentication Authentication to the API is performed via HTTP Basic Auth. Provide your API key as the basic auth username value. You do not need to provide a password. The Stripe API uses API keys to authenticate requests. If you need to authenticate via bearer auth (e.g., for a cross-origin request), use -H \"Authorization: Bearer sk_test_4eC39HqLyjWDarjtT1zdp7dc\" As of now all API requests can be made over HTTPS or HTTP both. Calls made over plain HTTP will fail going ahead. Errors Our API uses HTTP response codes to indicate the success or failure of an API request. 200 - OK Everything worked as expected. 400 - Bad Request The request was unacceptable, often due to missing a required parameter. 401 - Unauthorized No valid API key provided. 402 - Request Failed The parameters were valid but the request failed. 403 - Forbidden The API key doesn't have permissions to perform the request. 404 - Not Found The requested resource doesn't exist. 409 - Conflict The request conflicts with another request (perhaps due to using the same idempotent key). 429 - Too Many Requests Too many requests hit the API too quickly. We recommend an exponential backoff of your requests. 500, 502, 503, 504 - Server Errors Something went wrong on Stripe's end. (These are rare.) Handling errors We are in process of writing code that gracefully handles all possible API exceptions. This is something work in progress and will be available soon. Endpoints Supported The recognize object POST /v1/recognize/ Request Attributes config - Holds the configuration objects for language,transcriptionFormat and audio format. Child Attributes: language : REQUIRED - Specify the value of the language and its attributes Child Attributes: value : string : REQUIRED - Specify a langauge code for the audio transcription. Enum: ['en', 'hi', 'ta', 'te', 'kn', 'or', 'gu', 'en-IN'] transcriptionFormat : string : OPTIONAL - Determine the output format as either SRT or TRANSCRIPT.Default value is TRANSCRIPT. Enum : ['SRT','TRANSCRIPT'] audioFormat : string : OPTIONAL - Determine the input audio formats from the list supported.Default value is WAV. Enum : ['WAV', 'MP3', 'PCM'] audio - Specify the audio properties configuration. Either provide the audio URL or the audio bytes. Child Attributes: Either of the below attributes is REQUIRED. audioUri : string : REQUIRED - Specify the audio URL path audioContent : string : REQUIRED - Specify the byte representation of the audio as part of the request. Request body Example Schema { \"config\": { \"language\": { \"value\": \"hi\" }, \"transcriptionFormat\": \"SRT\", \"audioFormat\": \"WAV\" }, \"audio\": { \"audioUri\": \"https://codmento.com/ekstep/test/changed.wav\" } } Responses Code Description 200 On successful completion of the job. Response Attributes srt : string - The subtitle as output if transcription format is chosen at SRT. transcript : string - The transcript as output if transcription format is chosen at TRANSCRIPT. Response body Example Schema { \"srt\": \"1\\n00:00:01,29 --> 00:00:04,88\\n\u0939\u093f\u0902\u0926\u0940 \u092e\u0949\u0921\u0932 \u091f\u0947\u0938\u094d\u091f \u0915\u0930 \u0930\u0939\u0940 \u0939\u0942\u0902\\n\\n2\\n00:00:05,00 --> 00:00:09,89\\n\u0915\u0948\u0938\u093e \u091a\u0932 \u0930\u0939\u093e \u0939\u0948 \u092e\u0947\u0930\u0947 \u0935\u093f\u0902\u0921\u094b\u091c \u0932\u0948\u092a\u091f\u0949\u092a \u0938\u0947\\n\\n\" } { \"transcript\": \"\u0939\u093f\u0902\u0926\u0940 \u092e\u0949\u0921\u0932 \u091f\u0947\u0938\u094d\u091f \u0915\u0930 \u0930\u0939\u0940 \u0939\u0942\u0902 \u0915\u0948\u0938\u093e \u091a\u0932 \u0930\u0939\u093e \u0939\u0948 \u092e\u0947\u0930\u0947 \u0935\u093f\u0902\u0921\u094b\u091c \u0932\u0948\u092a\u091f\u0949\u092a \u0938\u0947\" } Responses Code Description 400 On input errors causing a failure in the job. Response Attributes code : string - status code as encountered in the processing life-cycle. message : string - human understandable format. Response body Example Schema { \"code\": 400, \"message\": \"config.audioFormat: invalid value \\\"MP4\\\" for type type.googleapis.com/ekstep.speech_recognition.RecognitionConfig.AudioFormat\" } Responses Code Description 500 Internal error causing a failure in the job. Response Attributes code : string - status code as encountered in the processing life-cycle. message : string - human understandable format. Response body Example Schema { \"code\": 2, \"message\": \"Exception calling application: An unknown error has occurred.Please try again.\" } Sample Request curl --location --request POST 'https://<gateway-url>/v1/recognize/' \\ --header 'Content-Type: text/plain' \\ --data-raw '{ \"config\": { \"language\": { \"value\": \"hi\" }, \"transcriptionFormat\": \"SRT\", \"audioFormat\": \"WAV\" }, \"audio\": { \"audioUri\": \"https://codmento.com/ekstep/test/changed.wav\" } }' Sample Response { \"srt\": \"1\\n00:00:01,29 --> 00:00:04,88\\n\u0939\u093f\u0902\u0926\u0940 \u092e\u0949\u0921\u0932 \u091f\u0947\u0938\u094d\u091f \u0915\u0930 \u0930\u0939\u0940 \u0939\u0942\u0902\\n\\n2\\n00:00:05,00 --> 00:00:09,89\\n\u0915\u0948\u0938\u093e \u091a\u0932 \u0930\u0939\u093e \u0939\u0948 \u092e\u0947\u0930\u0947 \u0935\u093f\u0902\u0921\u094b\u091c \u0932\u0948\u092a\u091f\u0949\u092a \u0938\u0947\\n\\n\" } Sample Request curl --location --request POST 'https://<gateway-url>/v1/recognize/' \\ --header 'Content-Type: text/plain' \\ --data-raw '{ \"config\": { \"language\": { \"value\": \"hi\" }, \"transcriptionFormat\": \"TRANSCRIPT\", \"audioFormat\": \"WAV\" }, \"audio\": { \"audioUri\": \"https://codmento.com/ekstep/test/changed.wav\" } }' Sample Response { \"transcript\": \"\u0939\u093f\u0902\u0926\u0940 \u092e\u0949\u0921\u0932 \u091f\u0947\u0938\u094d\u091f \u0915\u0930 \u0930\u0939\u0940 \u0939\u0942\u0902 \u0915\u0948\u0938\u093e \u091a\u0932 \u0930\u0939\u093e \u0939\u0948 \u092e\u0947\u0930\u0947 \u0935\u093f\u0902\u0921\u094b\u091c \u0932\u0948\u092a\u091f\u0949\u092a \u0938\u0947\" } Client Code reference \u00b6 We provide client libraries as stubs for different programming languages. python import grpc from stub.speech_recognition_open_api_pb2_grpc import SpeechRecognizerStub from stub.speech_recognition_open_api_pb2 import Language, RecognitionConfig, RecognitionAudio, \\ SpeechRecognitionRequest import wave from grpc_interceptor import ClientCallDetails, ClientInterceptor class GrpcAuth(grpc.AuthMetadataPlugin): def __init__(self, key): self._key = key def __call__(self, context, callback): callback((('rpc-auth-header', self._key),), None) class MetadataClientInterceptor(ClientInterceptor): def __init__(self, key): self._key = key def intercept( self, method, request_or_iterator, call_details: grpc.ClientCallDetails, ): new_details = ClientCallDetails( call_details.method, call_details.timeout, [(\"authorization\", \"Bearer \" + self._key)], call_details.credentials, call_details.wait_for_ready, call_details.compression, ) return method(request_or_iterator, new_details) def read_audio(): with wave.open('changed.wav', 'rb') as f: return f.readframes(f.getnframes()) def transcribe_audio_bytes(stub): language = \"hi\" audio_bytes = read_audio() lang = Language(value=language, name='Hindi') config = RecognitionConfig(language=lang, audioFormat='WAV', transcriptionFormat='TRANSCRIPT') audio = RecognitionAudio(audioContent=audio_bytes) request = SpeechRecognitionRequest(audio=audio, config=config) # creds = grpc.metadata_call_credentials( # metadata_plugin=GrpcAuth('access_key') # ) response = stub.recognize(request) print(response.transcript) def transcribe_audio_url(stub): language = \"hi\" url = \"https://codmento.com/ekstep/test/changed.wav\" lang = Language(value=language, name='Hindi') config = RecognitionConfig(language=lang, audioFormat='WAV') audio = RecognitionAudio(audioUri=url) request = SpeechRecognitionRequest(audio=audio, config=config) response = stub.recognize(request) print(response.transcript) def get_srt_audio_bytes(stub): language = \"hi\" audio_bytes = read_audio() lang = Language(value=language, name='Hindi') config = RecognitionConfig(language=lang, audioFormat='WAV', transcriptionFormat='SRT') audio = RecognitionAudio(audioContent=audio_bytes) request = SpeechRecognitionRequest(audio=audio, config=config) # creds = grpc.metadata_call_credentials( # metadata_plugin=GrpcAuth('access_key') # ) response = stub.recognize(request) print(response.srt) def get_srt_audio_url(stub): language = \"hi\" url = \"https://codmento.com/ekstep/test/changed.wav\" lang = Language(value=language, name='Hindi') config = RecognitionConfig(language=lang, audioFormat='WAV', transcriptionFormat='SRT') audio = RecognitionAudio(audioUri=url) request = SpeechRecognitionRequest(audio=audio, config=config) response = stub.recognize(request) print(response.srt) if __name__ == '__main__': key = \"mysecrettoken\" interceptors = [MetadataClientInterceptor(key)] with grpc.insecure_channel('34.70.114.226:50051') as channel: channel = grpc.intercept_channel(channel, *interceptors) stub = SpeechRecognizerStub(channel) transcribe_audio_url(stub) transcribe_audio_bytes(stub) get_srt_audio_url(stub) get_srt_audio_bytes(stub) Java package com.ekstep.endpoints.speech_recognition; import com.google.protobuf.ByteString; import io.grpc.Channel; import io.grpc.ManagedChannel; import io.grpc.ManagedChannelBuilder; import io.grpc.StatusRuntimeException; import java.util.concurrent.TimeUnit; import java.util.logging.Level; import java.util.logging.Logger; public class SpeechRecognitionClient { private static final Logger logger = Logger.getLogger(SpeechRecognitionClient.class.getName()); private final SpeechRecognizerGrpc.SpeechRecognizerBlockingStub blockingStub; public SpeechRecognitionClient(Channel channel) { blockingStub = SpeechRecognizerGrpc.newBlockingStub(channel); } public SpeechRecognitionResult transcribeUrlV2() { String audioUrl = \"https://codmento.com/ekstep/test/changed.wav\"; logger.info(\"Will try to request \" + audioUrl + \" ...\"); RecognitionConfig config = RecognitionConfig.newBuilder() .setLanguage(Language.newBuilder().setValue(Language.LanguageCode.hi).build()) .setAudioFormat(RecognitionConfig.AudioFormat.WAV) .build(); RecognitionAudio audio = RecognitionAudio.newBuilder().setAudioUri(audioUrl).build(); SpeechRecognitionRequest request = SpeechRecognitionRequest.newBuilder() .setAudio(audio) .setConfig(config) .build(); SpeechRecognitionResult response; try { response = blockingStub.recognize(request); return response; } catch (StatusRuntimeException e) { logger.log(Level.WARNING, \"RPC failed: {0}\", e.getStatus()); return SpeechRecognitionResult.newBuilder().build(); } } public SpeechRecognitionResult transcribeBytesV2() { logger.info(\"Will try to request ...\"); AudioFiles audioFiles = new AudioFiles(); String file = \"/Users/nireshkumarr/Documents/ekstep/speech-recognition-open-api/examples/python/speech-recognition/changed.wav\"; byte[] data2 = audioFiles.readAudioFileData(file); ByteString byteString = ByteString.copyFrom(data2); RecognitionConfig config = RecognitionConfig.newBuilder() .setLanguage(Language.newBuilder().setValue(Language.LanguageCode.hi).build()) .setAudioFormat(RecognitionConfig.AudioFormat.WAV) .build(); RecognitionAudio audio = RecognitionAudio.newBuilder().setAudioContent(byteString).build(); SpeechRecognitionRequest request = SpeechRecognitionRequest.newBuilder() .setAudio(audio) .setConfig(config) .build(); SpeechRecognitionResult response; try { response = blockingStub.recognize(request); return response; } catch (StatusRuntimeException e) { logger.log(Level.WARNING, \"RPC failed: {0}\", e.getStatus()); return SpeechRecognitionResult.newBuilder().build(); } } public SpeechRecognitionResult srtUrlV2() { String audioUrl = \"https://codmento.com/ekstep/test/changed.wav\"; logger.info(\"Will try to request \" + audioUrl + \" ...\"); RecognitionConfig config = RecognitionConfig.newBuilder() .setLanguage(Language.newBuilder().setValue(Language.LanguageCode.hi).build()) .setAudioFormat(RecognitionConfig.AudioFormat.WAV) .setTranscriptionFormat(RecognitionConfig.TranscriptionFormat.SRT) .build(); RecognitionAudio audio = RecognitionAudio.newBuilder().setAudioUri(audioUrl).build(); SpeechRecognitionRequest request = SpeechRecognitionRequest.newBuilder() .setAudio(audio) .setConfig(config) .build(); SpeechRecognitionResult response; try { response = blockingStub.recognize(request); return response; } catch (StatusRuntimeException e) { logger.log(Level.WARNING, \"RPC failed: {0}\", e.getStatus()); return SpeechRecognitionResult.newBuilder().build(); } } public SpeechRecognitionResult srtBytesV2() { logger.info(\"Will try to request ...\"); AudioFiles audioFiles = new AudioFiles(); String file = \"/Users/nireshkumarr/Documents/ekstep/speech-recognition-open-api/examples/python/speech-recognition/changed.wav\"; byte[] data2 = audioFiles.readAudioFileData(file); ByteString byteString = ByteString.copyFrom(data2); RecognitionConfig config = RecognitionConfig.newBuilder() .setLanguage(Language.newBuilder().setValue(Language.LanguageCode.hi).build()) .setAudioFormat(RecognitionConfig.AudioFormat.WAV) .setTranscriptionFormat(RecognitionConfig.TranscriptionFormat.SRT) .build(); RecognitionAudio audio = RecognitionAudio.newBuilder().setAudioContent(byteString).build(); SpeechRecognitionRequest request = SpeechRecognitionRequest.newBuilder() .setAudio(audio) .setConfig(config) .build(); SpeechRecognitionResult response; try { response = blockingStub.recognize(request); return response; } catch (StatusRuntimeException e) { logger.log(Level.WARNING, \"RPC failed: {0}\", e.getStatus()); return SpeechRecognitionResult.newBuilder().build(); } } public static void main(String[] args) throws Exception { String target = \"34.70.114.226:50051\"; ManagedChannel channel = ManagedChannelBuilder.forTarget(target) .usePlaintext() .build(); try { SpeechRecognitionClient client = new SpeechRecognitionClient(channel); SpeechRecognitionResult srtUrlResponse = client.srtUrlV2() SpeechRecognitionResult srtBytesResponse = client.srtBytesV2(); SpeechRecognitionResult bytesResponse = client.transcribeBytesV2(); SpeechRecognitionResult urlResponse = client.transcribeUrlV2(); System.out.println(bytesResponse.getTranscript()); System.out.println(urlResponse.getTranscript()); System.out.println(srtBytesResponse.getSrt()); System.out.println(srtUrlResponse.getSrt()); } finally { channel.shutdownNow().awaitTermination(5, TimeUnit.SECONDS); } } } Setup and getting started guide \u00b6 Clone our github repo : https://github.com/Open-Speech-EkStep/speech-recognition-open-api.git Setup the grpc server: Without docker 1. Create and activate a new environment : ```conda create --name <env> python=3.8 && conda activate <env>``` Install required libraries using the following command: pip install -r requirements.txt Bootstrap the model code and other models as pre requisites: sh model_bootstrap.sh 4. Download models and update the right model paths in model_dict.json. 5. Start the server at port 50051: python server.py With docker docker build -t speech_recognition_model_api . sudo docker run --cpus=6 -m 20000m -itd -p <<host_port>>:50051 --name speech_recognition_model_api -v <<host_model_path>>/deployed_models:<<container_model_path>>/deployed_models/ -i -t speech_recognition_model_api Using the model api as part of client code: In python, python examples/python/speech-recognition/main.py Using the model api as part of REST call using api-gateway: Create api config in api gateway: gcloud api-gateway api-configs create CONFIG_ID \\ --api=API_ID --project=PROJECT_ID \\ --grpc-files=api_descriptor.pb,api_config.yaml Deploy gateway in api gateway: gcloud api-gateway gateways create GATEWAY_ID \\ --api=API_ID --api-config=CONFIG_ID \\ --location=GCP_REGION --project=PROJECT_ID View gateway information: gcloud api-gateway gateways describe GATEWAY_ID \\ --location=GCP_REGION --project=PROJECT_ID To run tests, use the following command: py.test --grpc-fake-server --ignore=wav2letter --ignore=wav2vec-infer --ignore=kenlm Contributing \u00b6 Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request License \u00b6 Distributed under the [MIT] License. See LICENSE for more information. Git repository \u00b6 https://github.com/Open-Speech-EkStep/speech-recognition-open-api.git Contact \u00b6 Connect with community on Gitter Project Link: https://github.com/Open-Speech-EkStep/speech-recognition-open-api.git","title":"Speech Recognititon Model API"},{"location":"model_api/#speech-recognition-model-api","text":"","title":"Speech Recognition model API"},{"location":"model_api/#about-the-project","text":"Our speech to text interface enables you to accurately convert speech into text using an API powered by deep learning neural network algorithms for automatic speech recognition (ASR). This is enabled to provide the following features: Speech to text transcription support for a growing list of indic languages. Transcribe your content in real time from stored files or audio bytes. Generate subtitle or transcript for your audios as per your choice of output. Support for various audio formats like WAV,MP3,PCM. [beta]Enables transcription optimized for domain-specific quality requirements associating domain models in backend. [beta]Speech-to-Text accurately punctuates transcriptions (e.g., commas, question marks, and periods). The Developer documentation provides you with a complete set of guidelines which you need to get started with: Architecture overview API reference Client Code reference Setup and getting started guide Extend this project Contribute to this project","title":"About The Project"},{"location":"model_api/#architecture-overview","text":"The logical architecture here is built with a grpc server hosting our speech recognition models and dependencies, which cna be run in any environment or docker. With gRPC we can define our service once in a .proto file and generate clients and servers in any of gRPC\u2019s supported languages, which in turn can be run in environments ranging from servers inside a large data center to your own tablet \u2014 all the complexity of communication between different languages and environments is handled for you by gRPC. We also get all the advantages of working with protocol buffers, including efficient serialization, a simple IDL, and easy interface updating. In gRPC, a client application can directly call a method on a server application on a different machine as if it were a local object, making it easier for you to create distributed applications and services. Here we can use the grpc generated stubs from client code in any language and make requests using gRPC calls and receive the responses from the server.On the server side, the server implements this interface and runs a gRPC server to handle client calls. By default, gRPC uses Protocol Buffers, Google\u2019s mature open source mechanism for serializing structured data (although it can be used with other data formats such as JSON). gRPC uses protoc with a special gRPC plugin to generate code from your proto file: you get generated gRPC client and server code, as well as the regular protocol buffer code for populating, serializing, and retrieving your message types. Apart from using gRPC stubs, we have added the support for REST calls to the gRPC server via an api-gateway. With API Gateway for gRPC, you can use the API management capabilities of API Gateway to add monitoring, hosting, tracing, authentication, and more to your gRPC services on Cloud Run. In addition, once you specify special mapping rules, API Gateway translates RESTful JSON over HTTP into gRPC requests. This means that you can deploy a gRPC server managed by API Gateway and call its API using a gRPC or JSON/HTTP client, giving you much more flexibility and ease of integration with other systems.","title":"Architecture Overview"},{"location":"model_api/#api-reference","text":"Our API has predictable resource-oriented URLs, accepts form-encoded request bodies, returns JSON-encoded responses, and uses standard HTTP response codes, authentication, and verbs. Base URL https://<gateway-url>/v1/recognize/ Authentication Authentication to the API is performed via HTTP Basic Auth. Provide your API key as the basic auth username value. You do not need to provide a password. The Stripe API uses API keys to authenticate requests. If you need to authenticate via bearer auth (e.g., for a cross-origin request), use -H \"Authorization: Bearer sk_test_4eC39HqLyjWDarjtT1zdp7dc\" As of now all API requests can be made over HTTPS or HTTP both. Calls made over plain HTTP will fail going ahead. Errors Our API uses HTTP response codes to indicate the success or failure of an API request. 200 - OK Everything worked as expected. 400 - Bad Request The request was unacceptable, often due to missing a required parameter. 401 - Unauthorized No valid API key provided. 402 - Request Failed The parameters were valid but the request failed. 403 - Forbidden The API key doesn't have permissions to perform the request. 404 - Not Found The requested resource doesn't exist. 409 - Conflict The request conflicts with another request (perhaps due to using the same idempotent key). 429 - Too Many Requests Too many requests hit the API too quickly. We recommend an exponential backoff of your requests. 500, 502, 503, 504 - Server Errors Something went wrong on Stripe's end. (These are rare.) Handling errors We are in process of writing code that gracefully handles all possible API exceptions. This is something work in progress and will be available soon. Endpoints Supported The recognize object POST /v1/recognize/ Request Attributes config - Holds the configuration objects for language,transcriptionFormat and audio format. Child Attributes: language : REQUIRED - Specify the value of the language and its attributes Child Attributes: value : string : REQUIRED - Specify a langauge code for the audio transcription. Enum: ['en', 'hi', 'ta', 'te', 'kn', 'or', 'gu', 'en-IN'] transcriptionFormat : string : OPTIONAL - Determine the output format as either SRT or TRANSCRIPT.Default value is TRANSCRIPT. Enum : ['SRT','TRANSCRIPT'] audioFormat : string : OPTIONAL - Determine the input audio formats from the list supported.Default value is WAV. Enum : ['WAV', 'MP3', 'PCM'] audio - Specify the audio properties configuration. Either provide the audio URL or the audio bytes. Child Attributes: Either of the below attributes is REQUIRED. audioUri : string : REQUIRED - Specify the audio URL path audioContent : string : REQUIRED - Specify the byte representation of the audio as part of the request. Request body Example Schema { \"config\": { \"language\": { \"value\": \"hi\" }, \"transcriptionFormat\": \"SRT\", \"audioFormat\": \"WAV\" }, \"audio\": { \"audioUri\": \"https://codmento.com/ekstep/test/changed.wav\" } } Responses Code Description 200 On successful completion of the job. Response Attributes srt : string - The subtitle as output if transcription format is chosen at SRT. transcript : string - The transcript as output if transcription format is chosen at TRANSCRIPT. Response body Example Schema { \"srt\": \"1\\n00:00:01,29 --> 00:00:04,88\\n\u0939\u093f\u0902\u0926\u0940 \u092e\u0949\u0921\u0932 \u091f\u0947\u0938\u094d\u091f \u0915\u0930 \u0930\u0939\u0940 \u0939\u0942\u0902\\n\\n2\\n00:00:05,00 --> 00:00:09,89\\n\u0915\u0948\u0938\u093e \u091a\u0932 \u0930\u0939\u093e \u0939\u0948 \u092e\u0947\u0930\u0947 \u0935\u093f\u0902\u0921\u094b\u091c \u0932\u0948\u092a\u091f\u0949\u092a \u0938\u0947\\n\\n\" } { \"transcript\": \"\u0939\u093f\u0902\u0926\u0940 \u092e\u0949\u0921\u0932 \u091f\u0947\u0938\u094d\u091f \u0915\u0930 \u0930\u0939\u0940 \u0939\u0942\u0902 \u0915\u0948\u0938\u093e \u091a\u0932 \u0930\u0939\u093e \u0939\u0948 \u092e\u0947\u0930\u0947 \u0935\u093f\u0902\u0921\u094b\u091c \u0932\u0948\u092a\u091f\u0949\u092a \u0938\u0947\" } Responses Code Description 400 On input errors causing a failure in the job. Response Attributes code : string - status code as encountered in the processing life-cycle. message : string - human understandable format. Response body Example Schema { \"code\": 400, \"message\": \"config.audioFormat: invalid value \\\"MP4\\\" for type type.googleapis.com/ekstep.speech_recognition.RecognitionConfig.AudioFormat\" } Responses Code Description 500 Internal error causing a failure in the job. Response Attributes code : string - status code as encountered in the processing life-cycle. message : string - human understandable format. Response body Example Schema { \"code\": 2, \"message\": \"Exception calling application: An unknown error has occurred.Please try again.\" } Sample Request curl --location --request POST 'https://<gateway-url>/v1/recognize/' \\ --header 'Content-Type: text/plain' \\ --data-raw '{ \"config\": { \"language\": { \"value\": \"hi\" }, \"transcriptionFormat\": \"SRT\", \"audioFormat\": \"WAV\" }, \"audio\": { \"audioUri\": \"https://codmento.com/ekstep/test/changed.wav\" } }' Sample Response { \"srt\": \"1\\n00:00:01,29 --> 00:00:04,88\\n\u0939\u093f\u0902\u0926\u0940 \u092e\u0949\u0921\u0932 \u091f\u0947\u0938\u094d\u091f \u0915\u0930 \u0930\u0939\u0940 \u0939\u0942\u0902\\n\\n2\\n00:00:05,00 --> 00:00:09,89\\n\u0915\u0948\u0938\u093e \u091a\u0932 \u0930\u0939\u093e \u0939\u0948 \u092e\u0947\u0930\u0947 \u0935\u093f\u0902\u0921\u094b\u091c \u0932\u0948\u092a\u091f\u0949\u092a \u0938\u0947\\n\\n\" } Sample Request curl --location --request POST 'https://<gateway-url>/v1/recognize/' \\ --header 'Content-Type: text/plain' \\ --data-raw '{ \"config\": { \"language\": { \"value\": \"hi\" }, \"transcriptionFormat\": \"TRANSCRIPT\", \"audioFormat\": \"WAV\" }, \"audio\": { \"audioUri\": \"https://codmento.com/ekstep/test/changed.wav\" } }' Sample Response { \"transcript\": \"\u0939\u093f\u0902\u0926\u0940 \u092e\u0949\u0921\u0932 \u091f\u0947\u0938\u094d\u091f \u0915\u0930 \u0930\u0939\u0940 \u0939\u0942\u0902 \u0915\u0948\u0938\u093e \u091a\u0932 \u0930\u0939\u093e \u0939\u0948 \u092e\u0947\u0930\u0947 \u0935\u093f\u0902\u0921\u094b\u091c \u0932\u0948\u092a\u091f\u0949\u092a \u0938\u0947\" }","title":"API reference"},{"location":"model_api/#client-code-reference","text":"We provide client libraries as stubs for different programming languages. python import grpc from stub.speech_recognition_open_api_pb2_grpc import SpeechRecognizerStub from stub.speech_recognition_open_api_pb2 import Language, RecognitionConfig, RecognitionAudio, \\ SpeechRecognitionRequest import wave from grpc_interceptor import ClientCallDetails, ClientInterceptor class GrpcAuth(grpc.AuthMetadataPlugin): def __init__(self, key): self._key = key def __call__(self, context, callback): callback((('rpc-auth-header', self._key),), None) class MetadataClientInterceptor(ClientInterceptor): def __init__(self, key): self._key = key def intercept( self, method, request_or_iterator, call_details: grpc.ClientCallDetails, ): new_details = ClientCallDetails( call_details.method, call_details.timeout, [(\"authorization\", \"Bearer \" + self._key)], call_details.credentials, call_details.wait_for_ready, call_details.compression, ) return method(request_or_iterator, new_details) def read_audio(): with wave.open('changed.wav', 'rb') as f: return f.readframes(f.getnframes()) def transcribe_audio_bytes(stub): language = \"hi\" audio_bytes = read_audio() lang = Language(value=language, name='Hindi') config = RecognitionConfig(language=lang, audioFormat='WAV', transcriptionFormat='TRANSCRIPT') audio = RecognitionAudio(audioContent=audio_bytes) request = SpeechRecognitionRequest(audio=audio, config=config) # creds = grpc.metadata_call_credentials( # metadata_plugin=GrpcAuth('access_key') # ) response = stub.recognize(request) print(response.transcript) def transcribe_audio_url(stub): language = \"hi\" url = \"https://codmento.com/ekstep/test/changed.wav\" lang = Language(value=language, name='Hindi') config = RecognitionConfig(language=lang, audioFormat='WAV') audio = RecognitionAudio(audioUri=url) request = SpeechRecognitionRequest(audio=audio, config=config) response = stub.recognize(request) print(response.transcript) def get_srt_audio_bytes(stub): language = \"hi\" audio_bytes = read_audio() lang = Language(value=language, name='Hindi') config = RecognitionConfig(language=lang, audioFormat='WAV', transcriptionFormat='SRT') audio = RecognitionAudio(audioContent=audio_bytes) request = SpeechRecognitionRequest(audio=audio, config=config) # creds = grpc.metadata_call_credentials( # metadata_plugin=GrpcAuth('access_key') # ) response = stub.recognize(request) print(response.srt) def get_srt_audio_url(stub): language = \"hi\" url = \"https://codmento.com/ekstep/test/changed.wav\" lang = Language(value=language, name='Hindi') config = RecognitionConfig(language=lang, audioFormat='WAV', transcriptionFormat='SRT') audio = RecognitionAudio(audioUri=url) request = SpeechRecognitionRequest(audio=audio, config=config) response = stub.recognize(request) print(response.srt) if __name__ == '__main__': key = \"mysecrettoken\" interceptors = [MetadataClientInterceptor(key)] with grpc.insecure_channel('34.70.114.226:50051') as channel: channel = grpc.intercept_channel(channel, *interceptors) stub = SpeechRecognizerStub(channel) transcribe_audio_url(stub) transcribe_audio_bytes(stub) get_srt_audio_url(stub) get_srt_audio_bytes(stub) Java package com.ekstep.endpoints.speech_recognition; import com.google.protobuf.ByteString; import io.grpc.Channel; import io.grpc.ManagedChannel; import io.grpc.ManagedChannelBuilder; import io.grpc.StatusRuntimeException; import java.util.concurrent.TimeUnit; import java.util.logging.Level; import java.util.logging.Logger; public class SpeechRecognitionClient { private static final Logger logger = Logger.getLogger(SpeechRecognitionClient.class.getName()); private final SpeechRecognizerGrpc.SpeechRecognizerBlockingStub blockingStub; public SpeechRecognitionClient(Channel channel) { blockingStub = SpeechRecognizerGrpc.newBlockingStub(channel); } public SpeechRecognitionResult transcribeUrlV2() { String audioUrl = \"https://codmento.com/ekstep/test/changed.wav\"; logger.info(\"Will try to request \" + audioUrl + \" ...\"); RecognitionConfig config = RecognitionConfig.newBuilder() .setLanguage(Language.newBuilder().setValue(Language.LanguageCode.hi).build()) .setAudioFormat(RecognitionConfig.AudioFormat.WAV) .build(); RecognitionAudio audio = RecognitionAudio.newBuilder().setAudioUri(audioUrl).build(); SpeechRecognitionRequest request = SpeechRecognitionRequest.newBuilder() .setAudio(audio) .setConfig(config) .build(); SpeechRecognitionResult response; try { response = blockingStub.recognize(request); return response; } catch (StatusRuntimeException e) { logger.log(Level.WARNING, \"RPC failed: {0}\", e.getStatus()); return SpeechRecognitionResult.newBuilder().build(); } } public SpeechRecognitionResult transcribeBytesV2() { logger.info(\"Will try to request ...\"); AudioFiles audioFiles = new AudioFiles(); String file = \"/Users/nireshkumarr/Documents/ekstep/speech-recognition-open-api/examples/python/speech-recognition/changed.wav\"; byte[] data2 = audioFiles.readAudioFileData(file); ByteString byteString = ByteString.copyFrom(data2); RecognitionConfig config = RecognitionConfig.newBuilder() .setLanguage(Language.newBuilder().setValue(Language.LanguageCode.hi).build()) .setAudioFormat(RecognitionConfig.AudioFormat.WAV) .build(); RecognitionAudio audio = RecognitionAudio.newBuilder().setAudioContent(byteString).build(); SpeechRecognitionRequest request = SpeechRecognitionRequest.newBuilder() .setAudio(audio) .setConfig(config) .build(); SpeechRecognitionResult response; try { response = blockingStub.recognize(request); return response; } catch (StatusRuntimeException e) { logger.log(Level.WARNING, \"RPC failed: {0}\", e.getStatus()); return SpeechRecognitionResult.newBuilder().build(); } } public SpeechRecognitionResult srtUrlV2() { String audioUrl = \"https://codmento.com/ekstep/test/changed.wav\"; logger.info(\"Will try to request \" + audioUrl + \" ...\"); RecognitionConfig config = RecognitionConfig.newBuilder() .setLanguage(Language.newBuilder().setValue(Language.LanguageCode.hi).build()) .setAudioFormat(RecognitionConfig.AudioFormat.WAV) .setTranscriptionFormat(RecognitionConfig.TranscriptionFormat.SRT) .build(); RecognitionAudio audio = RecognitionAudio.newBuilder().setAudioUri(audioUrl).build(); SpeechRecognitionRequest request = SpeechRecognitionRequest.newBuilder() .setAudio(audio) .setConfig(config) .build(); SpeechRecognitionResult response; try { response = blockingStub.recognize(request); return response; } catch (StatusRuntimeException e) { logger.log(Level.WARNING, \"RPC failed: {0}\", e.getStatus()); return SpeechRecognitionResult.newBuilder().build(); } } public SpeechRecognitionResult srtBytesV2() { logger.info(\"Will try to request ...\"); AudioFiles audioFiles = new AudioFiles(); String file = \"/Users/nireshkumarr/Documents/ekstep/speech-recognition-open-api/examples/python/speech-recognition/changed.wav\"; byte[] data2 = audioFiles.readAudioFileData(file); ByteString byteString = ByteString.copyFrom(data2); RecognitionConfig config = RecognitionConfig.newBuilder() .setLanguage(Language.newBuilder().setValue(Language.LanguageCode.hi).build()) .setAudioFormat(RecognitionConfig.AudioFormat.WAV) .setTranscriptionFormat(RecognitionConfig.TranscriptionFormat.SRT) .build(); RecognitionAudio audio = RecognitionAudio.newBuilder().setAudioContent(byteString).build(); SpeechRecognitionRequest request = SpeechRecognitionRequest.newBuilder() .setAudio(audio) .setConfig(config) .build(); SpeechRecognitionResult response; try { response = blockingStub.recognize(request); return response; } catch (StatusRuntimeException e) { logger.log(Level.WARNING, \"RPC failed: {0}\", e.getStatus()); return SpeechRecognitionResult.newBuilder().build(); } } public static void main(String[] args) throws Exception { String target = \"34.70.114.226:50051\"; ManagedChannel channel = ManagedChannelBuilder.forTarget(target) .usePlaintext() .build(); try { SpeechRecognitionClient client = new SpeechRecognitionClient(channel); SpeechRecognitionResult srtUrlResponse = client.srtUrlV2() SpeechRecognitionResult srtBytesResponse = client.srtBytesV2(); SpeechRecognitionResult bytesResponse = client.transcribeBytesV2(); SpeechRecognitionResult urlResponse = client.transcribeUrlV2(); System.out.println(bytesResponse.getTranscript()); System.out.println(urlResponse.getTranscript()); System.out.println(srtBytesResponse.getSrt()); System.out.println(srtUrlResponse.getSrt()); } finally { channel.shutdownNow().awaitTermination(5, TimeUnit.SECONDS); } } }","title":"Client Code reference"},{"location":"model_api/#setup-and-getting-started-guide","text":"Clone our github repo : https://github.com/Open-Speech-EkStep/speech-recognition-open-api.git Setup the grpc server: Without docker 1. Create and activate a new environment : ```conda create --name <env> python=3.8 && conda activate <env>``` Install required libraries using the following command: pip install -r requirements.txt Bootstrap the model code and other models as pre requisites: sh model_bootstrap.sh 4. Download models and update the right model paths in model_dict.json. 5. Start the server at port 50051: python server.py With docker docker build -t speech_recognition_model_api . sudo docker run --cpus=6 -m 20000m -itd -p <<host_port>>:50051 --name speech_recognition_model_api -v <<host_model_path>>/deployed_models:<<container_model_path>>/deployed_models/ -i -t speech_recognition_model_api Using the model api as part of client code: In python, python examples/python/speech-recognition/main.py Using the model api as part of REST call using api-gateway: Create api config in api gateway: gcloud api-gateway api-configs create CONFIG_ID \\ --api=API_ID --project=PROJECT_ID \\ --grpc-files=api_descriptor.pb,api_config.yaml Deploy gateway in api gateway: gcloud api-gateway gateways create GATEWAY_ID \\ --api=API_ID --api-config=CONFIG_ID \\ --location=GCP_REGION --project=PROJECT_ID View gateway information: gcloud api-gateway gateways describe GATEWAY_ID \\ --location=GCP_REGION --project=PROJECT_ID To run tests, use the following command: py.test --grpc-fake-server --ignore=wav2letter --ignore=wav2vec-infer --ignore=kenlm","title":"Setup and getting started guide"},{"location":"model_api/#contributing","text":"Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request","title":"Contributing"},{"location":"model_api/#license","text":"Distributed under the [MIT] License. See LICENSE for more information.","title":"License"},{"location":"model_api/#git-repository","text":"https://github.com/Open-Speech-EkStep/speech-recognition-open-api.git","title":"Git repository"},{"location":"model_api/#contact","text":"Connect with community on Gitter Project Link: https://github.com/Open-Speech-EkStep/speech-recognition-open-api.git","title":"Contact"},{"location":"model_training/","text":"Pretrained Models \u00b6 We are releasing pretrained models in various Indic Languages. Please head over to this repo . Table of contents \u00b6 * Installation and Setup * Directory Structure * Data Description * Usage * For Pretraining * For Finetuning * For Inference * For Single File Inference * License Installation and Setup \u00b6 git clone https://github.com/Open-Speech-EkStep/vakyansh-wav2vec2-experimentation.git conda create --name <env_name> python=3.7 conda activate <env_name> cd vakyansh-wav2vec2-experimentation ### Packages pip install packaging soundfile swifter pip install -r requirements.txt pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html ### For fairseq setup(fairseq should be installed outside vakyansh-wav2vec2-experimentation repo) cd .. git clone -b ekstep-wav2vec2 https://github.com/Open-Speech-EkStep/fairseq.git cd fairseq pip install -e . ### install other libraries ### For Kenlm, openblas cd .. sudo apt-get install liblzma-dev libbz2-dev libzstd-dev libsndfile1-dev libopenblas-dev libfftw3-dev libgflags-dev libgoogle-glog-dev sudo apt install build-essential cmake libboost-system-dev libboost-thread-dev libboost-program-options-dev libboost-test-dev libeigen3-dev zlib1g-dev libbz2-dev liblzma-dev git clone https://github.com/kpu/kenlm.git cd kenlm mkdir -p build && cd build cmake .. make -j 16 cd .. export KENLM_ROOT_DIR=$PWD export USE_CUDA=0 ## for cpu cd .. ### wav2letter git clone -b v0.2 https://github.com/facebookresearch/wav2letter.git cd wav2letter git checkout b1d1f89f586120a978a4666cffd45c55f0a2e564 cd bindings/python pip install -e . Directory Structure \u00b6 root-directory . |-- ./checkpoints | |-- ./checkpoints/custom_model | | `-- ./checkpoints/custom_model/ | |-- ./checkpoints/finetuning | | `-- ./checkpoints/finetuning/ | `-- ./checkpoints/pretraining | `-- ./checkpoints/pretraining/ |-- ./data | |-- ./data/finetuning | | `-- ./data/finetuning/ | |-- ./data/inference | | `-- ./data/inference/ | |-- ./data/pretraining | | `-- ./data/pretraining/ | `-- ./data/processed | `-- ./data/processed/ |-- ./lm | `-- ./lm/ |-- ./logs | |-- ./logs/finetuning | | `-- ./logs/finetuning/ | `-- ./logs/pretraining | `-- ./logs/pretraining/ |-- ./notebooks | `-- ./notebooks/ |-- ./results | `-- ./results/ |-- ./scripts | |-- ./scripts/data | | `-- ./scripts/data/ | |-- ./scripts/parse_yaml.sh | |-- ./scripts/finetuning | | |-- ./scripts/finetuning/start_finetuning.sh | | |-- ./scripts/finetuning/prepare_data.sh | | `-- ./scripts/finetuning/README.md | |-- ./scripts/lm | | |-- ./scripts/lm/run_lm_pipeline.sh | | `-- ./scripts/lm/README.md | |-- ./scripts/pretraining | | |-- ./scripts/pretraining/start_pretraining_base.sh | | |-- ./scripts/pretraining/start_pretraining_large.sh | | |-- ./scripts/pretraining/prepare_data.sh | | `-- ./scripts/pretraining/README.md | `-- ./scripts/inference | |-- ./scripts/inference/infer.sh | |-- ./scripts/inference/prepare_data.sh | |-- ./scripts/inference/generate_custom_model.sh | |-- ./scripts/inference/single_file_inference.sh | `-- ./scripts/inference/README.md |-- ./config | |-- ./config/finetuning.yaml | |-- ./config/pretraining_base.yaml | |-- ./config/pretraining_large.yaml | `-- ./config/README.md |-- ./requirements.txt |-- ./utils | |-- ./utils/analysis | | `-- ./utils/analysis/generate_wav_report_from_tsv.py | |-- ./utils/prep_scripts | | |-- ./utils/prep_scripts/dict_and_lexicon_maker.py | | |-- ./utils/prep_scripts/labels.py | | `-- ./utils/prep_scripts/manifest.py | |-- ./utils/wer | | |-- ./utils/wer/wer.py | | `-- ./utils/wer/wer_wav2vec.py | |-- ./utils/inference | | |-- ./utils/inference/generate_custom_model.py | | `-- ./utils/inference/single_file_inference.py | `-- ./utils/lm | |-- ./utils/lm/concatenate_text.py | |-- ./utils/lm/make_lexicon_lst.py | |-- ./utils/lm/generate_lm.py | |-- ./utils/lm/clean_text.py | `-- ./utils/lm/remove_duplicate_lines.py `-- ./README.md Data Description \u00b6 For Audio Files. Sample Rate [Hz] = 16000 Channels = 'mono' Bit Rate [kbit/s] = 256 Precision [bits] = 16 Audio length should be less than 30 seconds otherwise it will be ignored during data preparation After scripts/finetuning/prepare_data.sh is run, analysis will be generated which can be used to tune min/max_sample_size in the config files For Text Files Corresponding text file of each audio file must be on the same directory as its audio Text file should not contain any punctuation characters Check dict.ltr.txt file generated after prepare_data so that it does not contain any foreign language character For Language Model Character set of text used for language model should be same as character set used for training Sample code for cleaning text file for english language is given here clean_text.py Sample code for removing duplicate line from text file is given here remove_duplicate_lines.py Usage \u00b6 For Pretraining \u00b6 Edit the path to data in the scripts/pretraining/prepare_data.sh file. To prepare the data: $ cd scripts/pretraining $ bash prepare_data.sh Edit the config/pretraining_base.yaml or config/pretraining_large.yaml for different parameter configurations.Check the required paths and values in start_pretraining_base.sh or start_pretraining_large.sh. Refer to config README To start run: $ bash start_pretraining_base.sh Refer this for pretraining parameters. For Finetuning \u00b6 Edit the path to data in the scripts/finetuning/prepare_data.sh file. To prepare the data: $ cd scripts/finetuning $ bash prepare_data.sh Edit the config/finetuning.yaml for different parameter configurations.Check the required paths and values in start_finetuning.sh. Refer to config README To start run: $ bash start_finetuning.sh Refer this for finetuning parameters. For Inference \u00b6 Edit the path to data in the scripts/inference/prepare_data.sh file. To prepare the test data run: $ cd scripts/inference/ $ bash prepare_data.sh Edit the infer.sh file for required paths. To start inference run: $ bash infer.sh Refer this for inference parameters. For Single File Inference \u00b6 To generate custom model, run: $ cd scripts/inference $ bash generate_custom_model.sh To infer for single file, change path in single_file_inference.sh. Then run: $ bash single_file_inference.sh For generating LM \u00b6 Edit the run_lm_pipeline.sh variables as required, then run: $ cd scripts/lm $ bash run_lm_pipeline.sh Refer this for LM pipeline. License \u00b6 fairseq(-py) is MIT-licensed. The license applies to the pre-trained models as well.","title":"Model Training Pipeline"},{"location":"model_training/#pretrained-models","text":"We are releasing pretrained models in various Indic Languages. Please head over to this repo .","title":"Pretrained Models"},{"location":"model_training/#table-of-contents","text":"* Installation and Setup * Directory Structure * Data Description * Usage * For Pretraining * For Finetuning * For Inference * For Single File Inference * License","title":"Table of contents"},{"location":"model_training/#installation-and-setup","text":"git clone https://github.com/Open-Speech-EkStep/vakyansh-wav2vec2-experimentation.git conda create --name <env_name> python=3.7 conda activate <env_name> cd vakyansh-wav2vec2-experimentation ### Packages pip install packaging soundfile swifter pip install -r requirements.txt pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html ### For fairseq setup(fairseq should be installed outside vakyansh-wav2vec2-experimentation repo) cd .. git clone -b ekstep-wav2vec2 https://github.com/Open-Speech-EkStep/fairseq.git cd fairseq pip install -e . ### install other libraries ### For Kenlm, openblas cd .. sudo apt-get install liblzma-dev libbz2-dev libzstd-dev libsndfile1-dev libopenblas-dev libfftw3-dev libgflags-dev libgoogle-glog-dev sudo apt install build-essential cmake libboost-system-dev libboost-thread-dev libboost-program-options-dev libboost-test-dev libeigen3-dev zlib1g-dev libbz2-dev liblzma-dev git clone https://github.com/kpu/kenlm.git cd kenlm mkdir -p build && cd build cmake .. make -j 16 cd .. export KENLM_ROOT_DIR=$PWD export USE_CUDA=0 ## for cpu cd .. ### wav2letter git clone -b v0.2 https://github.com/facebookresearch/wav2letter.git cd wav2letter git checkout b1d1f89f586120a978a4666cffd45c55f0a2e564 cd bindings/python pip install -e .","title":"Installation and Setup"},{"location":"model_training/#directory-structure","text":"root-directory . |-- ./checkpoints | |-- ./checkpoints/custom_model | | `-- ./checkpoints/custom_model/ | |-- ./checkpoints/finetuning | | `-- ./checkpoints/finetuning/ | `-- ./checkpoints/pretraining | `-- ./checkpoints/pretraining/ |-- ./data | |-- ./data/finetuning | | `-- ./data/finetuning/ | |-- ./data/inference | | `-- ./data/inference/ | |-- ./data/pretraining | | `-- ./data/pretraining/ | `-- ./data/processed | `-- ./data/processed/ |-- ./lm | `-- ./lm/ |-- ./logs | |-- ./logs/finetuning | | `-- ./logs/finetuning/ | `-- ./logs/pretraining | `-- ./logs/pretraining/ |-- ./notebooks | `-- ./notebooks/ |-- ./results | `-- ./results/ |-- ./scripts | |-- ./scripts/data | | `-- ./scripts/data/ | |-- ./scripts/parse_yaml.sh | |-- ./scripts/finetuning | | |-- ./scripts/finetuning/start_finetuning.sh | | |-- ./scripts/finetuning/prepare_data.sh | | `-- ./scripts/finetuning/README.md | |-- ./scripts/lm | | |-- ./scripts/lm/run_lm_pipeline.sh | | `-- ./scripts/lm/README.md | |-- ./scripts/pretraining | | |-- ./scripts/pretraining/start_pretraining_base.sh | | |-- ./scripts/pretraining/start_pretraining_large.sh | | |-- ./scripts/pretraining/prepare_data.sh | | `-- ./scripts/pretraining/README.md | `-- ./scripts/inference | |-- ./scripts/inference/infer.sh | |-- ./scripts/inference/prepare_data.sh | |-- ./scripts/inference/generate_custom_model.sh | |-- ./scripts/inference/single_file_inference.sh | `-- ./scripts/inference/README.md |-- ./config | |-- ./config/finetuning.yaml | |-- ./config/pretraining_base.yaml | |-- ./config/pretraining_large.yaml | `-- ./config/README.md |-- ./requirements.txt |-- ./utils | |-- ./utils/analysis | | `-- ./utils/analysis/generate_wav_report_from_tsv.py | |-- ./utils/prep_scripts | | |-- ./utils/prep_scripts/dict_and_lexicon_maker.py | | |-- ./utils/prep_scripts/labels.py | | `-- ./utils/prep_scripts/manifest.py | |-- ./utils/wer | | |-- ./utils/wer/wer.py | | `-- ./utils/wer/wer_wav2vec.py | |-- ./utils/inference | | |-- ./utils/inference/generate_custom_model.py | | `-- ./utils/inference/single_file_inference.py | `-- ./utils/lm | |-- ./utils/lm/concatenate_text.py | |-- ./utils/lm/make_lexicon_lst.py | |-- ./utils/lm/generate_lm.py | |-- ./utils/lm/clean_text.py | `-- ./utils/lm/remove_duplicate_lines.py `-- ./README.md","title":"Directory Structure"},{"location":"model_training/#data-description","text":"For Audio Files. Sample Rate [Hz] = 16000 Channels = 'mono' Bit Rate [kbit/s] = 256 Precision [bits] = 16 Audio length should be less than 30 seconds otherwise it will be ignored during data preparation After scripts/finetuning/prepare_data.sh is run, analysis will be generated which can be used to tune min/max_sample_size in the config files For Text Files Corresponding text file of each audio file must be on the same directory as its audio Text file should not contain any punctuation characters Check dict.ltr.txt file generated after prepare_data so that it does not contain any foreign language character For Language Model Character set of text used for language model should be same as character set used for training Sample code for cleaning text file for english language is given here clean_text.py Sample code for removing duplicate line from text file is given here remove_duplicate_lines.py","title":"Data Description"},{"location":"model_training/#usage","text":"","title":"Usage"},{"location":"model_training/#for-pretraining","text":"Edit the path to data in the scripts/pretraining/prepare_data.sh file. To prepare the data: $ cd scripts/pretraining $ bash prepare_data.sh Edit the config/pretraining_base.yaml or config/pretraining_large.yaml for different parameter configurations.Check the required paths and values in start_pretraining_base.sh or start_pretraining_large.sh. Refer to config README To start run: $ bash start_pretraining_base.sh Refer this for pretraining parameters.","title":"For Pretraining"},{"location":"model_training/#for-finetuning","text":"Edit the path to data in the scripts/finetuning/prepare_data.sh file. To prepare the data: $ cd scripts/finetuning $ bash prepare_data.sh Edit the config/finetuning.yaml for different parameter configurations.Check the required paths and values in start_finetuning.sh. Refer to config README To start run: $ bash start_finetuning.sh Refer this for finetuning parameters.","title":"For Finetuning"},{"location":"model_training/#for-inference","text":"Edit the path to data in the scripts/inference/prepare_data.sh file. To prepare the test data run: $ cd scripts/inference/ $ bash prepare_data.sh Edit the infer.sh file for required paths. To start inference run: $ bash infer.sh Refer this for inference parameters.","title":"For Inference"},{"location":"model_training/#for-single-file-inference","text":"To generate custom model, run: $ cd scripts/inference $ bash generate_custom_model.sh To infer for single file, change path in single_file_inference.sh. Then run: $ bash single_file_inference.sh","title":"For Single File Inference"},{"location":"model_training/#for-generating-lm","text":"Edit the run_lm_pipeline.sh variables as required, then run: $ cd scripts/lm $ bash run_lm_pipeline.sh Refer this for LM pipeline.","title":"For generating LM"},{"location":"model_training/#license","text":"fairseq(-py) is MIT-licensed. The license applies to the pre-trained models as well.","title":"License"},{"location":"speaker_clustering/","text":"Speaker Clustering \u00b6 Table of Contents \u00b6 Speaker Clustering Table of Contents About The Project Working Embeddings Clustering algorithm Repetitive merging Splitting Fitting Noise points Hyperparameters About The Project \u00b6 Speaker Clustering, or identification of speakers in the wild is mainly useful for audio sources with no mapping between audios and a speaker label/name. It is the task of identifying the unique speakers in a set of audio recordings (each belonging to exactly one speaker) without knowing who and how many speakers are present in the entire data. Our intelligent data pipelines split each audio, based on Voice Activity Detection, as one of the starting steps. These shorter audio utterances are then used to train deep learning models. We assume that the utterances are short enough to have one speaker per utterance, since the splitting logic is using unvoiced segments as points to split an audio. Working \u00b6 This documentation will help you understand the various steps involved and take you through the hyperparameters, so you can tune them to achieve the best possible results. Embeddings \u00b6 We use the Voice Encoder model proposed here , and implemented here for converting our audio utterances into fixed length embeddings. System overview for training Voice encoder. Different colours indicate utterances/embeddings from different speakers. Voice Encoder is a speaker-discriminative model trained on a text-independent speaker verification task. Thus it allows us to derive a high-level representation of the voice present in an audio. An embedding is a 256 dimensional vector capable of summarizing the characteristics of the voice spoken. The data used to train the model contained 1.8k speakers from LibriSpeech-other, Voxceleb, Vox celeb2; making a final of more than 1000 hrs of data in English. Since our experiment sources for audios were in Hindi, we did a small experiment to determine whether embeddings on Hindi data using this pretrained model were able to separate speakers. The resulting dist plot is presented below. x-axis is the Cosine Similarity. A distplot showing separation between embeddings belonging to different speakers, based on Cosine similarity. Clustering algorithm \u00b6 Embeddings for a source are passed as a matrix for clustering in this step. If the number of embeddings are greater than the parameter partial_set_size , they are divided into multiple partial sets of this size. This step is done to reduce the computational cost of calculating cosine distances and other matrix operations during clustering. We use Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) as our core clustering algorithm on each of these partial sets. This step also classifies some points as noise points - meaning they couldn't be used up in any clusters formed for this partial set. We keep a record of all these noise points for fitting later. Repetitive merging \u00b6 We found in our experiments that some of the speakers had their clusters distributed as separate ones - even in one partial set. This step helps in allowing such clusters to merge. Merging is based on cosine similarity (94-96% similar clusters are merged repetitively). Also, clusters for the same speaker but from different partial sets also get merged in this step. Initial EOM clustering and repetitive merging of clusters. Splitting \u00b6 For some sources in our experiments, big clusters usually contained very high diversity of speakers. Splitting is done on such \"bigger\" clusters to make sure we have high cluster purity. A cluster is called big if it has more than 3 times the average number of points across all clusters. We also tried splitting a large cluster containing audios from one speaker only and it was retained as is, meaning this step should not affect large clusters belonging to one speaker only. Cluster splitting is achieved by using Leaf HDBSCAN clustering on the big clusters. This allows for more more fine grained clustering. More details on this follow in the next section. Repetitive merging is applied again after splitting to allow clusters with high cosine simialrities to be merged again, if any. Splitting big clusters and repetitive merging of clusters. Fitting Noise points \u00b6 All the noise points - points which could not be put into a cluster, are allowed to merge with a clusters if they have a cosine similairy of >= 80% with a noise point. This parameter is also configurable. Hyperparameters \u00b6 min_cluster_size: the smallest size grouping that you wish to consider a cluster min_cluster_size can be increased if the source is expected to have large number of utterances for a less number of speakers. min_samples: number of points required in the neighborhood of a point to be considered a core point smallest value = 1, max value = min_cluster_size smaller values of min_samples have an effect of lowering the number of noise points in our experiments, around 30% of points across a source were being classified as noise before fitting. You can try with min_samples=1 for least possible noise points classification. partial_set_size: number of utterances to treat as one set for clustering. We used partial_set_size =11122 as this number represents around 20 hours of data on an average for us, and was computationally less demanding. fit_noise_on_similarity: cosine similarity between a noise point and a cluster, at which the point can be fit to a cluster. default = 0.80, meaning points with cosine similarity >=0.80 with a cluster's mean embedding will be fit to that cluster. values of fit_noise_on_similarity lesser than 0.80 can have an effect of decresing cluster purity. cluster_selection_method: can either be \u2018eom\u2019 or \u2018leaf\u2019 'eom' or Excess of Mass is the default way of HDBSCAN's working. 'leaf' will select leaf nodes from the tree, producing many small homogeneous clusters. Allowing for fine-grained clusters. This is used while splitting clusters into smaller ones.","title":"Speaker Clustering"},{"location":"speaker_clustering/#speaker-clustering","text":"","title":"Speaker Clustering"},{"location":"speaker_clustering/#table-of-contents","text":"Speaker Clustering Table of Contents About The Project Working Embeddings Clustering algorithm Repetitive merging Splitting Fitting Noise points Hyperparameters","title":"Table of Contents"},{"location":"speaker_clustering/#about-the-project","text":"Speaker Clustering, or identification of speakers in the wild is mainly useful for audio sources with no mapping between audios and a speaker label/name. It is the task of identifying the unique speakers in a set of audio recordings (each belonging to exactly one speaker) without knowing who and how many speakers are present in the entire data. Our intelligent data pipelines split each audio, based on Voice Activity Detection, as one of the starting steps. These shorter audio utterances are then used to train deep learning models. We assume that the utterances are short enough to have one speaker per utterance, since the splitting logic is using unvoiced segments as points to split an audio.","title":"About The Project"},{"location":"speaker_clustering/#working","text":"This documentation will help you understand the various steps involved and take you through the hyperparameters, so you can tune them to achieve the best possible results.","title":"Working"},{"location":"speaker_clustering/#embeddings","text":"We use the Voice Encoder model proposed here , and implemented here for converting our audio utterances into fixed length embeddings. System overview for training Voice encoder. Different colours indicate utterances/embeddings from different speakers. Voice Encoder is a speaker-discriminative model trained on a text-independent speaker verification task. Thus it allows us to derive a high-level representation of the voice present in an audio. An embedding is a 256 dimensional vector capable of summarizing the characteristics of the voice spoken. The data used to train the model contained 1.8k speakers from LibriSpeech-other, Voxceleb, Vox celeb2; making a final of more than 1000 hrs of data in English. Since our experiment sources for audios were in Hindi, we did a small experiment to determine whether embeddings on Hindi data using this pretrained model were able to separate speakers. The resulting dist plot is presented below. x-axis is the Cosine Similarity. A distplot showing separation between embeddings belonging to different speakers, based on Cosine similarity.","title":"Embeddings"},{"location":"speaker_clustering/#clustering-algorithm","text":"Embeddings for a source are passed as a matrix for clustering in this step. If the number of embeddings are greater than the parameter partial_set_size , they are divided into multiple partial sets of this size. This step is done to reduce the computational cost of calculating cosine distances and other matrix operations during clustering. We use Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) as our core clustering algorithm on each of these partial sets. This step also classifies some points as noise points - meaning they couldn't be used up in any clusters formed for this partial set. We keep a record of all these noise points for fitting later.","title":"Clustering algorithm"},{"location":"speaker_clustering/#repetitive-merging","text":"We found in our experiments that some of the speakers had their clusters distributed as separate ones - even in one partial set. This step helps in allowing such clusters to merge. Merging is based on cosine similarity (94-96% similar clusters are merged repetitively). Also, clusters for the same speaker but from different partial sets also get merged in this step. Initial EOM clustering and repetitive merging of clusters.","title":"Repetitive merging"},{"location":"speaker_clustering/#splitting","text":"For some sources in our experiments, big clusters usually contained very high diversity of speakers. Splitting is done on such \"bigger\" clusters to make sure we have high cluster purity. A cluster is called big if it has more than 3 times the average number of points across all clusters. We also tried splitting a large cluster containing audios from one speaker only and it was retained as is, meaning this step should not affect large clusters belonging to one speaker only. Cluster splitting is achieved by using Leaf HDBSCAN clustering on the big clusters. This allows for more more fine grained clustering. More details on this follow in the next section. Repetitive merging is applied again after splitting to allow clusters with high cosine simialrities to be merged again, if any. Splitting big clusters and repetitive merging of clusters.","title":"Splitting"},{"location":"speaker_clustering/#fitting-noise-points","text":"All the noise points - points which could not be put into a cluster, are allowed to merge with a clusters if they have a cosine similairy of >= 80% with a noise point. This parameter is also configurable.","title":"Fitting Noise points"},{"location":"speaker_clustering/#hyperparameters","text":"min_cluster_size: the smallest size grouping that you wish to consider a cluster min_cluster_size can be increased if the source is expected to have large number of utterances for a less number of speakers. min_samples: number of points required in the neighborhood of a point to be considered a core point smallest value = 1, max value = min_cluster_size smaller values of min_samples have an effect of lowering the number of noise points in our experiments, around 30% of points across a source were being classified as noise before fitting. You can try with min_samples=1 for least possible noise points classification. partial_set_size: number of utterances to treat as one set for clustering. We used partial_set_size =11122 as this number represents around 20 hours of data on an average for us, and was computationally less demanding. fit_noise_on_similarity: cosine similarity between a noise point and a cluster, at which the point can be fit to a cluster. default = 0.80, meaning points with cosine similarity >=0.80 with a cluster's mean embedding will be fit to that cluster. values of fit_noise_on_similarity lesser than 0.80 can have an effect of decresing cluster purity. cluster_selection_method: can either be \u2018eom\u2019 or \u2018leaf\u2019 'eom' or Excess of Mass is the default way of HDBSCAN's working. 'leaf' will select leaf nodes from the tree, producing many small homogeneous clusters. Allowing for fine-grained clusters. This is used while splitting clusters into smaller ones.","title":"Hyperparameters"}]}