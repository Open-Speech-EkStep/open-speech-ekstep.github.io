{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Vakyansh \u00b6 Recipies to build Speech Recognition models Introduction \u00b6 Vakyansh aims to host the key essentials of Automatic Speech Recognition (ASR) technology, focusing on Indian languages. It is a resource that allows people to build applications that leverage speech recognition. The site will host open data for training ASR models, open source utilities and pipelines to train ASR models and open ASR models themselves. Vakyansh also hosts tools to contribute your voices to create a diverse open data repository of Indian voices to this end. This data will be available in an open manner for all to use. It is a resource that allows people to build applications that leverage speech recognition. Open Source \u00b6 Open sourcing the speech recognition technology empowers us to bring our languages to the core of our fundamentals. Vakyansh aims to open source the speech recognition models in various languages, the datasets collected through various channels and the linguistic utilities developed to process and clean the data and make it usable by speech recognition tools. The open source strategy will enable the various language communities, individuals and technologists, who are passionate about their language, to develop speech recognition applications, and integrate them in various domains for the good of the community. Getting started \u00b6 Vakyansh's developer documentation is meant for its adopters, developers and contributors. It would enable people to innovate and improve and build Speech Recognition Models in any language. Modeling Process \u00b6 Vakyansh Components \u00b6 1. Data Collection Pipeline \u00b6 Pipelines to collect data in automated way for the language you want 2. Crowdsourcing Platform \u00b6 Platform to record and validate voice data from various speakers. 3. Intelligent Data Pipeline \u00b6 Pipelines to transform raw data and prepare data for model training. They clean, process and balance data for model training 4. Model Training Pipeline \u00b6 Pipeline to build state of the art Speech Recognition Model using the data provided by Intelligent Data Pipeline Vakyansh Technical Overview \u00b6 The Developer documentation provides you with a complete set of guidelines which you need to: Install Vakyansh Configure Vakyansh Customize Vakyansh Extend Vakyansh Contribute to Vakyansh Discussion \u00b6 Connect with community on Gitter","title":"Home"},{"location":"#welcome-to-vakyansh","text":"Recipies to build Speech Recognition models","title":"Welcome to Vakyansh"},{"location":"#introduction","text":"Vakyansh aims to host the key essentials of Automatic Speech Recognition (ASR) technology, focusing on Indian languages. It is a resource that allows people to build applications that leverage speech recognition. The site will host open data for training ASR models, open source utilities and pipelines to train ASR models and open ASR models themselves. Vakyansh also hosts tools to contribute your voices to create a diverse open data repository of Indian voices to this end. This data will be available in an open manner for all to use. It is a resource that allows people to build applications that leverage speech recognition.","title":"Introduction"},{"location":"#open-source","text":"Open sourcing the speech recognition technology empowers us to bring our languages to the core of our fundamentals. Vakyansh aims to open source the speech recognition models in various languages, the datasets collected through various channels and the linguistic utilities developed to process and clean the data and make it usable by speech recognition tools. The open source strategy will enable the various language communities, individuals and technologists, who are passionate about their language, to develop speech recognition applications, and integrate them in various domains for the good of the community.","title":"Open Source"},{"location":"#getting-started","text":"Vakyansh's developer documentation is meant for its adopters, developers and contributors. It would enable people to innovate and improve and build Speech Recognition Models in any language.","title":"Getting started"},{"location":"#modeling-process","text":"","title":"Modeling Process"},{"location":"#vakyansh-components","text":"","title":"Vakyansh Components"},{"location":"#1-data-collection-pipeline","text":"Pipelines to collect data in automated way for the language you want","title":"1. Data Collection Pipeline"},{"location":"#2-crowdsourcing-platform","text":"Platform to record and validate voice data from various speakers.","title":"2. Crowdsourcing Platform"},{"location":"#3-intelligent-data-pipeline","text":"Pipelines to transform raw data and prepare data for model training. They clean, process and balance data for model training","title":"3. Intelligent Data Pipeline"},{"location":"#4-model-training-pipeline","text":"Pipeline to build state of the art Speech Recognition Model using the data provided by Intelligent Data Pipeline","title":"4. Model Training Pipeline"},{"location":"#vakyansh-technical-overview","text":"The Developer documentation provides you with a complete set of guidelines which you need to: Install Vakyansh Configure Vakyansh Customize Vakyansh Extend Vakyansh Contribute to Vakyansh","title":"Vakyansh Technical Overview"},{"location":"#discussion","text":"Connect with community on Gitter","title":"Discussion"},{"location":"CONTRIBUTING/","text":"Contributing to this project \u00b6 Contributors Agreement Introduction I don't want to read this whole thing I just have a question!!! How Can I Contribute? Reporting Bugs Suggesting Enhancements Creating Pull Requests Contributors Agreement \u00b6 By submitting patches to this project you agree to allow them to be redistributed under the project's license, according to the normal forms and usages of the open-source community. Introduction \u00b6 First off, thank you for considering contributing to this project. It's people like you that make it such a great tool. Following these guidelines helps to communicate that you respect the time of the developers managing and developing this open source project. In return, they should reciprocate that respect in addressing your issue, assessing changes, and helping you finalize your pull requests. This is an open source project and we love to receive contributions from our community \u2014 you! There are many ways to contribute, from writing tutorials or blog posts, improving the documentation, submitting bug reports and feature requests or writing code which can be incorporated into the main project itself. I don't want to read this whole thing I just have a question!!! \u00b6 We currently allow our users to use the issue tracker for support questions. But please be wary that maintaining an open source project can take a lot of time from the maintainers. If asking for a support question, state it clearly and take the time to explain your problem properly. Also, if your problem is not strictly related to this project we recommend you to use Stack Overlow instead. How Can I Contribute? \u00b6 Reporting Bugs \u00b6 Before creating bug reports, please check the existing bug reports as you might find out that you don't need to create one. When you are creating a bug report, please include as many details as possible. How Do I Submit A (Good) Bug Report? \u00b6 Bugs are tracked as GitHub issues . Create an issue on the project's repository and provide the following information. Explain the problem and include additional details to help maintainers reproduce the problem: Use a clear and descriptive title for the issue to identify the problem. Describe the exact steps which reproduce the problem in as many details as possible. For example, start by explaining how you used the project. When listing steps, don't just say what you did, but explain how you did it . Provide specific examples to demonstrate the steps . It's always better to get more information. You can include links to files or GitHub projects, copy/pasteable snippets or even print screens or animated GIFS. If you're providing snippets in the issue, use Markdown code blocks . Describe the behavior you observed after following the steps and point out what exactly is the problem with that behavior. Explain which behavior you expected to see instead and why. If the problem wasn't triggered by a specific action , describe what you were doing before the problem happened and share more information using the guidelines below. Provide more context by answering these questions: Did the problem start happening recently (e.g. after updating to a new version) or was this always a problem? If the problem started happening recently, can you reproduce the problem in an older version? What's the most recent version in which the problem doesn't happen? Can you reliably reproduce the issue? If not, provide details about how often the problem happens and under which conditions it normally happens. Include details about your configuration and environment: Which version of the project are you using? What's the name and version of the OS you're using ? Any other information that could be useful about you environment Suggesting Enhancements \u00b6 This section guides you through submitting an enhancement suggestion for this project, including completely new features and minor improvements to existing functionality. Following these guidelines helps maintainers and the community understand your suggestion and find related suggestions. Before creating enhancement suggestions, please check the list of enhancements suggestions in the issue tracker as you might find out that you don't need to create one. When you are creating an enhancement suggestion, please include as many details as possible. How Do I Submit A (Good) Enhancement Suggestion? \u00b6 Enhancement suggestions are tracked as GitHub issues . Create an issue on the project's repository and provide the following information: Use a clear and descriptive title for the issue to identify the suggestion. Provide a step-by-step description of the suggested enhancement in as many details as possible. Provide specific examples to demonstrate the steps . It's always better to get more information. You can include links to files or GitHub projects, copy/pasteable snippets or even print screens or animated GIFS. If you're providing snippets in the issue, use Markdown code blocks . Describe the current behavior and explain which behavior you expected to see instead and why. List some other similar projects where this enhancement exists. Specify which version of the project you're using. Specify the current environment you're using. if this is a useful information. Creating Pull Requests \u00b6 How Do I Submit A (Good) Pull Request? \u00b6 Be warned that the contributors agreement displayed on top of this document is applicable as soon as you create a pull request . Use a clear and descriptive title for the pull request to state the improvement you made to the code or the bug you solved. Provide a link to the related issue if the pull request is a follow up of an existing bug report or enhancement suggestion. Comment why this pull request represents an enhancement and give a rationale explaining why you did it that way and not another way. Use the same coding style than the one used in this project . Welcome suggestions from the maintainers to improve your pull request .","title":"Contributions"},{"location":"CONTRIBUTING/#contributing-to-this-project","text":"Contributors Agreement Introduction I don't want to read this whole thing I just have a question!!! How Can I Contribute? Reporting Bugs Suggesting Enhancements Creating Pull Requests","title":"Contributing to this project"},{"location":"CONTRIBUTING/#contributors-agreement","text":"By submitting patches to this project you agree to allow them to be redistributed under the project's license, according to the normal forms and usages of the open-source community.","title":"Contributors Agreement"},{"location":"CONTRIBUTING/#introduction","text":"First off, thank you for considering contributing to this project. It's people like you that make it such a great tool. Following these guidelines helps to communicate that you respect the time of the developers managing and developing this open source project. In return, they should reciprocate that respect in addressing your issue, assessing changes, and helping you finalize your pull requests. This is an open source project and we love to receive contributions from our community \u2014 you! There are many ways to contribute, from writing tutorials or blog posts, improving the documentation, submitting bug reports and feature requests or writing code which can be incorporated into the main project itself.","title":"Introduction"},{"location":"CONTRIBUTING/#i-dont-want-to-read-this-whole-thing-i-just-have-a-question","text":"We currently allow our users to use the issue tracker for support questions. But please be wary that maintaining an open source project can take a lot of time from the maintainers. If asking for a support question, state it clearly and take the time to explain your problem properly. Also, if your problem is not strictly related to this project we recommend you to use Stack Overlow instead.","title":"I don't want to read this whole thing I just have a question!!!"},{"location":"CONTRIBUTING/#how-can-i-contribute","text":"","title":"How Can I Contribute?"},{"location":"CONTRIBUTING/#reporting-bugs","text":"Before creating bug reports, please check the existing bug reports as you might find out that you don't need to create one. When you are creating a bug report, please include as many details as possible.","title":"Reporting Bugs"},{"location":"CONTRIBUTING/#how-do-i-submit-a-good-bug-report","text":"Bugs are tracked as GitHub issues . Create an issue on the project's repository and provide the following information. Explain the problem and include additional details to help maintainers reproduce the problem: Use a clear and descriptive title for the issue to identify the problem. Describe the exact steps which reproduce the problem in as many details as possible. For example, start by explaining how you used the project. When listing steps, don't just say what you did, but explain how you did it . Provide specific examples to demonstrate the steps . It's always better to get more information. You can include links to files or GitHub projects, copy/pasteable snippets or even print screens or animated GIFS. If you're providing snippets in the issue, use Markdown code blocks . Describe the behavior you observed after following the steps and point out what exactly is the problem with that behavior. Explain which behavior you expected to see instead and why. If the problem wasn't triggered by a specific action , describe what you were doing before the problem happened and share more information using the guidelines below. Provide more context by answering these questions: Did the problem start happening recently (e.g. after updating to a new version) or was this always a problem? If the problem started happening recently, can you reproduce the problem in an older version? What's the most recent version in which the problem doesn't happen? Can you reliably reproduce the issue? If not, provide details about how often the problem happens and under which conditions it normally happens. Include details about your configuration and environment: Which version of the project are you using? What's the name and version of the OS you're using ? Any other information that could be useful about you environment","title":"How Do I Submit A (Good) Bug Report?"},{"location":"CONTRIBUTING/#suggesting-enhancements","text":"This section guides you through submitting an enhancement suggestion for this project, including completely new features and minor improvements to existing functionality. Following these guidelines helps maintainers and the community understand your suggestion and find related suggestions. Before creating enhancement suggestions, please check the list of enhancements suggestions in the issue tracker as you might find out that you don't need to create one. When you are creating an enhancement suggestion, please include as many details as possible.","title":"Suggesting Enhancements"},{"location":"CONTRIBUTING/#how-do-i-submit-a-good-enhancement-suggestion","text":"Enhancement suggestions are tracked as GitHub issues . Create an issue on the project's repository and provide the following information: Use a clear and descriptive title for the issue to identify the suggestion. Provide a step-by-step description of the suggested enhancement in as many details as possible. Provide specific examples to demonstrate the steps . It's always better to get more information. You can include links to files or GitHub projects, copy/pasteable snippets or even print screens or animated GIFS. If you're providing snippets in the issue, use Markdown code blocks . Describe the current behavior and explain which behavior you expected to see instead and why. List some other similar projects where this enhancement exists. Specify which version of the project you're using. Specify the current environment you're using. if this is a useful information.","title":"How Do I Submit A (Good) Enhancement Suggestion?"},{"location":"CONTRIBUTING/#creating-pull-requests","text":"","title":"Creating Pull Requests"},{"location":"CONTRIBUTING/#how-do-i-submit-a-good-pull-request","text":"Be warned that the contributors agreement displayed on top of this document is applicable as soon as you create a pull request . Use a clear and descriptive title for the pull request to state the improvement you made to the code or the bug you solved. Provide a link to the related issue if the pull request is a follow up of an existing bug report or enhancement suggestion. Comment why this pull request represents an enhancement and give a rationale explaining why you did it that way and not another way. Use the same coding style than the one used in this project . Welcome suggestions from the maintainers to improve your pull request .","title":"How Do I Submit A (Good) Pull Request?"},{"location":"RELEASE_NOTES/","text":"","title":"Release notes"},{"location":"about/","text":"","title":"About"},{"location":"adr/","text":"Architecture Decision Records \u00b6 Crowdsourcing: Eliminate direct RDBMS access: IN_PROGRESS Cache above RDMS: PROPOSED Messaging Queue System: PROPOSED Continous Intelligent Data Pipelines: PROPOSED UI Framework: PROPOSED Discuss \u00b6 Crowdsourcing platform: Eliminate direct RDBMS access \u00b6 Status: APPROVED Driver(s): Rajat Singhal Approver(s): Pramod Verma, Vivek Raghavan Impact Area: High Level Architecture Date created: 2021-03-24 Date closed: 2021-03-26 Technical Story: Eliminate direct RDBMS access Context and Problem Statement \u00b6 Currently, the application service directly talks with RDBMS to fetch the sentences shown while doing Contributions and to fetch contributions while doing the Validations. When the application will scale and more features are added, there may be some contention at RDBMS layer. See current architecture Solutions \u00b6 Add cache layer over RDMS Write to topics asynchronously Tech Choices \u00b6 For cache : AWS ElasticCache - Redis, Apache Ignite : ADR For queue: AWS Kafka, AWS Kinesis : ADR Proposed Architecture \u00b6 Caching and messaging queue layer can be added with small imcremental changes Positive Consequences \u00b6 RDBMS will be off loaded from frequent reads and writes Users will experience lower latency System will be able to scale easily at data layer Audio processing like 'automatic validation', 'SNR' etc can be done in 'Contributions processor' Contribution processor can be in any other langauge like Python which is more suited for audio processing If in future the analytics need to be moved to separate data store, rich application domain events can be used rather than crude CDC events. Additional domains can be on boarded easily like 'Likho India' by adding new domain topics, processors and services Can be easily evolved into event driven microservices architecture assuming more features will come onto this platform Negative Consequences \u00b6 Overhead of maintaining cache and queue system The user experience may change (may be for go) due to async behaviour of the system Decision Outcome \u00b6 Cache: Redis A cache layer has been added to offload RDBMS from frequent reads. Queue was not added for purpose of offloading writes, as it would have impacted the application flow. Choose Cache above RDBMS \u00b6 Status: IMPLEMENTED Deciders: Rajat Singhal , Heera Ballabh, Umair Manzoor Impact Area: Tools and frameworks Date: 2021-03-24 Technical Story: [] Context and Problem Statement \u00b6 Currently, the application service directly talks with RDBMS to fetch the sentences shown while doing Contributions and to fetch contributions while doing the Validations. When the application will scale and more features are added, there may be some contention at RDBMS layer. Options \u00b6 AWS ElasticCache(Redis), Apache Ignite Solutions \u00b6 Add cache layer over RDMS Decision Outcome \u00b6 Cache: Redis A cache layer has been added to offload RDBMS from frequent reads. Choose messaging queue system \u00b6 Status: IMPLEMENTED Deciders: Rajat Singhal , Heera Ballabh, Umair Manzoor Impact Area: Tools and frameworks Date: 2021-03-24 Technical Story: [] Context and Problem Statement \u00b6 Currently, the application service directly talks with RDBMS to fetch the sentences shown while doing Contributions and to fetch contributions while doing the Validations. When the application will scale and more features are added, there may be some contention at RDBMS layer. Options \u00b6 Apache Kafka, AWS Kinesis Solutions \u00b6 Write to topics asynchronously Decision Outcome \u00b6 Kafka was chosen as the message queue. Asynchronous writes are done only for auto-validations, other writes have not been migrated as they impacted existing application flow. Continuous Intelligent Data Pipeline \u00b6 Status: PROPOSED Deciders: Rajat Singhal , Pramod Verma, Vivek Raghavan Impact Area: High Level Architecture Date: 2021-03-24 Technical Story: Continous Intelligent Data Pipelines Context and Problem Statement \u00b6 Solutions \u00b6 Decision Outcome \u00b6 Choose a UI framework \u00b6 Status: IMPLEMENTED Deciders: Rajat Singhal , Ayush Singhal Impact Area: Tools and frameworks Date: 2021-04-17 Technical Story: [] Context and Problem Statement \u00b6 The current application written in vanilla javascript has become difficult to maintain and incorporating new features is taking a lot of time Options \u00b6 Vue, React, Custom framework, NextJs Solutions \u00b6 Switching to a UI framework and off load the developers and they will be able to respond better to changing requirements. Decision Outcome \u00b6 NextJs, has been chosen as the framework we will be migrating to.","title":"Architecture Decision Records"},{"location":"adr/#architecture-decision-records","text":"Crowdsourcing: Eliminate direct RDBMS access: IN_PROGRESS Cache above RDMS: PROPOSED Messaging Queue System: PROPOSED Continous Intelligent Data Pipelines: PROPOSED UI Framework: PROPOSED","title":"Architecture Decision Records"},{"location":"adr/#discuss","text":"","title":"Discuss"},{"location":"adr/#crowdsourcing-platform-eliminate-direct-rdbms-access","text":"Status: APPROVED Driver(s): Rajat Singhal Approver(s): Pramod Verma, Vivek Raghavan Impact Area: High Level Architecture Date created: 2021-03-24 Date closed: 2021-03-26 Technical Story: Eliminate direct RDBMS access","title":"Crowdsourcing platform: Eliminate direct RDBMS access"},{"location":"adr/#context-and-problem-statement","text":"Currently, the application service directly talks with RDBMS to fetch the sentences shown while doing Contributions and to fetch contributions while doing the Validations. When the application will scale and more features are added, there may be some contention at RDBMS layer. See current architecture","title":"Context and Problem Statement"},{"location":"adr/#solutions","text":"Add cache layer over RDMS Write to topics asynchronously","title":"Solutions"},{"location":"adr/#tech-choices","text":"For cache : AWS ElasticCache - Redis, Apache Ignite : ADR For queue: AWS Kafka, AWS Kinesis : ADR","title":"Tech Choices"},{"location":"adr/#proposed-architecture","text":"Caching and messaging queue layer can be added with small imcremental changes","title":"Proposed Architecture"},{"location":"adr/#positive-consequences","text":"RDBMS will be off loaded from frequent reads and writes Users will experience lower latency System will be able to scale easily at data layer Audio processing like 'automatic validation', 'SNR' etc can be done in 'Contributions processor' Contribution processor can be in any other langauge like Python which is more suited for audio processing If in future the analytics need to be moved to separate data store, rich application domain events can be used rather than crude CDC events. Additional domains can be on boarded easily like 'Likho India' by adding new domain topics, processors and services Can be easily evolved into event driven microservices architecture assuming more features will come onto this platform","title":"Positive Consequences"},{"location":"adr/#negative-consequences","text":"Overhead of maintaining cache and queue system The user experience may change (may be for go) due to async behaviour of the system","title":"Negative Consequences"},{"location":"adr/#decision-outcome","text":"Cache: Redis A cache layer has been added to offload RDBMS from frequent reads. Queue was not added for purpose of offloading writes, as it would have impacted the application flow.","title":"Decision Outcome"},{"location":"adr/#choose-cache-above-rdbms","text":"Status: IMPLEMENTED Deciders: Rajat Singhal , Heera Ballabh, Umair Manzoor Impact Area: Tools and frameworks Date: 2021-03-24 Technical Story: []","title":"Choose Cache above RDBMS"},{"location":"adr/#context-and-problem-statement_1","text":"Currently, the application service directly talks with RDBMS to fetch the sentences shown while doing Contributions and to fetch contributions while doing the Validations. When the application will scale and more features are added, there may be some contention at RDBMS layer.","title":"Context and Problem Statement"},{"location":"adr/#options","text":"AWS ElasticCache(Redis), Apache Ignite","title":"Options"},{"location":"adr/#solutions_1","text":"Add cache layer over RDMS","title":"Solutions"},{"location":"adr/#decision-outcome_1","text":"Cache: Redis A cache layer has been added to offload RDBMS from frequent reads.","title":"Decision Outcome"},{"location":"adr/#choose-messaging-queue-system","text":"Status: IMPLEMENTED Deciders: Rajat Singhal , Heera Ballabh, Umair Manzoor Impact Area: Tools and frameworks Date: 2021-03-24 Technical Story: []","title":"Choose messaging queue system"},{"location":"adr/#context-and-problem-statement_2","text":"Currently, the application service directly talks with RDBMS to fetch the sentences shown while doing Contributions and to fetch contributions while doing the Validations. When the application will scale and more features are added, there may be some contention at RDBMS layer.","title":"Context and Problem Statement"},{"location":"adr/#options_1","text":"Apache Kafka, AWS Kinesis","title":"Options"},{"location":"adr/#solutions_2","text":"Write to topics asynchronously","title":"Solutions"},{"location":"adr/#decision-outcome_2","text":"Kafka was chosen as the message queue. Asynchronous writes are done only for auto-validations, other writes have not been migrated as they impacted existing application flow.","title":"Decision Outcome"},{"location":"adr/#continuous-intelligent-data-pipeline","text":"Status: PROPOSED Deciders: Rajat Singhal , Pramod Verma, Vivek Raghavan Impact Area: High Level Architecture Date: 2021-03-24 Technical Story: Continous Intelligent Data Pipelines","title":"Continuous Intelligent Data Pipeline"},{"location":"adr/#context-and-problem-statement_3","text":"","title":"Context and Problem Statement"},{"location":"adr/#solutions_3","text":"","title":"Solutions"},{"location":"adr/#decision-outcome_3","text":"","title":"Decision Outcome"},{"location":"adr/#choose-a-ui-framework","text":"Status: IMPLEMENTED Deciders: Rajat Singhal , Ayush Singhal Impact Area: Tools and frameworks Date: 2021-04-17 Technical Story: []","title":"Choose a UI framework"},{"location":"adr/#context-and-problem-statement_4","text":"The current application written in vanilla javascript has become difficult to maintain and incorporating new features is taking a lot of time","title":"Context and Problem Statement"},{"location":"adr/#options_2","text":"Vue, React, Custom framework, NextJs","title":"Options"},{"location":"adr/#solutions_4","text":"Switching to a UI framework and off load the developers and they will be able to respond better to changing requirements.","title":"Solutions"},{"location":"adr/#decision-outcome_4","text":"NextJs, has been chosen as the framework we will be migrating to.","title":"Decision Outcome"},{"location":"adr_template/","text":"[short title of solved problem and solution] \u00b6 Status: [proposed | rejected | accepted | deprecated | \u2026 | superseded by ADR-0005 ] Deciders: [list everyone involved in the decision] Date: [YYYY-MM-DD when the decision was last updated] Technical Story: [description | ticket/issue URL] Context and Problem Statement \u00b6 [Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.] Decision Drivers \u00b6 [driver 1, e.g., a force, facing concern, \u2026] [driver 2, e.g., a force, facing concern, \u2026] \u2026 Considered Options \u00b6 [option 1] [option 2] [option 3] \u2026 Decision Outcome \u00b6 Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)]. Positive Consequences \u00b6 [e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] \u2026 Negative Consequences \u00b6 [e.g., compromising quality attribute, follow-up decisions required, \u2026] \u2026 Pros and Cons of the Options \u00b6 [option 1] \u00b6 [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 [option 2] \u00b6 [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 [option 3] \u00b6 [example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026 Links \u00b6 [Link type] [Link to ADR] \u2026","title":"[short title of solved problem and solution]"},{"location":"adr_template/#short-title-of-solved-problem-and-solution","text":"Status: [proposed | rejected | accepted | deprecated | \u2026 | superseded by ADR-0005 ] Deciders: [list everyone involved in the decision] Date: [YYYY-MM-DD when the decision was last updated] Technical Story: [description | ticket/issue URL]","title":"[short title of solved problem and solution]"},{"location":"adr_template/#context-and-problem-statement","text":"[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]","title":"Context and Problem Statement"},{"location":"adr_template/#decision-drivers","text":"[driver 1, e.g., a force, facing concern, \u2026] [driver 2, e.g., a force, facing concern, \u2026] \u2026","title":"Decision Drivers "},{"location":"adr_template/#considered-options","text":"[option 1] [option 2] [option 3] \u2026","title":"Considered Options"},{"location":"adr_template/#decision-outcome","text":"Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force force | \u2026 | comes out best (see below)].","title":"Decision Outcome"},{"location":"adr_template/#positive-consequences","text":"[e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026] \u2026","title":"Positive Consequences "},{"location":"adr_template/#negative-consequences","text":"[e.g., compromising quality attribute, follow-up decisions required, \u2026] \u2026","title":"Negative Consequences "},{"location":"adr_template/#pros-and-cons-of-the-options","text":"","title":"Pros and Cons of the Options "},{"location":"adr_template/#option-1","text":"[example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026","title":"[option 1]"},{"location":"adr_template/#option-2","text":"[example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026","title":"[option 2]"},{"location":"adr_template/#option-3","text":"[example | description | pointer to more information | \u2026] Good, because [argument a] Good, because [argument b] Bad, because [argument c] \u2026","title":"[option 3]"},{"location":"adr_template/#links","text":"[Link type] [Link to ADR] \u2026","title":"Links "},{"location":"api_deployment/","text":"ASR and TTS API Deployment Guide \u00b6 We have developed two services for deployment in the CDAC Kubernetes environment. The services are: AUTOMATIC SPEECH RECOGNITION(ASR) \u00b6 Our speech-to-text interface enables you to accurately convert speech into text using an API powered by deep learning neural network algorithms for automatic speech recognition (ASR) in Indic languages. This service enables us to infer in a batch mode over REST protocol to transcribe speech audio files. This also has a streaming service that provides an interface to accept chunks of the continuous audio stream that can be transcribed in real-time to text. Architecture \u00b6 The architecture for ASR service with both batch and real-time mode is shown here. Fig 1 Artifacts \u00b6 The latest image for the ASR service can be found at gcr.io/ekstepspeechrecognition/speech_recognition_model_api:<version> The code repository for the ASR API service can be found at https://github.com/Open-Speech-EkStep/speech-recognition-open-api The deployment infra code repository for the ASR API service can be found at https://github.com/Open-Speech-EkStep/speech-recognition-open-api-infra Infrastructure Requirements \u00b6 1.A fully operational Kubernetes cluster with at least 3 nodes, each containing at least 8CPUs 64Gb RAM and 1 Tesla T4 GPU of 16GB. This is a basic requirement to support 50-100 concurrent users at ease. For higher user support, better GPU configurations are highly recommended. 2.A mountable disk or network drive of at least 50GB to store the models and artifacts for ASR. This disk/drive should be part of the same zone as the K8 cluster so it can be easily mounted to the Kubernetes pods. 3.The cluster should be publicly accessible using an IP exposed by the load balancer. 4.Jenkins can be set up optionally as a deployment pipeline of CI/CD and configured with the infra code repository. Deployment components and steps \u00b6 The various components or modules as shown in the above Fig 1 are required to be installed in a package for the service to work successfully. Loading models files and artifacts : The first towards deployment is dumping the model files for every language into their respective language directories and placing them inside the external mountable disk at a designated location. Once the models are placed, create a file named \u2018model_dict.json\u2019 in the same hierarchy as the language directories. The model_dict.json should have the respective paths of the language model files and the setting for ITN and punctuation for every language. The ASR models for every languauge can be downloaded from \u200b\u200b https://storage.googleapis.com/asr-public-models/data-sources-deployment/ . ITN and punctuation models are downloaded automatically from the buckets,so not needed to be placed manually. Sample model_dict.json { \"en\" : { \"path\" : \"/indian_english/final_model.pt\" , \"enablePunctuation\" : true , \"enableITN\" : true }, \"hi\" : { \"path\" : \"/hindi/hindi.pt\" , \"enablePunctuation\" : true , \"enableITN\" : true }, \"ta\" : { \"path\" : \"/tamil/final_model.pt\" , \"enablePunctuation\" : true , \"enableITN\" : true } } The models from the external volume needs to be mounted at /opt/text_to_speech_open_api/deployed_models/ inside the pod. The volume mounts are specified in https://github.com/Open-Speech-EkStep/speech-recognition-open-api-infra Ngnix : A NGNIX router should be deployed and be publicly exposed as a load balancer service. The ngnix conf file should be updated with proper location blocks to support asr and punctuate endpoints. Sample ngnix.conf server { server_ na me mei t y - dev - asr.ulcaco ntr ib.org; lis ten 443 ssl de fault h tt p 2 ; lis ten [::]: 443 ssl de fault h tt p 2 ipv 6 o nl y=o n ; ssl_cer t i f ica te /e t c/ssl/ulcaco ntr ib_ssl.cr t ; ssl_cer t i f ica te _key /e t c/ssl/ulcaco ntr ib_ssl.key; resolver 127.0.0.11 ; u n derscores_i n _headers o n ; clie nt _max_body_size 10 M; clie nt _header_ t imeou t 600 ; clie nt _body_ t imeou t 600 ; fast cgi_read_ t imeou t 600 ; proxy_co nne c t _ t imeou t 600 ; proxy_se n d_ t imeou t 600 ; proxy_read_ t imeou t 600 ; se n d_ t imeou t 600 ; gzip o n ; gzip_proxied a n y; gzip_ t ypes te x t /plai n te x t /xml te x t /css applica t io n /jso n applica t io n /x - javascrip t te x t /javascrip t applica t io n /javascrip t ; gzip_vary o n ; gzip_disable \"MSIE [1-6]\\.(?!.*SV1)\" ; gzip_comp_level 6 ; gzip_bu ffers 16 8 k; gzip_mi n _le n g t h 512 ; add_header Access - Co ntr ol - Allow - Origi n * always; add_header Access - Co ntr ol - Allow - Headers * always; loca t io n /pu n c tuate { proxy_se t _header Upgrade $h tt p_upgrade; proxy_se t _header Co nne c t io n \"upgrade\" ; proxy_h tt p_versio n 1.1 ; proxy_se t _header X - Forwarded - For $proxy_add_x_ f orwarded_ f or; proxy_se t _header Hos t $hos t ; proxy_pass h tt ps : //i nferen ce.vakya ns h.i n /pu n c tuate ; } loca t io n /asr { resolver 127.0.0.11 ; re turn 302 /asr/; } loca t io n /asr/ { resolver 127.0.0.11 ; proxy_pass h tt p : //asr - model - v 2-e n voy/; proxy_h tt p_versio n 1.1 ; proxy_se t _header Upgrade $h tt p_upgrade; proxy_se t _header Co nne c t io n \"Upgrade\" ; proxy_se t _header Hos t $hos t ; } loca t io n /socke t .io/ { proxy_se t _header Upgrade $h tt p_upgrade; proxy_se t _header Co nne c t io n \"upgrade\" ; proxy_h tt p_versio n 1.1 ; proxy_se t _header X - Forwarded - For $proxy_add_x_ f orwarded_ f or; proxy_se t _header Hos t $hos t ; proxy_pass h tt p : //asr - model - v 2- proxy - server : 9009 /socke t .io/; } } server { server_ na me localhos t ; resolver 127.0.0.11 ; lis ten 50051 h tt p 2 ; loca t io n / { grpc_pass grpc : //asr - model - v 2-e n voy; } } Ngnix is a manual deployment by the infra team and used as a common component for all services. Streaming Proxy : This is an important component for the streaming service to work with Socket.io connections from the browser clients. Envoy : This is a transcoder that helps to convert REST calls from the batch API to GRPC calls to the API. All GRPC and REST connections have to pass through the envoy before it reaches the ASR API servers. Model API server : The model API server for every language or cluster of languages is deployed as separate pods in the cluster. The language clusters and node accelerators can be configured in the file app_config.yaml. The volume paths can also be correctly configured in the deployment.yaml files and values.yaml files. These files post changes can be committed to the repo, https://github.com/Open-Speech-EkStep/speech-recognition-open-api-infra/tree/cdac-deploy The deployment for proxy, envoy and API server language pods can be configured using Jenkins or can be triggered manually by running the below command in a bastion/window node of CDAC. All three components can be deployed at once. python3 deploy.py --namespace <kubernetes namespace> --api-updated True --image-name gcr.io/ekstepspeechrecognition/speech_recognition_model_api --image-version <latest image version> With the successful deployment of components, the pods and services should get spawned for proxy, envoy, and language APIs. API for CDAC: \u00b6 The REST API endpoint can be accessible for POST requests https://cdac.ulcacontrib.org/asr/v1/recognize/<language_code> For streaming from browser, Client SDK can be used for accessing ASR models. https://github.com/Open-Speech-EkStep/speech-recognition-open-api-client TEXT TO SPEECH SERVICE(TTS): \u00b6 Text To Speech (TTS), also known as Speech Synthesis, is a process where text is converted into a human-sounding voice. TTS has been a popular choice for developers and business users alike when building IVR (Interactive Voice Response) solutions and other voice applications, as it accelerates time to production without having to record audio files with human voices. Using recorded files requires recording each message with a human voice, whereas TTS prompts can be dynamically generated from raw text. Our TTS service can enable us to generate life-like speech synthesis in both male and female voices for an array of Indic languages like Hindi, Tamil, Malayalam,Kannada, and many more. Architecture: \u00b6 The architecture for TTS service is shown here. This service only supports batch inference from REST clients via POST requests. The response returned by the API contains an audio bytes payload for successful inference. Fig 2 Artifacts: \u00b6 The latest image for the TTS service can be found at gcr.io/ekstepspeechrecognition/text_to_speech_open_api:<version> The code repository for the TTS API service can be found at https://github.com/Open-Speech-EkStep/text-to-speech-open-api The deployment infra code repository for the TTS API service can be found at https://github.com/Open-Speech-EkStep/text-to-speech-open-api-infra Infrastructure Requirements: \u00b6 A fully operational Kubernetes cluster with at least 3 nodes, each containing at least 8CPUs 64Gb RAM and 1 Tesla T4 GPU of 16GB. This is a basic requirement to support 50-100 concurrent users at ease. For higher user support, better GPU configurations are highly recommended. A mountable disk or network drive of at least 50GB to store the models and artifacts for TTS. This disk/drive should be part of the same zone as the K8 cluster so it can be easily mounted to the Kubernetes pods. The cluster should be publicly accessible using an IP exposed by the load balancer. Jenkins can be set up optionally as a deployment pipeline of CI/CD and configured with the infra code repository. Deployment components and steps: \u00b6 The various components or modules as shown in the above Fig 1 are required to be installed in a package for the service to work successfully. Loading models files and artifacts : The first step towards deployment is dumping the model files for every language into their respective language directories and placing them inside the external mountable disk at a designated location. Here for TTS we have TTS models and transliteration models to be placed in the respective directories. Refer readme for directory structure https://github.com/Open-Speech-EkStep/text-to-speech-open-api . TTS MODELS :Once the tts models are downloaded from https://storage.googleapis.com/vakyaansh-open-models/tts-models/ , create a file named model_dict.json in the same hierarchy as the language directories. The model_dict.json should have the respective paths of the language model files and the setting for ITN and punctuation for every language. Sample model_dict.json { \"hi\" : { \"male_glow\" : \"hindi/male/glow_tts\" , \"male_hifi\" : \"hindi/male/hifi_tts\" , \"female_glow\" : \"hindi/female/glow_tts\" , \"female_hifi\" : \"hindi/female/hifi_tts\" } } TRANSLITERATION MODELS :The transliteration models aren\u2019t required to be downloaded and placed manually for the first time. But the file named \u2018default_lineup.json\u2019 needs to be placed in the directory named \u2018translit_models\u2019 in the external volume.This helps to get the models downloaded for the first time. The contents of the default_lineup.json should be: { \"bn\" : { \"name\" : \"Bengali - \u09ac\u09be\u0982\u09b2\u09be\" , \"eng_name\" : \"bengali\" , \"script\" : \"bengali/bn_scripts.json\" , \"vocab\" : \"bengali/bn_words_a4b.json\" , \"weight\" : \"bengali/bn_101_model.pth\" }, \"gu\" : { \"name\" : \"Gujarati - \u0a97\u0ac1\u0a9c\u0ab0\u0abe\u0aa4\u0ac0\" , \"eng_name\" : \"gujarati\" , \"script\" : \"gujarati/gu_scripts.json\" , \"vocab\" : \"gujarati/gu_words_a4b.json\" , \"weight\" : \"gujarati/gu_101_model.pth\" }, \"hi\" : { \"name\" : \"Hindi - \u0939\u093f\u0902\u0926\u0940\" , \"eng_name\" : \"hindi\" , \"script\" : \"hindi/hi_scripts.json\" , \"vocab\" : \"hindi/hi_words_a4b.json\" , \"weight\" : \"hindi/hi_111_model.pth\" }, \"kn\" : { \"name\" : \"Kannada - \u0c95\u0ca8\u0ccd\u0ca8\u0ca1\" , \"eng_name\" : \"kannada\" , \"script\" : \"kannada/kn_scripts.json\" , \"vocab\" : \"kannada/kn_words_a4b.json\" , \"weight\" : \"kannada/kn_101_model.pth\" }, \"gom\" : { \"name\" : \"Konkani (Goan) - \u0915\u094b\u0902\u0915\u0923\u0940\" , \"eng_name\" : \"konkani\" , \"script\" : \"konkani/gom_scripts.json\" , \"vocab\" : \"konkani/gom_words_subset.json\" , \"weight\" : \"konkani/gom_122_model.pth\" }, \"mai\" : { \"name\" : \"Maithili - \u092e\u0948\u0925\u093f\u0932\u0940\" , \"eng_name\" : \"maithili\" , \"script\" : \"maithili/mai_scripts.json\" , \"vocab\" : \"maithili/mai_words_subset.json\" , \"weight\" : \"maithili/mai_122_model.pth\" }, \"ml\" : { \"name\" : \"Malayalam - \u0d2e\u0d32\u0d2f\u0d3e\u0d33\u0d02\" , \"eng_name\" : \"malayalam\" , \"script\" : \"malayalam/ml_scripts.json\" , \"vocab\" : \"malayalam/ml_words_a4b.json\" , \"weight\" : \"malayalam/ml_101_model.pth\" }, \"mr\" : { \"name\" : \"Marathi - \u092e\u0930\u093e\u0920\u0940\" , \"eng_name\" : \"marathi\" , \"script\" : \"marathi/mr_scripts.json\" , \"vocab\" : \"marathi/mr_words_a4b.json\" , \"weight\" : \"marathi/mr_102_model.pth\" }, \"pa\" : { \"name\" : \"Panjabi - \u0a2a\u0a70\u0a1c\u0a3e\u0a2c\u0a40\" , \"eng_name\" : \"panjabi\" , \"script\" : \"panjabi/pa_scripts.json\" , \"vocab\" : \"panjabi/pa_words_a4b.json\" , \"weight\" : \"panjabi/pa_101_model.pth\" }, \"sd\" : { \"name\" : \"Sindhi - \u0633\u0646\u068c\u064a\u200e\" , \"eng_name\" : \"sindhi\" , \"script\" : \"sindhi/sd_scripts.json\" , \"vocab\" : \"sindhi/sd_words_ccset.json\" , \"weight\" : \"sindhi/sd_101_model.pth\" }, \"si\" : { \"name\" : \"Sinhala - \u0dc3\u0dd2\u0d82\u0dc4\u0dbd\" , \"eng_name\" : \"sinhala\" , \"script\" : \"sinhala/si_scripts.json\" , \"vocab\" : \"sinhala/si_words_ccset.json\" , \"weight\" : \"sinhala/si_101_model.pth\" }, \"ta\" : { \"name\" : \"Tamil - \u0ba4\u0bae\u0bbf\u0bb4\u0bcd\" , \"eng_name\" : \"tamil\" , \"script\" : \"tamil/ta_scripts.json\" , \"vocab\" : \"tamil/ta_words_a4b.json\" , \"weight\" : \"tamil/ta_101_model.pth\" }, \"te\" : { \"name\" : \"Telugu - \u0c24\u0c46\u0c32\u0c41\u0c17\u0c41\" , \"eng_name\" : \"telugu\" , \"script\" : \"telugu/te_scripts.json\" , \"vocab\" : \"telugu/te_words_a4b.json\" , \"weight\" : \"telugu/te_101_model.pth\" }, \"ur\" : { \"name\" : \"Urdu - \u0627\u064f\u0631\u062f\u064f\u0648\" , \"eng_name\" : \"urdu\" , \"script\" : \"urdu/ur_scripts.json\" , \"vocab\" : \"urdu/ur_words_ccset.json\" , \"weight\" : \"urdu/ur_101_model.pth\" } The models from the external volume needs to be mounted at /opt/text_to_speech_open_api/deployed_models/ inside the pod. The volume mounts are specified in https://github.com/Open-Speech-EkStep/text-to-speech-open-api-infra Ngnix : A NGNIX router should be deployed and be publicly exposed as a load balancer service.It acts as a reverse-proxy for all traffic coming in. The ngnix conf file should be updated with proper location blocks to support TTS endpoints similar to the ASR implementation. Ngnix is a manual deployment by the infra team and used as a common component for all services. Envoy : Here for TTS, the envoy acts as a load balancer service for redirecting traffic to the API pods at the backend.The envoy is responsible for handling all REST based requests to the API. Model API server : The model API server for every language or cluster of languages is deployed as separate pods in the cluster. The language clusters and node accelerators can be configured in the file app_config.yaml. The volume paths can also be correctly configured in the deployment.yaml files and values.yaml files. These files post changes can be committed to the repo, https://github.com/Open-Speech-EkStep/text-to-speech-open-api-infra/tree/cdac-deploy The deployment for envoy and API server language pods can be configured using Jenkins or can be triggered manually by running the below command in a bastion/window node of CDAC. All three components can be deployed at once. python3 deploy.py --namespace <kubernetes namespace> --api-updated True --image-name gcr.io/ekstepspeechrecognition/text-to-speech-open-api --image-version <latest image version> With the successful deployment of components, the pods and services should get spawned for envoy, and language APIs. API for CDAC: \u00b6 The REST API endpoint can be accessible for POST requests https://cdac.ulcacontrib.org/tts/v1/<language_code> The below figure shows how our pods are able to load the model files from the volume mounts and write back application logs to the volume. Fig 3 Contributing \u00b6 Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request License \u00b6 Distributed under the [MIT] License. See LICENSE for more information. Git repository \u00b6 https://github.com/Open-Speech-EkStep/text-to-speech-open-api-infra.git Contact \u00b6 Connect with community on Gitter Project Link: https://github.com/Open-Speech-EkStep/text-to-speech-open-api-infra.git","title":"API Deployment Guide"},{"location":"api_deployment/#asr-and-tts-api-deployment-guide","text":"We have developed two services for deployment in the CDAC Kubernetes environment. The services are:","title":"ASR and TTS API Deployment Guide"},{"location":"api_deployment/#automatic-speech-recognitionasr","text":"Our speech-to-text interface enables you to accurately convert speech into text using an API powered by deep learning neural network algorithms for automatic speech recognition (ASR) in Indic languages. This service enables us to infer in a batch mode over REST protocol to transcribe speech audio files. This also has a streaming service that provides an interface to accept chunks of the continuous audio stream that can be transcribed in real-time to text.","title":"AUTOMATIC SPEECH RECOGNITION(ASR)"},{"location":"api_deployment/#architecture","text":"The architecture for ASR service with both batch and real-time mode is shown here. Fig 1","title":"Architecture"},{"location":"api_deployment/#artifacts","text":"The latest image for the ASR service can be found at gcr.io/ekstepspeechrecognition/speech_recognition_model_api:<version> The code repository for the ASR API service can be found at https://github.com/Open-Speech-EkStep/speech-recognition-open-api The deployment infra code repository for the ASR API service can be found at https://github.com/Open-Speech-EkStep/speech-recognition-open-api-infra","title":"Artifacts"},{"location":"api_deployment/#infrastructure-requirements","text":"1.A fully operational Kubernetes cluster with at least 3 nodes, each containing at least 8CPUs 64Gb RAM and 1 Tesla T4 GPU of 16GB. This is a basic requirement to support 50-100 concurrent users at ease. For higher user support, better GPU configurations are highly recommended. 2.A mountable disk or network drive of at least 50GB to store the models and artifacts for ASR. This disk/drive should be part of the same zone as the K8 cluster so it can be easily mounted to the Kubernetes pods. 3.The cluster should be publicly accessible using an IP exposed by the load balancer. 4.Jenkins can be set up optionally as a deployment pipeline of CI/CD and configured with the infra code repository.","title":"Infrastructure Requirements"},{"location":"api_deployment/#deployment-components-and-steps","text":"The various components or modules as shown in the above Fig 1 are required to be installed in a package for the service to work successfully. Loading models files and artifacts : The first towards deployment is dumping the model files for every language into their respective language directories and placing them inside the external mountable disk at a designated location. Once the models are placed, create a file named \u2018model_dict.json\u2019 in the same hierarchy as the language directories. The model_dict.json should have the respective paths of the language model files and the setting for ITN and punctuation for every language. The ASR models for every languauge can be downloaded from \u200b\u200b https://storage.googleapis.com/asr-public-models/data-sources-deployment/ . ITN and punctuation models are downloaded automatically from the buckets,so not needed to be placed manually. Sample model_dict.json { \"en\" : { \"path\" : \"/indian_english/final_model.pt\" , \"enablePunctuation\" : true , \"enableITN\" : true }, \"hi\" : { \"path\" : \"/hindi/hindi.pt\" , \"enablePunctuation\" : true , \"enableITN\" : true }, \"ta\" : { \"path\" : \"/tamil/final_model.pt\" , \"enablePunctuation\" : true , \"enableITN\" : true } } The models from the external volume needs to be mounted at /opt/text_to_speech_open_api/deployed_models/ inside the pod. The volume mounts are specified in https://github.com/Open-Speech-EkStep/speech-recognition-open-api-infra Ngnix : A NGNIX router should be deployed and be publicly exposed as a load balancer service. The ngnix conf file should be updated with proper location blocks to support asr and punctuate endpoints. Sample ngnix.conf server { server_ na me mei t y - dev - asr.ulcaco ntr ib.org; lis ten 443 ssl de fault h tt p 2 ; lis ten [::]: 443 ssl de fault h tt p 2 ipv 6 o nl y=o n ; ssl_cer t i f ica te /e t c/ssl/ulcaco ntr ib_ssl.cr t ; ssl_cer t i f ica te _key /e t c/ssl/ulcaco ntr ib_ssl.key; resolver 127.0.0.11 ; u n derscores_i n _headers o n ; clie nt _max_body_size 10 M; clie nt _header_ t imeou t 600 ; clie nt _body_ t imeou t 600 ; fast cgi_read_ t imeou t 600 ; proxy_co nne c t _ t imeou t 600 ; proxy_se n d_ t imeou t 600 ; proxy_read_ t imeou t 600 ; se n d_ t imeou t 600 ; gzip o n ; gzip_proxied a n y; gzip_ t ypes te x t /plai n te x t /xml te x t /css applica t io n /jso n applica t io n /x - javascrip t te x t /javascrip t applica t io n /javascrip t ; gzip_vary o n ; gzip_disable \"MSIE [1-6]\\.(?!.*SV1)\" ; gzip_comp_level 6 ; gzip_bu ffers 16 8 k; gzip_mi n _le n g t h 512 ; add_header Access - Co ntr ol - Allow - Origi n * always; add_header Access - Co ntr ol - Allow - Headers * always; loca t io n /pu n c tuate { proxy_se t _header Upgrade $h tt p_upgrade; proxy_se t _header Co nne c t io n \"upgrade\" ; proxy_h tt p_versio n 1.1 ; proxy_se t _header X - Forwarded - For $proxy_add_x_ f orwarded_ f or; proxy_se t _header Hos t $hos t ; proxy_pass h tt ps : //i nferen ce.vakya ns h.i n /pu n c tuate ; } loca t io n /asr { resolver 127.0.0.11 ; re turn 302 /asr/; } loca t io n /asr/ { resolver 127.0.0.11 ; proxy_pass h tt p : //asr - model - v 2-e n voy/; proxy_h tt p_versio n 1.1 ; proxy_se t _header Upgrade $h tt p_upgrade; proxy_se t _header Co nne c t io n \"Upgrade\" ; proxy_se t _header Hos t $hos t ; } loca t io n /socke t .io/ { proxy_se t _header Upgrade $h tt p_upgrade; proxy_se t _header Co nne c t io n \"upgrade\" ; proxy_h tt p_versio n 1.1 ; proxy_se t _header X - Forwarded - For $proxy_add_x_ f orwarded_ f or; proxy_se t _header Hos t $hos t ; proxy_pass h tt p : //asr - model - v 2- proxy - server : 9009 /socke t .io/; } } server { server_ na me localhos t ; resolver 127.0.0.11 ; lis ten 50051 h tt p 2 ; loca t io n / { grpc_pass grpc : //asr - model - v 2-e n voy; } } Ngnix is a manual deployment by the infra team and used as a common component for all services. Streaming Proxy : This is an important component for the streaming service to work with Socket.io connections from the browser clients. Envoy : This is a transcoder that helps to convert REST calls from the batch API to GRPC calls to the API. All GRPC and REST connections have to pass through the envoy before it reaches the ASR API servers. Model API server : The model API server for every language or cluster of languages is deployed as separate pods in the cluster. The language clusters and node accelerators can be configured in the file app_config.yaml. The volume paths can also be correctly configured in the deployment.yaml files and values.yaml files. These files post changes can be committed to the repo, https://github.com/Open-Speech-EkStep/speech-recognition-open-api-infra/tree/cdac-deploy The deployment for proxy, envoy and API server language pods can be configured using Jenkins or can be triggered manually by running the below command in a bastion/window node of CDAC. All three components can be deployed at once. python3 deploy.py --namespace <kubernetes namespace> --api-updated True --image-name gcr.io/ekstepspeechrecognition/speech_recognition_model_api --image-version <latest image version> With the successful deployment of components, the pods and services should get spawned for proxy, envoy, and language APIs.","title":"Deployment components and steps"},{"location":"api_deployment/#api-for-cdac","text":"The REST API endpoint can be accessible for POST requests https://cdac.ulcacontrib.org/asr/v1/recognize/<language_code> For streaming from browser, Client SDK can be used for accessing ASR models. https://github.com/Open-Speech-EkStep/speech-recognition-open-api-client","title":"API for CDAC:"},{"location":"api_deployment/#text-to-speech-servicetts","text":"Text To Speech (TTS), also known as Speech Synthesis, is a process where text is converted into a human-sounding voice. TTS has been a popular choice for developers and business users alike when building IVR (Interactive Voice Response) solutions and other voice applications, as it accelerates time to production without having to record audio files with human voices. Using recorded files requires recording each message with a human voice, whereas TTS prompts can be dynamically generated from raw text. Our TTS service can enable us to generate life-like speech synthesis in both male and female voices for an array of Indic languages like Hindi, Tamil, Malayalam,Kannada, and many more.","title":"TEXT TO SPEECH SERVICE(TTS):"},{"location":"api_deployment/#architecture_1","text":"The architecture for TTS service is shown here. This service only supports batch inference from REST clients via POST requests. The response returned by the API contains an audio bytes payload for successful inference. Fig 2","title":"Architecture:"},{"location":"api_deployment/#artifacts_1","text":"The latest image for the TTS service can be found at gcr.io/ekstepspeechrecognition/text_to_speech_open_api:<version> The code repository for the TTS API service can be found at https://github.com/Open-Speech-EkStep/text-to-speech-open-api The deployment infra code repository for the TTS API service can be found at https://github.com/Open-Speech-EkStep/text-to-speech-open-api-infra","title":"Artifacts:"},{"location":"api_deployment/#infrastructure-requirements_1","text":"A fully operational Kubernetes cluster with at least 3 nodes, each containing at least 8CPUs 64Gb RAM and 1 Tesla T4 GPU of 16GB. This is a basic requirement to support 50-100 concurrent users at ease. For higher user support, better GPU configurations are highly recommended. A mountable disk or network drive of at least 50GB to store the models and artifacts for TTS. This disk/drive should be part of the same zone as the K8 cluster so it can be easily mounted to the Kubernetes pods. The cluster should be publicly accessible using an IP exposed by the load balancer. Jenkins can be set up optionally as a deployment pipeline of CI/CD and configured with the infra code repository.","title":"Infrastructure Requirements:"},{"location":"api_deployment/#deployment-components-and-steps_1","text":"The various components or modules as shown in the above Fig 1 are required to be installed in a package for the service to work successfully. Loading models files and artifacts : The first step towards deployment is dumping the model files for every language into their respective language directories and placing them inside the external mountable disk at a designated location. Here for TTS we have TTS models and transliteration models to be placed in the respective directories. Refer readme for directory structure https://github.com/Open-Speech-EkStep/text-to-speech-open-api . TTS MODELS :Once the tts models are downloaded from https://storage.googleapis.com/vakyaansh-open-models/tts-models/ , create a file named model_dict.json in the same hierarchy as the language directories. The model_dict.json should have the respective paths of the language model files and the setting for ITN and punctuation for every language. Sample model_dict.json { \"hi\" : { \"male_glow\" : \"hindi/male/glow_tts\" , \"male_hifi\" : \"hindi/male/hifi_tts\" , \"female_glow\" : \"hindi/female/glow_tts\" , \"female_hifi\" : \"hindi/female/hifi_tts\" } } TRANSLITERATION MODELS :The transliteration models aren\u2019t required to be downloaded and placed manually for the first time. But the file named \u2018default_lineup.json\u2019 needs to be placed in the directory named \u2018translit_models\u2019 in the external volume.This helps to get the models downloaded for the first time. The contents of the default_lineup.json should be: { \"bn\" : { \"name\" : \"Bengali - \u09ac\u09be\u0982\u09b2\u09be\" , \"eng_name\" : \"bengali\" , \"script\" : \"bengali/bn_scripts.json\" , \"vocab\" : \"bengali/bn_words_a4b.json\" , \"weight\" : \"bengali/bn_101_model.pth\" }, \"gu\" : { \"name\" : \"Gujarati - \u0a97\u0ac1\u0a9c\u0ab0\u0abe\u0aa4\u0ac0\" , \"eng_name\" : \"gujarati\" , \"script\" : \"gujarati/gu_scripts.json\" , \"vocab\" : \"gujarati/gu_words_a4b.json\" , \"weight\" : \"gujarati/gu_101_model.pth\" }, \"hi\" : { \"name\" : \"Hindi - \u0939\u093f\u0902\u0926\u0940\" , \"eng_name\" : \"hindi\" , \"script\" : \"hindi/hi_scripts.json\" , \"vocab\" : \"hindi/hi_words_a4b.json\" , \"weight\" : \"hindi/hi_111_model.pth\" }, \"kn\" : { \"name\" : \"Kannada - \u0c95\u0ca8\u0ccd\u0ca8\u0ca1\" , \"eng_name\" : \"kannada\" , \"script\" : \"kannada/kn_scripts.json\" , \"vocab\" : \"kannada/kn_words_a4b.json\" , \"weight\" : \"kannada/kn_101_model.pth\" }, \"gom\" : { \"name\" : \"Konkani (Goan) - \u0915\u094b\u0902\u0915\u0923\u0940\" , \"eng_name\" : \"konkani\" , \"script\" : \"konkani/gom_scripts.json\" , \"vocab\" : \"konkani/gom_words_subset.json\" , \"weight\" : \"konkani/gom_122_model.pth\" }, \"mai\" : { \"name\" : \"Maithili - \u092e\u0948\u0925\u093f\u0932\u0940\" , \"eng_name\" : \"maithili\" , \"script\" : \"maithili/mai_scripts.json\" , \"vocab\" : \"maithili/mai_words_subset.json\" , \"weight\" : \"maithili/mai_122_model.pth\" }, \"ml\" : { \"name\" : \"Malayalam - \u0d2e\u0d32\u0d2f\u0d3e\u0d33\u0d02\" , \"eng_name\" : \"malayalam\" , \"script\" : \"malayalam/ml_scripts.json\" , \"vocab\" : \"malayalam/ml_words_a4b.json\" , \"weight\" : \"malayalam/ml_101_model.pth\" }, \"mr\" : { \"name\" : \"Marathi - \u092e\u0930\u093e\u0920\u0940\" , \"eng_name\" : \"marathi\" , \"script\" : \"marathi/mr_scripts.json\" , \"vocab\" : \"marathi/mr_words_a4b.json\" , \"weight\" : \"marathi/mr_102_model.pth\" }, \"pa\" : { \"name\" : \"Panjabi - \u0a2a\u0a70\u0a1c\u0a3e\u0a2c\u0a40\" , \"eng_name\" : \"panjabi\" , \"script\" : \"panjabi/pa_scripts.json\" , \"vocab\" : \"panjabi/pa_words_a4b.json\" , \"weight\" : \"panjabi/pa_101_model.pth\" }, \"sd\" : { \"name\" : \"Sindhi - \u0633\u0646\u068c\u064a\u200e\" , \"eng_name\" : \"sindhi\" , \"script\" : \"sindhi/sd_scripts.json\" , \"vocab\" : \"sindhi/sd_words_ccset.json\" , \"weight\" : \"sindhi/sd_101_model.pth\" }, \"si\" : { \"name\" : \"Sinhala - \u0dc3\u0dd2\u0d82\u0dc4\u0dbd\" , \"eng_name\" : \"sinhala\" , \"script\" : \"sinhala/si_scripts.json\" , \"vocab\" : \"sinhala/si_words_ccset.json\" , \"weight\" : \"sinhala/si_101_model.pth\" }, \"ta\" : { \"name\" : \"Tamil - \u0ba4\u0bae\u0bbf\u0bb4\u0bcd\" , \"eng_name\" : \"tamil\" , \"script\" : \"tamil/ta_scripts.json\" , \"vocab\" : \"tamil/ta_words_a4b.json\" , \"weight\" : \"tamil/ta_101_model.pth\" }, \"te\" : { \"name\" : \"Telugu - \u0c24\u0c46\u0c32\u0c41\u0c17\u0c41\" , \"eng_name\" : \"telugu\" , \"script\" : \"telugu/te_scripts.json\" , \"vocab\" : \"telugu/te_words_a4b.json\" , \"weight\" : \"telugu/te_101_model.pth\" }, \"ur\" : { \"name\" : \"Urdu - \u0627\u064f\u0631\u062f\u064f\u0648\" , \"eng_name\" : \"urdu\" , \"script\" : \"urdu/ur_scripts.json\" , \"vocab\" : \"urdu/ur_words_ccset.json\" , \"weight\" : \"urdu/ur_101_model.pth\" } The models from the external volume needs to be mounted at /opt/text_to_speech_open_api/deployed_models/ inside the pod. The volume mounts are specified in https://github.com/Open-Speech-EkStep/text-to-speech-open-api-infra Ngnix : A NGNIX router should be deployed and be publicly exposed as a load balancer service.It acts as a reverse-proxy for all traffic coming in. The ngnix conf file should be updated with proper location blocks to support TTS endpoints similar to the ASR implementation. Ngnix is a manual deployment by the infra team and used as a common component for all services. Envoy : Here for TTS, the envoy acts as a load balancer service for redirecting traffic to the API pods at the backend.The envoy is responsible for handling all REST based requests to the API. Model API server : The model API server for every language or cluster of languages is deployed as separate pods in the cluster. The language clusters and node accelerators can be configured in the file app_config.yaml. The volume paths can also be correctly configured in the deployment.yaml files and values.yaml files. These files post changes can be committed to the repo, https://github.com/Open-Speech-EkStep/text-to-speech-open-api-infra/tree/cdac-deploy The deployment for envoy and API server language pods can be configured using Jenkins or can be triggered manually by running the below command in a bastion/window node of CDAC. All three components can be deployed at once. python3 deploy.py --namespace <kubernetes namespace> --api-updated True --image-name gcr.io/ekstepspeechrecognition/text-to-speech-open-api --image-version <latest image version> With the successful deployment of components, the pods and services should get spawned for envoy, and language APIs.","title":"Deployment components and steps:"},{"location":"api_deployment/#api-for-cdac_1","text":"The REST API endpoint can be accessible for POST requests https://cdac.ulcacontrib.org/tts/v1/<language_code> The below figure shows how our pods are able to load the model files from the volume mounts and write back application logs to the volume. Fig 3","title":"API for CDAC:"},{"location":"api_deployment/#contributing","text":"Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request","title":"Contributing"},{"location":"api_deployment/#license","text":"Distributed under the [MIT] License. See LICENSE for more information.","title":"License"},{"location":"api_deployment/#git-repository","text":"https://github.com/Open-Speech-EkStep/text-to-speech-open-api-infra.git","title":"Git repository"},{"location":"api_deployment/#contact","text":"Connect with community on Gitter Project Link: https://github.com/Open-Speech-EkStep/text-to-speech-open-api-infra.git","title":"Contact"},{"location":"asr_model_api/","text":"Speech Recognition model API \u00b6 About The Project \u00b6 Our speech to text interface enables you to accurately convert speech into text using an API powered by deep learning neural network algorithms for automatic speech recognition (ASR). This is enabled to provide the following features: Speech to text transcription support for a growing list of indic languages. Transcribe your content in real time from stored files or audio bytes. Generate subtitle or transcript for your audios as per your choice of output. Support for various audio formats like WAV,MP3,PCM. [beta]Enables transcription optimized for domain-specific quality requirements associating domain models in backend. [beta]Speech-to-Text accurately punctuates transcriptions (e.g., commas, question marks, and periods). The Developer documentation provides you with a complete set of guidelines which you need to get started with: Architecture overview API reference Client Code reference Setup and getting started guide Extend this project Contribute to this project Architecture Overview \u00b6 The logical architecture here is built with a grpc server hosting our speech recognition models and dependencies, which can be run in any environment or docker. With gRPC we can define our service once in a .proto file and generate clients and servers in any of gRPC\u2019s supported languages, which in turn can be run in environments ranging from servers inside a large data center to your own tablet \u2014 all the complexity of communication between different languages and environments is handled for you by gRPC. We also get all the advantages of working with protocol buffers, including efficient serialization, a simple IDL, and easy interface updating. In gRPC, a client application can directly call a method on a server application on a different machine as if it were a local object, making it easier for you to create distributed applications and services. Here we can use the grpc generated stubs from client code in any language and make requests using gRPC calls and receive the responses from the server.On the server side, the server implements this interface and runs a gRPC server to handle client calls. By default, gRPC uses Protocol Buffers, Google\u2019s mature open source mechanism for serializing structured data (although it can be used with other data formats such as JSON). gRPC uses protoc with a special gRPC plugin to generate code from your proto file: you get generated gRPC client and server code, as well as the regular protocol buffer code for populating, serializing, and retrieving your message types. Apart from using gRPC stubs, we have added the support for REST calls to the gRPC server via an api-gateway. With API Gateway for gRPC, you can use the API management capabilities of API Gateway to add monitoring, hosting, tracing, authentication, and more to your gRPC services on Cloud Run. In addition, once you specify special mapping rules, API Gateway translates RESTful JSON over HTTP into gRPC requests. This means that you can deploy a gRPC server managed by API Gateway and call its API using a gRPC or JSON/HTTP client, giving you much more flexibility and ease of integration with other systems. API reference \u00b6 Our API has predictable resource-oriented URLs, accepts form-encoded request bodies, returns JSON-encoded responses, and uses standard HTTP response codes, authentication, and verbs. Base URL https://<gateway-url>/v1/recognize/ Authentication Authentication to the API is performed via HTTP Basic Auth. Provide your API key as the basic auth username value. You do not need to provide a password. The Stripe API uses API keys to authenticate requests. If you need to authenticate via bearer auth (e.g., for a cross-origin request), use -H \"Authorization: Bearer sk_test_4eC39HqLyjWDarjtT1zdp7dc\" As of now all API requests can be made over HTTPS or HTTP both. Calls made over plain HTTP will fail going ahead. Errors Our API uses HTTP response codes to indicate the success or failure of an API request. 200 - OK Everything worked as expected. 400 - Bad Request The request was unacceptable, often due to missing a required parameter. 401 - Unauthorized No valid API key provided. 402 - Request Failed The parameters were valid but the request failed. 403 - Forbidden The API key doesn't have permissions to perform the request. 404 - Not Found The requested resource doesn't exist. 409 - Conflict The request conflicts with another request (perhaps due to using the same idempotent key). 429 - Too Many Requests Too many requests hit the API too quickly. We recommend an exponential backoff of your requests. 500, 502, 503, 504 - Server Errors Something went wrong on Stripe's end. (These are rare.) Handling errors We are in process of writing code that gracefully handles all possible API exceptions. This is something work in progress and will be available soon. Endpoints Supported The recognize object POST /v1/recognize/ Request Attributes config - Holds the configuration objects for language,transcriptionFormat and audio format. Child Attributes: language : REQUIRED - Specify the value of the language and its attributes Child Attributes: value : string : REQUIRED - Specify a langauge code for the audio transcription. Enum: ['en', 'hi', 'ta', 'te', 'kn', 'or', 'gu', 'en-IN'] transcriptionFormat : string : OPTIONAL - Determine the output format as either SRT or TRANSCRIPT.Default value is TRANSCRIPT. Enum : ['SRT','TRANSCRIPT'] audioFormat : string : OPTIONAL - Determine the input audio formats from the list supported.Default value is WAV. Enum : ['WAV', 'MP3', 'PCM'] audio - Specify the audio properties configuration. Either provide the audio URL or the audio bytes. Child Attributes: Either of the below attributes is REQUIRED. audioUri : string : REQUIRED - Specify the audio URL path audioContent : string : REQUIRED - Specify the byte representation of the audio as part of the request. Request body Example Schema { \"config\": { \"language\": { \"value\": \"hi\" }, \"transcriptionFormat\": \"SRT\", \"audioFormat\": \"WAV\" }, \"audio\": { \"audioUri\": \"https://codmento.com/ekstep/test/changed.wav\" } } Responses Code Description 200 On successful completion of the job. Response Attributes srt : string - The subtitle as output if transcription format is chosen at SRT. transcript : string - The transcript as output if transcription format is chosen at TRANSCRIPT. Response body Example Schema { \"srt\": \"1\\n00:00:01,29 --> 00:00:04,88\\n\u0939\u093f\u0902\u0926\u0940 \u092e\u0949\u0921\u0932 \u091f\u0947\u0938\u094d\u091f \u0915\u0930 \u0930\u0939\u0940 \u0939\u0942\u0902\\n\\n2\\n00:00:05,00 --> 00:00:09,89\\n\u0915\u0948\u0938\u093e \u091a\u0932 \u0930\u0939\u093e \u0939\u0948 \u092e\u0947\u0930\u0947 \u0935\u093f\u0902\u0921\u094b\u091c \u0932\u0948\u092a\u091f\u0949\u092a \u0938\u0947\\n\\n\" } { \"transcript\": \"\u0939\u093f\u0902\u0926\u0940 \u092e\u0949\u0921\u0932 \u091f\u0947\u0938\u094d\u091f \u0915\u0930 \u0930\u0939\u0940 \u0939\u0942\u0902 \u0915\u0948\u0938\u093e \u091a\u0932 \u0930\u0939\u093e \u0939\u0948 \u092e\u0947\u0930\u0947 \u0935\u093f\u0902\u0921\u094b\u091c \u0932\u0948\u092a\u091f\u0949\u092a \u0938\u0947\" } Responses Code Description 400 On input errors causing a failure in the job. Response Attributes code : string - status code as encountered in the processing life-cycle. message : string - human understandable format. Response body Example Schema { \"code\": 400, \"message\": \"config.audioFormat: invalid value \\\"MP4\\\" for type type.googleapis.com/ekstep.speech_recognition.RecognitionConfig.AudioFormat\" } Responses Code Description 500 Internal error causing a failure in the job. Response Attributes code : string - status code as encountered in the processing life-cycle. message : string - human understandable format. Response body Example Schema { \"code\": 2, \"message\": \"Exception calling application: An unknown error has occurred.Please try again.\" } Sample Request curl --location --request POST 'https://<gateway-url>/v1/recognize/' \\ --header 'Content-Type: text/plain' \\ --data-raw '{ \"config\": { \"language\": { \"value\": \"hi\" }, \"transcriptionFormat\": \"SRT\", \"audioFormat\": \"WAV\" }, \"audio\": { \"audioUri\": \"https://codmento.com/ekstep/test/changed.wav\" } }' Sample Response { \"srt\": \"1\\n00:00:01,29 --> 00:00:04,88\\n\u0939\u093f\u0902\u0926\u0940 \u092e\u0949\u0921\u0932 \u091f\u0947\u0938\u094d\u091f \u0915\u0930 \u0930\u0939\u0940 \u0939\u0942\u0902\\n\\n2\\n00:00:05,00 --> 00:00:09,89\\n\u0915\u0948\u0938\u093e \u091a\u0932 \u0930\u0939\u093e \u0939\u0948 \u092e\u0947\u0930\u0947 \u0935\u093f\u0902\u0921\u094b\u091c \u0932\u0948\u092a\u091f\u0949\u092a \u0938\u0947\\n\\n\" } Sample Request curl --location --request POST 'https://<gateway-url>/v1/recognize/' \\ --header 'Content-Type: text/plain' \\ --data-raw '{ \"config\": { \"language\": { \"value\": \"hi\" }, \"transcriptionFormat\": \"TRANSCRIPT\", \"audioFormat\": \"WAV\" }, \"audio\": { \"audioUri\": \"https://codmento.com/ekstep/test/changed.wav\" } }' Sample Response { \"transcript\": \"\u0939\u093f\u0902\u0926\u0940 \u092e\u0949\u0921\u0932 \u091f\u0947\u0938\u094d\u091f \u0915\u0930 \u0930\u0939\u0940 \u0939\u0942\u0902 \u0915\u0948\u0938\u093e \u091a\u0932 \u0930\u0939\u093e \u0939\u0948 \u092e\u0947\u0930\u0947 \u0935\u093f\u0902\u0921\u094b\u091c \u0932\u0948\u092a\u091f\u0949\u092a \u0938\u0947\" } Client Code reference \u00b6 We provide client libraries as stubs for different programming languages. python import grpc from stub.speech_recognition_open_api_pb2_grpc import SpeechRecognizerStub from stub.speech_recognition_open_api_pb2 import Language, RecognitionConfig, RecognitionAudio, \\ SpeechRecognitionRequest import wave from grpc_interceptor import ClientCallDetails, ClientInterceptor class GrpcAuth(grpc.AuthMetadataPlugin): def __init__(self, key): self._key = key def __call__(self, context, callback): callback((('rpc-auth-header', self._key),), None) class MetadataClientInterceptor(ClientInterceptor): def __init__(self, key): self._key = key def intercept( self, method, request_or_iterator, call_details: grpc.ClientCallDetails, ): new_details = ClientCallDetails( call_details.method, call_details.timeout, [(\"authorization\", \"Bearer \" + self._key)], call_details.credentials, call_details.wait_for_ready, call_details.compression, ) return method(request_or_iterator, new_details) def read_audio(): with wave.open('changed.wav', 'rb') as f: return f.readframes(f.getnframes()) def transcribe_audio_bytes(stub): language = \"hi\" audio_bytes = read_audio() lang = Language(value=language, name='Hindi') config = RecognitionConfig(language=lang, audioFormat='WAV', transcriptionFormat='TRANSCRIPT') audio = RecognitionAudio(audioContent=audio_bytes) request = SpeechRecognitionRequest(audio=audio, config=config) # creds = grpc.metadata_call_credentials( # metadata_plugin=GrpcAuth('access_key') # ) response = stub.recognize(request) print(response.transcript) def transcribe_audio_url(stub): language = \"hi\" url = \"https://codmento.com/ekstep/test/changed.wav\" lang = Language(value=language, name='Hindi') config = RecognitionConfig(language=lang, audioFormat='WAV') audio = RecognitionAudio(audioUri=url) request = SpeechRecognitionRequest(audio=audio, config=config) response = stub.recognize(request) print(response.transcript) def get_srt_audio_bytes(stub): language = \"hi\" audio_bytes = read_audio() lang = Language(value=language, name='Hindi') config = RecognitionConfig(language=lang, audioFormat='WAV', transcriptionFormat='SRT') audio = RecognitionAudio(audioContent=audio_bytes) request = SpeechRecognitionRequest(audio=audio, config=config) # creds = grpc.metadata_call_credentials( # metadata_plugin=GrpcAuth('access_key') # ) response = stub.recognize(request) print(response.srt) def get_srt_audio_url(stub): language = \"hi\" url = \"https://codmento.com/ekstep/test/changed.wav\" lang = Language(value=language, name='Hindi') config = RecognitionConfig(language=lang, audioFormat='WAV', transcriptionFormat='SRT') audio = RecognitionAudio(audioUri=url) request = SpeechRecognitionRequest(audio=audio, config=config) response = stub.recognize(request) print(response.srt) if __name__ == '__main__': key = \"mysecrettoken\" interceptors = [MetadataClientInterceptor(key)] with grpc.insecure_channel('34.70.114.226:50051') as channel: channel = grpc.intercept_channel(channel, *interceptors) stub = SpeechRecognizerStub(channel) transcribe_audio_url(stub) transcribe_audio_bytes(stub) get_srt_audio_url(stub) get_srt_audio_bytes(stub) Java package com.ekstep.endpoints.speech_recognition; import com.google.protobuf.ByteString; import io.grpc.Channel; import io.grpc.ManagedChannel; import io.grpc.ManagedChannelBuilder; import io.grpc.StatusRuntimeException; import java.util.concurrent.TimeUnit; import java.util.logging.Level; import java.util.logging.Logger; public class SpeechRecognitionClient { private static final Logger logger = Logger.getLogger(SpeechRecognitionClient.class.getName()); private final SpeechRecognizerGrpc.SpeechRecognizerBlockingStub blockingStub; public SpeechRecognitionClient(Channel channel) { blockingStub = SpeechRecognizerGrpc.newBlockingStub(channel); } public SpeechRecognitionResult transcribeUrlV2() { String audioUrl = \"https://codmento.com/ekstep/test/changed.wav\"; logger.info(\"Will try to request \" + audioUrl + \" ...\"); RecognitionConfig config = RecognitionConfig.newBuilder() .setLanguage(Language.newBuilder().setValue(Language.LanguageCode.hi).build()) .setAudioFormat(RecognitionConfig.AudioFormat.WAV) .build(); RecognitionAudio audio = RecognitionAudio.newBuilder().setAudioUri(audioUrl).build(); SpeechRecognitionRequest request = SpeechRecognitionRequest.newBuilder() .setAudio(audio) .setConfig(config) .build(); SpeechRecognitionResult response; try { response = blockingStub.recognize(request); return response; } catch (StatusRuntimeException e) { logger.log(Level.WARNING, \"RPC failed: {0}\", e.getStatus()); return SpeechRecognitionResult.newBuilder().build(); } } public SpeechRecognitionResult transcribeBytesV2() { logger.info(\"Will try to request ...\"); AudioFiles audioFiles = new AudioFiles(); String file = \"/Users/nireshkumarr/Documents/ekstep/speech-recognition-open-api/examples/python/speech-recognition/changed.wav\"; byte[] data2 = audioFiles.readAudioFileData(file); ByteString byteString = ByteString.copyFrom(data2); RecognitionConfig config = RecognitionConfig.newBuilder() .setLanguage(Language.newBuilder().setValue(Language.LanguageCode.hi).build()) .setAudioFormat(RecognitionConfig.AudioFormat.WAV) .build(); RecognitionAudio audio = RecognitionAudio.newBuilder().setAudioContent(byteString).build(); SpeechRecognitionRequest request = SpeechRecognitionRequest.newBuilder() .setAudio(audio) .setConfig(config) .build(); SpeechRecognitionResult response; try { response = blockingStub.recognize(request); return response; } catch (StatusRuntimeException e) { logger.log(Level.WARNING, \"RPC failed: {0}\", e.getStatus()); return SpeechRecognitionResult.newBuilder().build(); } } public SpeechRecognitionResult srtUrlV2() { String audioUrl = \"https://codmento.com/ekstep/test/changed.wav\"; logger.info(\"Will try to request \" + audioUrl + \" ...\"); RecognitionConfig config = RecognitionConfig.newBuilder() .setLanguage(Language.newBuilder().setValue(Language.LanguageCode.hi).build()) .setAudioFormat(RecognitionConfig.AudioFormat.WAV) .setTranscriptionFormat(RecognitionConfig.TranscriptionFormat.SRT) .build(); RecognitionAudio audio = RecognitionAudio.newBuilder().setAudioUri(audioUrl).build(); SpeechRecognitionRequest request = SpeechRecognitionRequest.newBuilder() .setAudio(audio) .setConfig(config) .build(); SpeechRecognitionResult response; try { response = blockingStub.recognize(request); return response; } catch (StatusRuntimeException e) { logger.log(Level.WARNING, \"RPC failed: {0}\", e.getStatus()); return SpeechRecognitionResult.newBuilder().build(); } } public SpeechRecognitionResult srtBytesV2() { logger.info(\"Will try to request ...\"); AudioFiles audioFiles = new AudioFiles(); String file = \"/Users/nireshkumarr/Documents/ekstep/speech-recognition-open-api/examples/python/speech-recognition/changed.wav\"; byte[] data2 = audioFiles.readAudioFileData(file); ByteString byteString = ByteString.copyFrom(data2); RecognitionConfig config = RecognitionConfig.newBuilder() .setLanguage(Language.newBuilder().setValue(Language.LanguageCode.hi).build()) .setAudioFormat(RecognitionConfig.AudioFormat.WAV) .setTranscriptionFormat(RecognitionConfig.TranscriptionFormat.SRT) .build(); RecognitionAudio audio = RecognitionAudio.newBuilder().setAudioContent(byteString).build(); SpeechRecognitionRequest request = SpeechRecognitionRequest.newBuilder() .setAudio(audio) .setConfig(config) .build(); SpeechRecognitionResult response; try { response = blockingStub.recognize(request); return response; } catch (StatusRuntimeException e) { logger.log(Level.WARNING, \"RPC failed: {0}\", e.getStatus()); return SpeechRecognitionResult.newBuilder().build(); } } public static void main(String[] args) throws Exception { String target = \"34.70.114.226:50051\"; ManagedChannel channel = ManagedChannelBuilder.forTarget(target) .usePlaintext() .build(); try { SpeechRecognitionClient client = new SpeechRecognitionClient(channel); SpeechRecognitionResult srtUrlResponse = client.srtUrlV2() SpeechRecognitionResult srtBytesResponse = client.srtBytesV2(); SpeechRecognitionResult bytesResponse = client.transcribeBytesV2(); SpeechRecognitionResult urlResponse = client.transcribeUrlV2(); System.out.println(bytesResponse.getTranscript()); System.out.println(urlResponse.getTranscript()); System.out.println(srtBytesResponse.getSrt()); System.out.println(srtUrlResponse.getSrt()); } finally { channel.shutdownNow().awaitTermination(5, TimeUnit.SECONDS); } } } Setup and getting started guide \u00b6 Clone our github repo : https://github.com/Open-Speech-EkStep/speech-recognition-open-api.git Setup the grpc server: Without docker 1. Create and activate a new environment : ```conda create --name <env> python=3.8 && conda activate <env>``` Install required libraries using the following command: pip install -r requirements.txt Bootstrap the model code and other models as pre requisites: sh model_bootstrap.sh 4. Download models and update the right model paths in model_dict.json. 5. Start the server at port 50051: python server.py With docker docker build -t speech_recognition_model_api . sudo docker run --cpus=6 -m 20000m -itd -p <<host_port>>:50051 --name speech_recognition_model_api -v <<host_model_path>>/deployed_models:<<container_model_path>>/deployed_models/ -i -t speech_recognition_model_api Using the model api as part of client code: In python, python examples/python/speech-recognition/main.py Using the model api as part of REST call using api-gateway: Create api config in api gateway: gcloud api-gateway api-configs create CONFIG_ID \\ --api=API_ID --project=PROJECT_ID \\ --grpc-files=api_descriptor.pb,api_config.yaml Deploy gateway in api gateway: gcloud api-gateway gateways create GATEWAY_ID \\ --api=API_ID --api-config=CONFIG_ID \\ --location=GCP_REGION --project=PROJECT_ID View gateway information: gcloud api-gateway gateways describe GATEWAY_ID \\ --location=GCP_REGION --project=PROJECT_ID To run tests, use the following command: py.test --grpc-fake-server --ignore=wav2letter --ignore=wav2vec-infer --ignore=kenlm Contributing \u00b6 Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request License \u00b6 Distributed under the [MIT] License. See LICENSE for more information. Git repository \u00b6 https://github.com/Open-Speech-EkStep/speech-recognition-open-api.git Contact \u00b6 Connect with community on Gitter Project Link: https://github.com/Open-Speech-EkStep/speech-recognition-open-api.git","title":"Speech Recognititon Model API"},{"location":"asr_model_api/#speech-recognition-model-api","text":"","title":"Speech Recognition model API"},{"location":"asr_model_api/#about-the-project","text":"Our speech to text interface enables you to accurately convert speech into text using an API powered by deep learning neural network algorithms for automatic speech recognition (ASR). This is enabled to provide the following features: Speech to text transcription support for a growing list of indic languages. Transcribe your content in real time from stored files or audio bytes. Generate subtitle or transcript for your audios as per your choice of output. Support for various audio formats like WAV,MP3,PCM. [beta]Enables transcription optimized for domain-specific quality requirements associating domain models in backend. [beta]Speech-to-Text accurately punctuates transcriptions (e.g., commas, question marks, and periods). The Developer documentation provides you with a complete set of guidelines which you need to get started with: Architecture overview API reference Client Code reference Setup and getting started guide Extend this project Contribute to this project","title":"About The Project"},{"location":"asr_model_api/#architecture-overview","text":"The logical architecture here is built with a grpc server hosting our speech recognition models and dependencies, which can be run in any environment or docker. With gRPC we can define our service once in a .proto file and generate clients and servers in any of gRPC\u2019s supported languages, which in turn can be run in environments ranging from servers inside a large data center to your own tablet \u2014 all the complexity of communication between different languages and environments is handled for you by gRPC. We also get all the advantages of working with protocol buffers, including efficient serialization, a simple IDL, and easy interface updating. In gRPC, a client application can directly call a method on a server application on a different machine as if it were a local object, making it easier for you to create distributed applications and services. Here we can use the grpc generated stubs from client code in any language and make requests using gRPC calls and receive the responses from the server.On the server side, the server implements this interface and runs a gRPC server to handle client calls. By default, gRPC uses Protocol Buffers, Google\u2019s mature open source mechanism for serializing structured data (although it can be used with other data formats such as JSON). gRPC uses protoc with a special gRPC plugin to generate code from your proto file: you get generated gRPC client and server code, as well as the regular protocol buffer code for populating, serializing, and retrieving your message types. Apart from using gRPC stubs, we have added the support for REST calls to the gRPC server via an api-gateway. With API Gateway for gRPC, you can use the API management capabilities of API Gateway to add monitoring, hosting, tracing, authentication, and more to your gRPC services on Cloud Run. In addition, once you specify special mapping rules, API Gateway translates RESTful JSON over HTTP into gRPC requests. This means that you can deploy a gRPC server managed by API Gateway and call its API using a gRPC or JSON/HTTP client, giving you much more flexibility and ease of integration with other systems.","title":"Architecture Overview"},{"location":"asr_model_api/#api-reference","text":"Our API has predictable resource-oriented URLs, accepts form-encoded request bodies, returns JSON-encoded responses, and uses standard HTTP response codes, authentication, and verbs. Base URL https://<gateway-url>/v1/recognize/ Authentication Authentication to the API is performed via HTTP Basic Auth. Provide your API key as the basic auth username value. You do not need to provide a password. The Stripe API uses API keys to authenticate requests. If you need to authenticate via bearer auth (e.g., for a cross-origin request), use -H \"Authorization: Bearer sk_test_4eC39HqLyjWDarjtT1zdp7dc\" As of now all API requests can be made over HTTPS or HTTP both. Calls made over plain HTTP will fail going ahead. Errors Our API uses HTTP response codes to indicate the success or failure of an API request. 200 - OK Everything worked as expected. 400 - Bad Request The request was unacceptable, often due to missing a required parameter. 401 - Unauthorized No valid API key provided. 402 - Request Failed The parameters were valid but the request failed. 403 - Forbidden The API key doesn't have permissions to perform the request. 404 - Not Found The requested resource doesn't exist. 409 - Conflict The request conflicts with another request (perhaps due to using the same idempotent key). 429 - Too Many Requests Too many requests hit the API too quickly. We recommend an exponential backoff of your requests. 500, 502, 503, 504 - Server Errors Something went wrong on Stripe's end. (These are rare.) Handling errors We are in process of writing code that gracefully handles all possible API exceptions. This is something work in progress and will be available soon. Endpoints Supported The recognize object POST /v1/recognize/ Request Attributes config - Holds the configuration objects for language,transcriptionFormat and audio format. Child Attributes: language : REQUIRED - Specify the value of the language and its attributes Child Attributes: value : string : REQUIRED - Specify a langauge code for the audio transcription. Enum: ['en', 'hi', 'ta', 'te', 'kn', 'or', 'gu', 'en-IN'] transcriptionFormat : string : OPTIONAL - Determine the output format as either SRT or TRANSCRIPT.Default value is TRANSCRIPT. Enum : ['SRT','TRANSCRIPT'] audioFormat : string : OPTIONAL - Determine the input audio formats from the list supported.Default value is WAV. Enum : ['WAV', 'MP3', 'PCM'] audio - Specify the audio properties configuration. Either provide the audio URL or the audio bytes. Child Attributes: Either of the below attributes is REQUIRED. audioUri : string : REQUIRED - Specify the audio URL path audioContent : string : REQUIRED - Specify the byte representation of the audio as part of the request. Request body Example Schema { \"config\": { \"language\": { \"value\": \"hi\" }, \"transcriptionFormat\": \"SRT\", \"audioFormat\": \"WAV\" }, \"audio\": { \"audioUri\": \"https://codmento.com/ekstep/test/changed.wav\" } } Responses Code Description 200 On successful completion of the job. Response Attributes srt : string - The subtitle as output if transcription format is chosen at SRT. transcript : string - The transcript as output if transcription format is chosen at TRANSCRIPT. Response body Example Schema { \"srt\": \"1\\n00:00:01,29 --> 00:00:04,88\\n\u0939\u093f\u0902\u0926\u0940 \u092e\u0949\u0921\u0932 \u091f\u0947\u0938\u094d\u091f \u0915\u0930 \u0930\u0939\u0940 \u0939\u0942\u0902\\n\\n2\\n00:00:05,00 --> 00:00:09,89\\n\u0915\u0948\u0938\u093e \u091a\u0932 \u0930\u0939\u093e \u0939\u0948 \u092e\u0947\u0930\u0947 \u0935\u093f\u0902\u0921\u094b\u091c \u0932\u0948\u092a\u091f\u0949\u092a \u0938\u0947\\n\\n\" } { \"transcript\": \"\u0939\u093f\u0902\u0926\u0940 \u092e\u0949\u0921\u0932 \u091f\u0947\u0938\u094d\u091f \u0915\u0930 \u0930\u0939\u0940 \u0939\u0942\u0902 \u0915\u0948\u0938\u093e \u091a\u0932 \u0930\u0939\u093e \u0939\u0948 \u092e\u0947\u0930\u0947 \u0935\u093f\u0902\u0921\u094b\u091c \u0932\u0948\u092a\u091f\u0949\u092a \u0938\u0947\" } Responses Code Description 400 On input errors causing a failure in the job. Response Attributes code : string - status code as encountered in the processing life-cycle. message : string - human understandable format. Response body Example Schema { \"code\": 400, \"message\": \"config.audioFormat: invalid value \\\"MP4\\\" for type type.googleapis.com/ekstep.speech_recognition.RecognitionConfig.AudioFormat\" } Responses Code Description 500 Internal error causing a failure in the job. Response Attributes code : string - status code as encountered in the processing life-cycle. message : string - human understandable format. Response body Example Schema { \"code\": 2, \"message\": \"Exception calling application: An unknown error has occurred.Please try again.\" } Sample Request curl --location --request POST 'https://<gateway-url>/v1/recognize/' \\ --header 'Content-Type: text/plain' \\ --data-raw '{ \"config\": { \"language\": { \"value\": \"hi\" }, \"transcriptionFormat\": \"SRT\", \"audioFormat\": \"WAV\" }, \"audio\": { \"audioUri\": \"https://codmento.com/ekstep/test/changed.wav\" } }' Sample Response { \"srt\": \"1\\n00:00:01,29 --> 00:00:04,88\\n\u0939\u093f\u0902\u0926\u0940 \u092e\u0949\u0921\u0932 \u091f\u0947\u0938\u094d\u091f \u0915\u0930 \u0930\u0939\u0940 \u0939\u0942\u0902\\n\\n2\\n00:00:05,00 --> 00:00:09,89\\n\u0915\u0948\u0938\u093e \u091a\u0932 \u0930\u0939\u093e \u0939\u0948 \u092e\u0947\u0930\u0947 \u0935\u093f\u0902\u0921\u094b\u091c \u0932\u0948\u092a\u091f\u0949\u092a \u0938\u0947\\n\\n\" } Sample Request curl --location --request POST 'https://<gateway-url>/v1/recognize/' \\ --header 'Content-Type: text/plain' \\ --data-raw '{ \"config\": { \"language\": { \"value\": \"hi\" }, \"transcriptionFormat\": \"TRANSCRIPT\", \"audioFormat\": \"WAV\" }, \"audio\": { \"audioUri\": \"https://codmento.com/ekstep/test/changed.wav\" } }' Sample Response { \"transcript\": \"\u0939\u093f\u0902\u0926\u0940 \u092e\u0949\u0921\u0932 \u091f\u0947\u0938\u094d\u091f \u0915\u0930 \u0930\u0939\u0940 \u0939\u0942\u0902 \u0915\u0948\u0938\u093e \u091a\u0932 \u0930\u0939\u093e \u0939\u0948 \u092e\u0947\u0930\u0947 \u0935\u093f\u0902\u0921\u094b\u091c \u0932\u0948\u092a\u091f\u0949\u092a \u0938\u0947\" }","title":"API reference"},{"location":"asr_model_api/#client-code-reference","text":"We provide client libraries as stubs for different programming languages. python import grpc from stub.speech_recognition_open_api_pb2_grpc import SpeechRecognizerStub from stub.speech_recognition_open_api_pb2 import Language, RecognitionConfig, RecognitionAudio, \\ SpeechRecognitionRequest import wave from grpc_interceptor import ClientCallDetails, ClientInterceptor class GrpcAuth(grpc.AuthMetadataPlugin): def __init__(self, key): self._key = key def __call__(self, context, callback): callback((('rpc-auth-header', self._key),), None) class MetadataClientInterceptor(ClientInterceptor): def __init__(self, key): self._key = key def intercept( self, method, request_or_iterator, call_details: grpc.ClientCallDetails, ): new_details = ClientCallDetails( call_details.method, call_details.timeout, [(\"authorization\", \"Bearer \" + self._key)], call_details.credentials, call_details.wait_for_ready, call_details.compression, ) return method(request_or_iterator, new_details) def read_audio(): with wave.open('changed.wav', 'rb') as f: return f.readframes(f.getnframes()) def transcribe_audio_bytes(stub): language = \"hi\" audio_bytes = read_audio() lang = Language(value=language, name='Hindi') config = RecognitionConfig(language=lang, audioFormat='WAV', transcriptionFormat='TRANSCRIPT') audio = RecognitionAudio(audioContent=audio_bytes) request = SpeechRecognitionRequest(audio=audio, config=config) # creds = grpc.metadata_call_credentials( # metadata_plugin=GrpcAuth('access_key') # ) response = stub.recognize(request) print(response.transcript) def transcribe_audio_url(stub): language = \"hi\" url = \"https://codmento.com/ekstep/test/changed.wav\" lang = Language(value=language, name='Hindi') config = RecognitionConfig(language=lang, audioFormat='WAV') audio = RecognitionAudio(audioUri=url) request = SpeechRecognitionRequest(audio=audio, config=config) response = stub.recognize(request) print(response.transcript) def get_srt_audio_bytes(stub): language = \"hi\" audio_bytes = read_audio() lang = Language(value=language, name='Hindi') config = RecognitionConfig(language=lang, audioFormat='WAV', transcriptionFormat='SRT') audio = RecognitionAudio(audioContent=audio_bytes) request = SpeechRecognitionRequest(audio=audio, config=config) # creds = grpc.metadata_call_credentials( # metadata_plugin=GrpcAuth('access_key') # ) response = stub.recognize(request) print(response.srt) def get_srt_audio_url(stub): language = \"hi\" url = \"https://codmento.com/ekstep/test/changed.wav\" lang = Language(value=language, name='Hindi') config = RecognitionConfig(language=lang, audioFormat='WAV', transcriptionFormat='SRT') audio = RecognitionAudio(audioUri=url) request = SpeechRecognitionRequest(audio=audio, config=config) response = stub.recognize(request) print(response.srt) if __name__ == '__main__': key = \"mysecrettoken\" interceptors = [MetadataClientInterceptor(key)] with grpc.insecure_channel('34.70.114.226:50051') as channel: channel = grpc.intercept_channel(channel, *interceptors) stub = SpeechRecognizerStub(channel) transcribe_audio_url(stub) transcribe_audio_bytes(stub) get_srt_audio_url(stub) get_srt_audio_bytes(stub) Java package com.ekstep.endpoints.speech_recognition; import com.google.protobuf.ByteString; import io.grpc.Channel; import io.grpc.ManagedChannel; import io.grpc.ManagedChannelBuilder; import io.grpc.StatusRuntimeException; import java.util.concurrent.TimeUnit; import java.util.logging.Level; import java.util.logging.Logger; public class SpeechRecognitionClient { private static final Logger logger = Logger.getLogger(SpeechRecognitionClient.class.getName()); private final SpeechRecognizerGrpc.SpeechRecognizerBlockingStub blockingStub; public SpeechRecognitionClient(Channel channel) { blockingStub = SpeechRecognizerGrpc.newBlockingStub(channel); } public SpeechRecognitionResult transcribeUrlV2() { String audioUrl = \"https://codmento.com/ekstep/test/changed.wav\"; logger.info(\"Will try to request \" + audioUrl + \" ...\"); RecognitionConfig config = RecognitionConfig.newBuilder() .setLanguage(Language.newBuilder().setValue(Language.LanguageCode.hi).build()) .setAudioFormat(RecognitionConfig.AudioFormat.WAV) .build(); RecognitionAudio audio = RecognitionAudio.newBuilder().setAudioUri(audioUrl).build(); SpeechRecognitionRequest request = SpeechRecognitionRequest.newBuilder() .setAudio(audio) .setConfig(config) .build(); SpeechRecognitionResult response; try { response = blockingStub.recognize(request); return response; } catch (StatusRuntimeException e) { logger.log(Level.WARNING, \"RPC failed: {0}\", e.getStatus()); return SpeechRecognitionResult.newBuilder().build(); } } public SpeechRecognitionResult transcribeBytesV2() { logger.info(\"Will try to request ...\"); AudioFiles audioFiles = new AudioFiles(); String file = \"/Users/nireshkumarr/Documents/ekstep/speech-recognition-open-api/examples/python/speech-recognition/changed.wav\"; byte[] data2 = audioFiles.readAudioFileData(file); ByteString byteString = ByteString.copyFrom(data2); RecognitionConfig config = RecognitionConfig.newBuilder() .setLanguage(Language.newBuilder().setValue(Language.LanguageCode.hi).build()) .setAudioFormat(RecognitionConfig.AudioFormat.WAV) .build(); RecognitionAudio audio = RecognitionAudio.newBuilder().setAudioContent(byteString).build(); SpeechRecognitionRequest request = SpeechRecognitionRequest.newBuilder() .setAudio(audio) .setConfig(config) .build(); SpeechRecognitionResult response; try { response = blockingStub.recognize(request); return response; } catch (StatusRuntimeException e) { logger.log(Level.WARNING, \"RPC failed: {0}\", e.getStatus()); return SpeechRecognitionResult.newBuilder().build(); } } public SpeechRecognitionResult srtUrlV2() { String audioUrl = \"https://codmento.com/ekstep/test/changed.wav\"; logger.info(\"Will try to request \" + audioUrl + \" ...\"); RecognitionConfig config = RecognitionConfig.newBuilder() .setLanguage(Language.newBuilder().setValue(Language.LanguageCode.hi).build()) .setAudioFormat(RecognitionConfig.AudioFormat.WAV) .setTranscriptionFormat(RecognitionConfig.TranscriptionFormat.SRT) .build(); RecognitionAudio audio = RecognitionAudio.newBuilder().setAudioUri(audioUrl).build(); SpeechRecognitionRequest request = SpeechRecognitionRequest.newBuilder() .setAudio(audio) .setConfig(config) .build(); SpeechRecognitionResult response; try { response = blockingStub.recognize(request); return response; } catch (StatusRuntimeException e) { logger.log(Level.WARNING, \"RPC failed: {0}\", e.getStatus()); return SpeechRecognitionResult.newBuilder().build(); } } public SpeechRecognitionResult srtBytesV2() { logger.info(\"Will try to request ...\"); AudioFiles audioFiles = new AudioFiles(); String file = \"/Users/nireshkumarr/Documents/ekstep/speech-recognition-open-api/examples/python/speech-recognition/changed.wav\"; byte[] data2 = audioFiles.readAudioFileData(file); ByteString byteString = ByteString.copyFrom(data2); RecognitionConfig config = RecognitionConfig.newBuilder() .setLanguage(Language.newBuilder().setValue(Language.LanguageCode.hi).build()) .setAudioFormat(RecognitionConfig.AudioFormat.WAV) .setTranscriptionFormat(RecognitionConfig.TranscriptionFormat.SRT) .build(); RecognitionAudio audio = RecognitionAudio.newBuilder().setAudioContent(byteString).build(); SpeechRecognitionRequest request = SpeechRecognitionRequest.newBuilder() .setAudio(audio) .setConfig(config) .build(); SpeechRecognitionResult response; try { response = blockingStub.recognize(request); return response; } catch (StatusRuntimeException e) { logger.log(Level.WARNING, \"RPC failed: {0}\", e.getStatus()); return SpeechRecognitionResult.newBuilder().build(); } } public static void main(String[] args) throws Exception { String target = \"34.70.114.226:50051\"; ManagedChannel channel = ManagedChannelBuilder.forTarget(target) .usePlaintext() .build(); try { SpeechRecognitionClient client = new SpeechRecognitionClient(channel); SpeechRecognitionResult srtUrlResponse = client.srtUrlV2() SpeechRecognitionResult srtBytesResponse = client.srtBytesV2(); SpeechRecognitionResult bytesResponse = client.transcribeBytesV2(); SpeechRecognitionResult urlResponse = client.transcribeUrlV2(); System.out.println(bytesResponse.getTranscript()); System.out.println(urlResponse.getTranscript()); System.out.println(srtBytesResponse.getSrt()); System.out.println(srtUrlResponse.getSrt()); } finally { channel.shutdownNow().awaitTermination(5, TimeUnit.SECONDS); } } }","title":"Client Code reference"},{"location":"asr_model_api/#setup-and-getting-started-guide","text":"Clone our github repo : https://github.com/Open-Speech-EkStep/speech-recognition-open-api.git Setup the grpc server: Without docker 1. Create and activate a new environment : ```conda create --name <env> python=3.8 && conda activate <env>``` Install required libraries using the following command: pip install -r requirements.txt Bootstrap the model code and other models as pre requisites: sh model_bootstrap.sh 4. Download models and update the right model paths in model_dict.json. 5. Start the server at port 50051: python server.py With docker docker build -t speech_recognition_model_api . sudo docker run --cpus=6 -m 20000m -itd -p <<host_port>>:50051 --name speech_recognition_model_api -v <<host_model_path>>/deployed_models:<<container_model_path>>/deployed_models/ -i -t speech_recognition_model_api Using the model api as part of client code: In python, python examples/python/speech-recognition/main.py Using the model api as part of REST call using api-gateway: Create api config in api gateway: gcloud api-gateway api-configs create CONFIG_ID \\ --api=API_ID --project=PROJECT_ID \\ --grpc-files=api_descriptor.pb,api_config.yaml Deploy gateway in api gateway: gcloud api-gateway gateways create GATEWAY_ID \\ --api=API_ID --api-config=CONFIG_ID \\ --location=GCP_REGION --project=PROJECT_ID View gateway information: gcloud api-gateway gateways describe GATEWAY_ID \\ --location=GCP_REGION --project=PROJECT_ID To run tests, use the following command: py.test --grpc-fake-server --ignore=wav2letter --ignore=wav2vec-infer --ignore=kenlm","title":"Setup and getting started guide"},{"location":"asr_model_api/#contributing","text":"Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request","title":"Contributing"},{"location":"asr_model_api/#license","text":"Distributed under the [MIT] License. See LICENSE for more information.","title":"License"},{"location":"asr_model_api/#git-repository","text":"https://github.com/Open-Speech-EkStep/speech-recognition-open-api.git","title":"Git repository"},{"location":"asr_model_api/#contact","text":"Connect with community on Gitter Project Link: https://github.com/Open-Speech-EkStep/speech-recognition-open-api.git","title":"Contact"},{"location":"asr_streaming_service/","text":"Speech Recognition Streaming Service \u00b6 Our speech to text interface enables you to accurately convert speech into text using an API powered by deep learning neural network algorithms for automatic speech recognition (ASR). To know more, Click Here This Streaming API provides an interface to accept chunks of continuous audio stream that can be transcribed in realtime to text by using the above mentioned speech to text interface. This service is enabled to provide the following features: Speech to text transcription support for a growing list of indic languages. Transcribe your content in real time from stored files or audio bytes. The Developer documentation provides you with a complete set of guidelines which you need to get started with. Table of Contents \u00b6 Architecture overview Components Grpc Server Grpc Client Proxy Service Reason for using proxy component Socket.io server Nodejs Grpc client Browser Client SDK Quick Start Pre-requisites Streaming server setup Streaming proxy server setup Website UI setup with streaming client sdk Tips Contribute to the project License Git Repositories Contact Architecture Overview \u00b6 Components \u00b6 This service consists of two main components. GRPC client GRPC server It also contains two plugin/support components. Proxy service. Browser client sdk. GRPC Server \u00b6 In this streaming service, the grpc server provides a method that supports bi-directional streaming to allow users to stream audio bytes continuously and get stream of text as output. The audio bytes provided in the stream are pre-processed and provided to the model (Speech to text model) for inferencing. As a result, we get a continuous stream of text as output for the given audio stream. This server is capable of handling multiple grpc clients and provide continuous streams of output. This grpc server is written in python programming language. For more info on grpc servers bi-directional streaming, refer here . This grpc server is available in the following Github Link GRPC Client \u00b6 The grpc client allows users/other platforms to connect to the grpc server through a channel and call the required RPC methods.Grpc clients are available in multiple programming languages like python, nodejs, java, etc. In this streaming service, we are using nodejs grpc client to connect with the streaming service. To allow users to connect to our grpc server from browsers, we have created a proxy service which uses socket.io to maintain two way connections so that audio can be streamed from the user using browsers and send it to the grpc server from the proxy using nodejs grpc client. Proxy Service \u00b6 This service allows browser-users/web-applications to stream audios to the grpc server and provides the streaming transcriptions back to the browser-users/web-applications. It is available in the following Github Link . Reason for using proxy component \u00b6 To use grpc client in browsers, we need to use a library called grpc-web. To know more about why grpc-web is needed, refer here . But, in the grpc-web library, there was no support for bi-directional streaming currently when this project was developed. So we have adopted to create our proxy service to create a realtime processing environment. The proxy service is developed using nodejs . The components of the proxy service are: Socket.io server Nodejs Grpc client Socket.io server \u00b6 This socket.io server create events and listens to the incoming audio streams and provides the received audio streams to the grpc client. Once the socket.io server receives the stream of transcriptions from the nodejs grpc client, it will emit result events to the socket.io client connected to this server. Nodejs Grpc client \u00b6 This nodejs grpc client creates a channel with the grpc server for each incoming user and streams the audio through the RPC method. Once the RPC method returns the stream of transcriptions, it will provide it to the socket.io server. Browser Client SDK \u00b6 To connect to the proxy and stream audio, we can use a socket.io client. Since we need to do a lot of pre-processing and maintain the socket events needed, we have created a client sdk available as a npm library. This client sdk can be imported in any node related frameworks such as React, angular, etc. It will provide methods to connect and stream audio to the proxy service. To know more, refer to this Github link . To get started with this Browser client sdk, refer to the Readme in the above github link. Quick Start \u00b6 Pre-requisites \u00b6 Download and install docker . Download and install git . Download latest stable version of nodejs . Streaming server setup \u00b6 Pre-built docker images are hosted on gcr.io/ekstepspeechrecognition/speech_recognition_model_api. We do not follow the latest tag, so you have to use a specific tag. You can pull the image using the command given below: docker pull gcr.io/ekstepspeechrecognition/speech_recognition_model_api:3.2.25 Create a directory deployed_models using the command: mkdir deployed_models Inside deployed_models folder, create a folder for each language. eg: mkdir hindi Download asr fine-tuned models and language models for the languages you need from the link given here . Directory structure of deployed_models/ : . | -- hindi | | -- hindi_infer.pt | | -- dict.ltr.txt | | -- lexicon.lst | | -- lm.binary | -- english | | -- english_infer.pt | | -- dict.ltr.txt | | -- lexicon.lst | | -- lm.binary | -- model_dict.json The contents of the model_dict.json file mentioned in above step should contain the path of the model files. For example: { \"en\" : { \"path\" : \"/english/english_infer.pt\" , \"enablePunctuation\" : true , \"enableITN\" : true }, \"hi\" : { \"path\" : \"/hindi/hindi_infer.pt\" , \"enablePunctuation\" : true , \"enableITN\" : true }, } Note: enablePunctuation flag is used if the transcription from the model needs to be punctuated. enabledITN flag is used if the inverse text normalization is needed for the transcripts from the model. Streaming text from server will not have the response punctuated or ITN applied. It needs to be done separately. Refer, streaming client sdk readme for more details. Run the streaming grpc server using the following command: docker run -itd -p 50051 :50051 --env gpu = True --env languages =[ 'en' ] --gpus all -v /home/user/project/deployed_models/:/opt/speech_recognition_open_api/deployed_models/ gcr.io/ekstepspeechrecognition/speech_recognition_model_api:3.2.25 This will keep the streaming grpc server up and running in port 50051 as mentioned in the above docker command. Streaming proxy service setup \u00b6 Clone the proxy service from github: git clone https://github.com/Open-Speech-EkStep/speech-recognition-open-api-proxy.git Run cd speech-recognition-open-api-proxy . Install the project dependencies: npm i . Configure the language_map.json file (in project-root-folder eg: /users/node/speech-recognition-open-api-proxy/language_map.json), so that it points to the grpc server which is hosted in port 50051 in the above steps. For example: { \"<ip-address/host>:<port>\" : [ \"hi\" , \"en\" ], \"localhost:50051\" : [ \"ta\" , \"te\" ], } Set the folder path of language_map.json as env variable config_base_path=\"<project-root-folder>\" (eg: /users/node/speech-recognition-open-api-proxy). Run the proxy service: npm start . The proxy service will be up and running in port 9009 . Website UI setup with Streaming client sdk \u00b6 To create a streaming web ui, clone the below repository from github: git clone https://github.com/Open-Speech-EkStep/speech-recognition-open-api-client.git Run cd speech-recognition-open-api-client && cd examples/react-example . Install the project dependencies: npm i . Open the file : src/App.js . In this file, in handleStart() method in line 25 and 26, modify the url and language as you need: example const url = 'http://localhost:9009' ; // url of the proxy service const language = 'hi' ; // this can be en, gu depends on what models you have hosted. Save and Close the file once the changes are done. Run the service using npm start . This will open a browser where you can click on the start button and start speaking. For every pause you provide, you will be getting a streamed transcription output. Tips \u00b6 In step 7, multiple languages can be given in one docker container by doing the following for languages env variable. --env languages =[ 'en' , 'hi' ] Multiple docker containers can be created with different language sets and they can all be accessed by using the proxy service. For example, docker container1 is hosted with en,hi on port 50051 and docker container2 is hosted with ta,te on port 50052, then language_config.json file content will be as follows: { \"localhost:50051\" : [ \"hi\" , \"en\" ] , \"localhost:50052\" : [ \"ta\" , \"te\" ] , } Port of the proxy service can be changed PORT env variable. Contribute to the project \u00b6 Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project. Create your Feature Branch ( git checkout -b feature/AmazingFeature ). Commit your Changes ( git commit -m 'Add some AmazingFeature' ). Push to the Branch ( git push origin feature/AmazingFeature ). Open a Pull Request. License \u00b6 Distributed under the [MIT] License. See LICENSE for more information. Git repositories \u00b6 Streaming Server : https://github.com/Open-Speech-EkStep/speech-recognition-open-api.git . Proxy Service: https://github.com/Open-Speech-EkStep/speech-recognition-open-api-proxy.git . Client SDK: https://github.com/Open-Speech-EkStep/speech-recognition-open-api-client.git . Contact \u00b6 Connect with community on Gitter .","title":"Speech Recognititon Streaming API"},{"location":"asr_streaming_service/#speech-recognition-streaming-service","text":"Our speech to text interface enables you to accurately convert speech into text using an API powered by deep learning neural network algorithms for automatic speech recognition (ASR). To know more, Click Here This Streaming API provides an interface to accept chunks of continuous audio stream that can be transcribed in realtime to text by using the above mentioned speech to text interface. This service is enabled to provide the following features: Speech to text transcription support for a growing list of indic languages. Transcribe your content in real time from stored files or audio bytes. The Developer documentation provides you with a complete set of guidelines which you need to get started with.","title":"Speech Recognition Streaming Service"},{"location":"asr_streaming_service/#table-of-contents","text":"Architecture overview Components Grpc Server Grpc Client Proxy Service Reason for using proxy component Socket.io server Nodejs Grpc client Browser Client SDK Quick Start Pre-requisites Streaming server setup Streaming proxy server setup Website UI setup with streaming client sdk Tips Contribute to the project License Git Repositories Contact","title":"Table of Contents"},{"location":"asr_streaming_service/#architecture-overview","text":"","title":"Architecture Overview"},{"location":"asr_streaming_service/#components","text":"This service consists of two main components. GRPC client GRPC server It also contains two plugin/support components. Proxy service. Browser client sdk.","title":"Components"},{"location":"asr_streaming_service/#grpc-server","text":"In this streaming service, the grpc server provides a method that supports bi-directional streaming to allow users to stream audio bytes continuously and get stream of text as output. The audio bytes provided in the stream are pre-processed and provided to the model (Speech to text model) for inferencing. As a result, we get a continuous stream of text as output for the given audio stream. This server is capable of handling multiple grpc clients and provide continuous streams of output. This grpc server is written in python programming language. For more info on grpc servers bi-directional streaming, refer here . This grpc server is available in the following Github Link","title":"GRPC Server"},{"location":"asr_streaming_service/#grpc-client","text":"The grpc client allows users/other platforms to connect to the grpc server through a channel and call the required RPC methods.Grpc clients are available in multiple programming languages like python, nodejs, java, etc. In this streaming service, we are using nodejs grpc client to connect with the streaming service. To allow users to connect to our grpc server from browsers, we have created a proxy service which uses socket.io to maintain two way connections so that audio can be streamed from the user using browsers and send it to the grpc server from the proxy using nodejs grpc client.","title":"GRPC Client"},{"location":"asr_streaming_service/#proxy-service","text":"This service allows browser-users/web-applications to stream audios to the grpc server and provides the streaming transcriptions back to the browser-users/web-applications. It is available in the following Github Link .","title":"Proxy Service"},{"location":"asr_streaming_service/#reason-for-using-proxy-component","text":"To use grpc client in browsers, we need to use a library called grpc-web. To know more about why grpc-web is needed, refer here . But, in the grpc-web library, there was no support for bi-directional streaming currently when this project was developed. So we have adopted to create our proxy service to create a realtime processing environment. The proxy service is developed using nodejs . The components of the proxy service are: Socket.io server Nodejs Grpc client","title":"Reason for using proxy component"},{"location":"asr_streaming_service/#socketio-server","text":"This socket.io server create events and listens to the incoming audio streams and provides the received audio streams to the grpc client. Once the socket.io server receives the stream of transcriptions from the nodejs grpc client, it will emit result events to the socket.io client connected to this server.","title":"Socket.io server"},{"location":"asr_streaming_service/#nodejs-grpc-client","text":"This nodejs grpc client creates a channel with the grpc server for each incoming user and streams the audio through the RPC method. Once the RPC method returns the stream of transcriptions, it will provide it to the socket.io server.","title":"Nodejs Grpc client"},{"location":"asr_streaming_service/#browser-client-sdk","text":"To connect to the proxy and stream audio, we can use a socket.io client. Since we need to do a lot of pre-processing and maintain the socket events needed, we have created a client sdk available as a npm library. This client sdk can be imported in any node related frameworks such as React, angular, etc. It will provide methods to connect and stream audio to the proxy service. To know more, refer to this Github link . To get started with this Browser client sdk, refer to the Readme in the above github link.","title":"Browser Client SDK"},{"location":"asr_streaming_service/#quick-start","text":"","title":"Quick Start"},{"location":"asr_streaming_service/#pre-requisites","text":"Download and install docker . Download and install git . Download latest stable version of nodejs .","title":"Pre-requisites"},{"location":"asr_streaming_service/#streaming-server-setup","text":"Pre-built docker images are hosted on gcr.io/ekstepspeechrecognition/speech_recognition_model_api. We do not follow the latest tag, so you have to use a specific tag. You can pull the image using the command given below: docker pull gcr.io/ekstepspeechrecognition/speech_recognition_model_api:3.2.25 Create a directory deployed_models using the command: mkdir deployed_models Inside deployed_models folder, create a folder for each language. eg: mkdir hindi Download asr fine-tuned models and language models for the languages you need from the link given here . Directory structure of deployed_models/ : . | -- hindi | | -- hindi_infer.pt | | -- dict.ltr.txt | | -- lexicon.lst | | -- lm.binary | -- english | | -- english_infer.pt | | -- dict.ltr.txt | | -- lexicon.lst | | -- lm.binary | -- model_dict.json The contents of the model_dict.json file mentioned in above step should contain the path of the model files. For example: { \"en\" : { \"path\" : \"/english/english_infer.pt\" , \"enablePunctuation\" : true , \"enableITN\" : true }, \"hi\" : { \"path\" : \"/hindi/hindi_infer.pt\" , \"enablePunctuation\" : true , \"enableITN\" : true }, } Note: enablePunctuation flag is used if the transcription from the model needs to be punctuated. enabledITN flag is used if the inverse text normalization is needed for the transcripts from the model. Streaming text from server will not have the response punctuated or ITN applied. It needs to be done separately. Refer, streaming client sdk readme for more details. Run the streaming grpc server using the following command: docker run -itd -p 50051 :50051 --env gpu = True --env languages =[ 'en' ] --gpus all -v /home/user/project/deployed_models/:/opt/speech_recognition_open_api/deployed_models/ gcr.io/ekstepspeechrecognition/speech_recognition_model_api:3.2.25 This will keep the streaming grpc server up and running in port 50051 as mentioned in the above docker command.","title":"Streaming server setup"},{"location":"asr_streaming_service/#streaming-proxy-service-setup","text":"Clone the proxy service from github: git clone https://github.com/Open-Speech-EkStep/speech-recognition-open-api-proxy.git Run cd speech-recognition-open-api-proxy . Install the project dependencies: npm i . Configure the language_map.json file (in project-root-folder eg: /users/node/speech-recognition-open-api-proxy/language_map.json), so that it points to the grpc server which is hosted in port 50051 in the above steps. For example: { \"<ip-address/host>:<port>\" : [ \"hi\" , \"en\" ], \"localhost:50051\" : [ \"ta\" , \"te\" ], } Set the folder path of language_map.json as env variable config_base_path=\"<project-root-folder>\" (eg: /users/node/speech-recognition-open-api-proxy). Run the proxy service: npm start . The proxy service will be up and running in port 9009 .","title":"Streaming proxy service setup"},{"location":"asr_streaming_service/#website-ui-setup-with-streaming-client-sdk","text":"To create a streaming web ui, clone the below repository from github: git clone https://github.com/Open-Speech-EkStep/speech-recognition-open-api-client.git Run cd speech-recognition-open-api-client && cd examples/react-example . Install the project dependencies: npm i . Open the file : src/App.js . In this file, in handleStart() method in line 25 and 26, modify the url and language as you need: example const url = 'http://localhost:9009' ; // url of the proxy service const language = 'hi' ; // this can be en, gu depends on what models you have hosted. Save and Close the file once the changes are done. Run the service using npm start . This will open a browser where you can click on the start button and start speaking. For every pause you provide, you will be getting a streamed transcription output.","title":"Website UI setup with Streaming client sdk"},{"location":"asr_streaming_service/#tips","text":"In step 7, multiple languages can be given in one docker container by doing the following for languages env variable. --env languages =[ 'en' , 'hi' ] Multiple docker containers can be created with different language sets and they can all be accessed by using the proxy service. For example, docker container1 is hosted with en,hi on port 50051 and docker container2 is hosted with ta,te on port 50052, then language_config.json file content will be as follows: { \"localhost:50051\" : [ \"hi\" , \"en\" ] , \"localhost:50052\" : [ \"ta\" , \"te\" ] , } Port of the proxy service can be changed PORT env variable.","title":"Tips"},{"location":"asr_streaming_service/#contribute-to-the-project","text":"Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project. Create your Feature Branch ( git checkout -b feature/AmazingFeature ). Commit your Changes ( git commit -m 'Add some AmazingFeature' ). Push to the Branch ( git push origin feature/AmazingFeature ). Open a Pull Request.","title":"Contribute to the project"},{"location":"asr_streaming_service/#license","text":"Distributed under the [MIT] License. See LICENSE for more information.","title":"License"},{"location":"asr_streaming_service/#git-repositories","text":"Streaming Server : https://github.com/Open-Speech-EkStep/speech-recognition-open-api.git . Proxy Service: https://github.com/Open-Speech-EkStep/speech-recognition-open-api-proxy.git . Client SDK: https://github.com/Open-Speech-EkStep/speech-recognition-open-api-client.git .","title":"Git repositories"},{"location":"asr_streaming_service/#contact","text":"Connect with community on Gitter .","title":"Contact"},{"location":"crowdsource_platform/","text":"Crowdsourcing Platform \u00b6 Table of Contents \u00b6 Crowdsourcing Platform Table of Contents About The Project Built With Architecture Logical Architecture Cloud Agnostic Architecture: Kubernetes Deployment Architecture: AWS Architecture Improvements Languages and Tools Dashboard Design UI Details Auto Validation CI/CD Infrastructure as Code Getting Started Prerequisites Installation Usage Common configuration steps: Setting credentials for Google cloud bucket Setting credentials for AWS cloud bucket Bucket configuration Environment file configurations Running services Database migrations Testing Unit Tests Functional Test Scalabiity Test Load Test Security Running cost estimates Architecture Decision Records Contributing License Git repository Contact About The Project \u00b6 This web app is a crowdsourcing platform that aims to create open datasets to develop Speech Recognition, Text-to-Speech, Machine Translation and Optical Character Recognition for Indian languages. This will empower our technologists, language enthusiasts and language communities to build world class digital applications in our own local languages. It can be used to crowdsource audio and validate them for various languages. It comprises four initiatives: Speech Contribution: It creates a repository of diverse voices speaking Indian languages, where volunteers record their voice by reading a text. They can also choose to validate the audio and corresponding text of other volunteers. Speech Validation: It creates an open dataset through transcription of audio files. Volunteers can validate the transcriptions for audio files in different languages. Translation Validation: It creates open parallel translation datasets between corresponding sentences in two languages. Volunteers validate these translations. Image Validation: It creates an open data repository of images and the corresponding text. Similar to other initiatives, volunteers can validate the labels against the image. The application makes use of NodeJs, Postgres for Database. It can be hosted on any cloud platform. The current application has code to support AWS and GCP as providers to store the recorded information. Crowdsourcing Platform\u2019s developer documentation is meant for its adopters, developers and contributors. The developer documentation helps you to get familiar with the bare necessities, giving you a quick and clean approach to get you up and running. If you are looking for ways to customize the workflow, or just breaking things down to build them back up, head to the reference section to dig into the mechanics of Crowdsourcing Platform. Data Collection Pipeline is based on an open platform, you are free to use any programming language to extend or customize it but we prefer to use python to perform smart scraping. The Developer documentation provides you with a complete set of guidelines which you need to: Install dependencies for the Crowdsourcing Platform Configure Crowdsourcing Platform Customize Crowdsourcing Platform Extend Crowdsourcing Platform Contribute to Crowdsourcing Platform Built With \u00b6 We have used Node.js to build this platform. Node Architecture \u00b6 Logical Architecture \u00b6 The logical architecture is 3 layered with UI layer implemented using EJS templates, Service layer is implemented in Javascript which run on Nodejs server and storage layer which has RDBS and Object Storage Cloud Agnostic Architecture: Kubernetes \u00b6 This the architecture for deployment on Kubernetes, wherein the service layer is scalable leveraging K8s capabilities. Deployment Architecture: AWS \u00b6 This is the deployement architecure for running portal on AWS infra. It leverages AWS manages services like EKS and Fargate for K8s cluster and AWS RDS for managed database. It also uses AWS managed Load Balancer Architecture Improvements \u00b6 We believe in continously improving the architecture. Here are some ADR opened : Architecture Decision Records Proposed Architecture: Languages and Tools \u00b6 Dashboard Design \u00b6 The transactional tables and view tables are kept separate. Materialized views are used which holds the data as well. This avoids on the fly computations for aggregation for each query. The materizaled view are refreshed every 4 hours As a part of the refresh job, the aggregated data is dumped as json that is be served directly via CDN. Advantages: Faster reads: Separate view with only 365 aggregated data points per year. Less overhead on DB as data queried is on a very small data set and served from S3 buckets Transactional tables are optimized for faster writes as we have separate views for reads Simplified read queries as complexity is abstracted in views AWS RDS managed DB. Can be scaled horizontally and vertically easily if required in future. UI Details \u00b6 Tech Stack : React, NextJs, HTML, CSS Libraries : Libraries Name Type License Chart Amcharts 4 Free version linkware license Keyboard react-simple-keyboard Open Source MIT Fonts Lato, Helvetica, sans serif Google fonts (Open Source) OFL Bootstrap Bootstrap Open Source MIT React Class Classnames Open Source MIT Blob get-blob-duratio Open Source MIT PDF jspdf Open Source MIT Date & Time moment Open Source MIT platform platform Open Source MIT Localisation next-i18next Open Source MIT Image sharp Open Source MIT react-slick react-slick Open Source MIT swr swr Open Source MIT js-levenshtein js-levenshtein Open Source MIT word-error-rate word-error-rate Open Source MIT Frontend for Crowdsourcing Platform. \u00b6 Features \u00b6 Supports these browsers and features. Development \u00b6 Make sure your following requirements for npm and node are met: Package Version npm 6.14.14 node 14.17.5 If you are using nvm , you can run nvm use in the root directory to install the correct version of node. Open your favorite Terminal and run these commands: npm install npm run dev # Local dev server will automatically starts on http://localhost:8080 Available Scripts \u00b6 In the project directory, you can run: npm run clean \u00b6 Clean up cached or build folders. npm run dev \u00b6 Runs the app in the development mode. Open http://localhost:8080 to view it in the browser. The page will reload if you make edits. npm run dev:axe \u00b6 Similar to npm run dev but also runs @axe-core/react . npm run lint \u00b6 For running eslint on source code. npm run lint:fix \u00b6 For fixing eslint errors. npm run stylelint \u00b6 For running stylelint on source code. npm run stylelint:fix \u00b6 For fixing stylelint errors. npm run format \u00b6 For running prettier on the source code. npm run typecheck \u00b6 For running typescript typecheck. npm run test \u00b6 Launches the test runner in the interactive watch mode. npm run test:coverage \u00b6 Launches the test runner with coverage. npm run test:lh-ci \u00b6 For running lighthouse-ci . Please ensure you ran npm run build first before running this command. npm run node-talisman \u00b6 For running talisman on the source code. npm run check \u00b6 For running lint, stylelint, typecheck, test with coverage and talisman. npm run build \u00b6 Builds the app for production to the .next folder. npm run build:docker \u00b6 Same as npm run build but for docker. npm start \u00b6 It will start the production server on http://localhost:8080 . Please ensure you ran npm run build first before running this command. npm run start:docker \u00b6 It will start the production server on http://localhost:3000 for docker. Please ensure you ran npm run build:docker first before running this command. Auto Validation \u00b6 Auto Validation feature validates and warns the users in case their inputs are detected to be different from what actual data should be. It is done by comparing the user input to a machine generated output and validated against a set threshold limit for every initiative. When Auto Validation is enabled for the application, user inputs during the validation are screened depending upon the threshold set for each language in their respective initiatives. For each initiatives, scores are calculated with user input and the machine generated output. If the scores do not pass the required threshold, the system displays a message on the application screen asking the user to double check their input. In case the users go ahead and submit their input despite the message, the response input is flagged and not validated further. Types of scores calculated for initiatives: ASR - WER (Word Error Rate) OCR - Levenstein method Parallel - BleuScore method The Text Initiative currently does not support auto validation feature. CI/CD \u00b6 CircleCI is used for CI/CD. Unit tests are run continously for each commit Functional Tests are run continously for each commit and act as one if the quality gates before Production deployment Automated deployment to K8s for multiple environments Database schema changes are done continously and automatically Trunk based developement is followed Infrastructure as Code \u00b6 Infrastructure defined in code with Terraform and shell scripts Easily migrate to another AWS account Spin up new env easily Getting Started \u00b6 To get started install the prerequisites and clone the repo to machine on which you wish to run the application. Prerequisites \u00b6 Install node library using commands mentioned below. For any linux based operating system (preferred Ubuntu): sudo apt-get install nodejs For Mac-os: brew install node Windows user can follow installation steps on https://nodejs.org/en/#home-downloadhead Install or connect to a postgres database Get credentials from google developer console for google cloud storage access/ or aws cli for amazon s3 storage access. Installation \u00b6 Clone the repo using git clone https://github.com/Open-Speech-EkStep/crowdsource-dataplatform.git Go inside the directory cd crowdsource-dataplatform Install node requirements npm install Usage \u00b6 Common configuration steps: \u00b6 Setting credentials for Google cloud bucket \u00b6 You can set credentials for Google cloud bucket by running the following command gcloud auth application-default login Setting credentials for AWS cloud bucket \u00b6 You can set credentials for AWS cloud bucket by running the following command aws configure Bucket configuration \u00b6 You can create a specific bucket to store the recorded samples on aws or gcp. And mention those in the environment variables. Environment file configurations \u00b6 The following are the variables required to run the application, for running on local these can be added to a .env file DB_HOST: The host url where your postgres instance is running DB_USER: The username to access the db DB_NAME: The database name DEV_DB_NAME: The database name specific to dev environment DB_PASS: The database password BUCKET_NAME: The bucket name configured on aws or gcp ENCRYPTION_KEY: Key to run unit tests PORT: Port to run the application on Running services \u00b6 Make sure the google credentials are present in project root folder in credentials.json file. You can run the project using the command npm run To run application using a Google cloud bucket npm run gcp To run application using a AWS cloud bucket npm run aws Database migrations \u00b6 This package is used to do migrations. To create the current database structure in your postgres instance, run the following command: db-migrate up It would read the configurations from the path migations/config/migration_config.json Once can also run the migrate up command by setting an environment variable DATABASE_URL=postgresql://${DB_USER}:${DB_PASS}@${DB_HOST}/${DB_NAME} To add a new migration db-migrate create add-new-table Using the above command with the --sqlFile flag would create corresponding .sql files in which one can write sql commands to do the operation. To rollback the last migration, one can db-migrate down Documentation for the package can be found here Testing \u00b6 Multiple types of tests are continously performed to make sure the application is in healthy state. Pyramid approach is followed with Unit tests at the base and Exploratory tests on top. Unit Tests \u00b6 Unit tests can be run using below command npm test Functional Test \u00b6 Functional tests can be run using below command npm run functional_test -- --env ( test | dev ) Scalabiity Test \u00b6 Scalabiity tests performed to verify that the system is elastically scalable Below tests were performed Test Objective: Scalability Test - Validate elastic scalability Resource Configuration: Environment: Dev Pod resources: 0.25 CPU/ 250M RAM Horizontal Pod Autoscaler : Scaling Threshold - 10% CPU Utilization Min pods: 1 Max Pods: 10 Test configuration: Number of concurrent users: 1000 Total Requests : 15000 Expected: Pods should scale if load increases and CPU utilization goes beyond 10% and should scale down after 5 mins Actual : Pods were scaled up after the CPU utilization went past 10%. Time to scale to desired state was around 2-3 mins Outcome: PASSED As surge started, pods started spinning up Load Test \u00b6 Load testing is performed to verify the system is able to handle 5K concurrent users without much impact on latency Test Objective: Load Test - Validate if application can handle 5K concurrent users Date: 04/03/2021 Resource Configuration: Environment: Test Initial Pods: 3 Pod resources: 2 CPU/ 2GB RAM Horizontal Pod Autoscaler : Scaling Threshold - 40% CPU Utilization Min pods: 3 , Max Pods: 10 Database CPU : 4 Test configuration: Number of concurrent users: 20000 Requests per user : 3 Ramp up time: 10 sec Iterations: 3 Outcome: PASSED ELB stats: Database stats: Jmeter stats: Summary: - This test had 20000 users ramped up within 1 min (3 times). - The test was performed from a single machine so 20K concurrent users could scale in 1 min. - All the requests were served within initial resources, no scaling was triggered. - All three endpoints served response in around 2 sec on an average. - The system was able to handle upto 12K concurrent users. - There were some errors thrown by AWS Load balancer may be due to single IP requests. - Database could handle the load and no connection leak is observed Security \u00b6 Security first approach is taken while building this application. The OWASP top 10 are ingrained in the application security DNA. Please reach out to srajat@thoughtworks or heerabal@thoughtworks.com for more information around Security Running cost estimates \u00b6 Cloud : AWS Amazon RDS (4 CPU): $400 WAF: $30 EKS + Fargate: $75 + $225 = $300 ELB: $150 Others: $200 Total: ~ $1100-1200 per month Architecture Decision Records \u00b6 Decision records are maintained HERE Cache above RDBMS Contributing \u00b6 Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request License \u00b6 Distributed under the [MIT] License. See LICENSE for more information. Git repository \u00b6 https://github.com/Open-Speech-EkStep/crowdsource-dataplatform/ Contact \u00b6 Connect with community on Gitter Project Link: https://github.com/Open-Speech-EkStep/crowdsource-dataplatform/","title":"Crowdsourcing Platform"},{"location":"crowdsource_platform/#crowdsourcing-platform","text":"","title":"Crowdsourcing Platform"},{"location":"crowdsource_platform/#table-of-contents","text":"Crowdsourcing Platform Table of Contents About The Project Built With Architecture Logical Architecture Cloud Agnostic Architecture: Kubernetes Deployment Architecture: AWS Architecture Improvements Languages and Tools Dashboard Design UI Details Auto Validation CI/CD Infrastructure as Code Getting Started Prerequisites Installation Usage Common configuration steps: Setting credentials for Google cloud bucket Setting credentials for AWS cloud bucket Bucket configuration Environment file configurations Running services Database migrations Testing Unit Tests Functional Test Scalabiity Test Load Test Security Running cost estimates Architecture Decision Records Contributing License Git repository Contact","title":"Table of Contents"},{"location":"crowdsource_platform/#about-the-project","text":"This web app is a crowdsourcing platform that aims to create open datasets to develop Speech Recognition, Text-to-Speech, Machine Translation and Optical Character Recognition for Indian languages. This will empower our technologists, language enthusiasts and language communities to build world class digital applications in our own local languages. It can be used to crowdsource audio and validate them for various languages. It comprises four initiatives: Speech Contribution: It creates a repository of diverse voices speaking Indian languages, where volunteers record their voice by reading a text. They can also choose to validate the audio and corresponding text of other volunteers. Speech Validation: It creates an open dataset through transcription of audio files. Volunteers can validate the transcriptions for audio files in different languages. Translation Validation: It creates open parallel translation datasets between corresponding sentences in two languages. Volunteers validate these translations. Image Validation: It creates an open data repository of images and the corresponding text. Similar to other initiatives, volunteers can validate the labels against the image. The application makes use of NodeJs, Postgres for Database. It can be hosted on any cloud platform. The current application has code to support AWS and GCP as providers to store the recorded information. Crowdsourcing Platform\u2019s developer documentation is meant for its adopters, developers and contributors. The developer documentation helps you to get familiar with the bare necessities, giving you a quick and clean approach to get you up and running. If you are looking for ways to customize the workflow, or just breaking things down to build them back up, head to the reference section to dig into the mechanics of Crowdsourcing Platform. Data Collection Pipeline is based on an open platform, you are free to use any programming language to extend or customize it but we prefer to use python to perform smart scraping. The Developer documentation provides you with a complete set of guidelines which you need to: Install dependencies for the Crowdsourcing Platform Configure Crowdsourcing Platform Customize Crowdsourcing Platform Extend Crowdsourcing Platform Contribute to Crowdsourcing Platform","title":"About The Project"},{"location":"crowdsource_platform/#built-with","text":"We have used Node.js to build this platform. Node","title":"Built With"},{"location":"crowdsource_platform/#architecture","text":"","title":"Architecture"},{"location":"crowdsource_platform/#logical-architecture","text":"The logical architecture is 3 layered with UI layer implemented using EJS templates, Service layer is implemented in Javascript which run on Nodejs server and storage layer which has RDBS and Object Storage","title":"Logical Architecture"},{"location":"crowdsource_platform/#cloud-agnostic-architecture-kubernetes","text":"This the architecture for deployment on Kubernetes, wherein the service layer is scalable leveraging K8s capabilities.","title":"Cloud Agnostic Architecture: Kubernetes"},{"location":"crowdsource_platform/#deployment-architecture-aws","text":"This is the deployement architecure for running portal on AWS infra. It leverages AWS manages services like EKS and Fargate for K8s cluster and AWS RDS for managed database. It also uses AWS managed Load Balancer","title":"Deployment Architecture: AWS"},{"location":"crowdsource_platform/#architecture-improvements","text":"We believe in continously improving the architecture. Here are some ADR opened : Architecture Decision Records Proposed Architecture:","title":"Architecture Improvements"},{"location":"crowdsource_platform/#languages-and-tools","text":"","title":"Languages and Tools"},{"location":"crowdsource_platform/#dashboard-design","text":"The transactional tables and view tables are kept separate. Materialized views are used which holds the data as well. This avoids on the fly computations for aggregation for each query. The materizaled view are refreshed every 4 hours As a part of the refresh job, the aggregated data is dumped as json that is be served directly via CDN. Advantages: Faster reads: Separate view with only 365 aggregated data points per year. Less overhead on DB as data queried is on a very small data set and served from S3 buckets Transactional tables are optimized for faster writes as we have separate views for reads Simplified read queries as complexity is abstracted in views AWS RDS managed DB. Can be scaled horizontally and vertically easily if required in future.","title":"Dashboard Design"},{"location":"crowdsource_platform/#ui-details","text":"Tech Stack : React, NextJs, HTML, CSS Libraries : Libraries Name Type License Chart Amcharts 4 Free version linkware license Keyboard react-simple-keyboard Open Source MIT Fonts Lato, Helvetica, sans serif Google fonts (Open Source) OFL Bootstrap Bootstrap Open Source MIT React Class Classnames Open Source MIT Blob get-blob-duratio Open Source MIT PDF jspdf Open Source MIT Date & Time moment Open Source MIT platform platform Open Source MIT Localisation next-i18next Open Source MIT Image sharp Open Source MIT react-slick react-slick Open Source MIT swr swr Open Source MIT js-levenshtein js-levenshtein Open Source MIT word-error-rate word-error-rate Open Source MIT","title":"UI Details"},{"location":"crowdsource_platform/#frontend-for-crowdsourcing-platform","text":"","title":"Frontend for Crowdsourcing Platform."},{"location":"crowdsource_platform/#features","text":"Supports these browsers and features.","title":"Features"},{"location":"crowdsource_platform/#development","text":"Make sure your following requirements for npm and node are met: Package Version npm 6.14.14 node 14.17.5 If you are using nvm , you can run nvm use in the root directory to install the correct version of node. Open your favorite Terminal and run these commands: npm install npm run dev # Local dev server will automatically starts on http://localhost:8080","title":"Development"},{"location":"crowdsource_platform/#available-scripts","text":"In the project directory, you can run:","title":"Available Scripts"},{"location":"crowdsource_platform/#npm-run-clean","text":"Clean up cached or build folders.","title":"npm run clean"},{"location":"crowdsource_platform/#npm-run-dev","text":"Runs the app in the development mode. Open http://localhost:8080 to view it in the browser. The page will reload if you make edits.","title":"npm run dev"},{"location":"crowdsource_platform/#npm-run-devaxe","text":"Similar to npm run dev but also runs @axe-core/react .","title":"npm run dev:axe"},{"location":"crowdsource_platform/#npm-run-lint","text":"For running eslint on source code.","title":"npm run lint"},{"location":"crowdsource_platform/#npm-run-lintfix","text":"For fixing eslint errors.","title":"npm run lint:fix"},{"location":"crowdsource_platform/#npm-run-stylelint","text":"For running stylelint on source code.","title":"npm run stylelint"},{"location":"crowdsource_platform/#npm-run-stylelintfix","text":"For fixing stylelint errors.","title":"npm run stylelint:fix"},{"location":"crowdsource_platform/#npm-run-format","text":"For running prettier on the source code.","title":"npm run format"},{"location":"crowdsource_platform/#npm-run-typecheck","text":"For running typescript typecheck.","title":"npm run typecheck"},{"location":"crowdsource_platform/#npm-run-test","text":"Launches the test runner in the interactive watch mode.","title":"npm run test"},{"location":"crowdsource_platform/#npm-run-testcoverage","text":"Launches the test runner with coverage.","title":"npm run test:coverage"},{"location":"crowdsource_platform/#npm-run-testlh-ci","text":"For running lighthouse-ci . Please ensure you ran npm run build first before running this command.","title":"npm run test:lh-ci"},{"location":"crowdsource_platform/#npm-run-node-talisman","text":"For running talisman on the source code.","title":"npm run node-talisman"},{"location":"crowdsource_platform/#npm-run-check","text":"For running lint, stylelint, typecheck, test with coverage and talisman.","title":"npm run check"},{"location":"crowdsource_platform/#npm-run-build","text":"Builds the app for production to the .next folder.","title":"npm run build"},{"location":"crowdsource_platform/#npm-run-builddocker","text":"Same as npm run build but for docker.","title":"npm run build:docker"},{"location":"crowdsource_platform/#npm-start","text":"It will start the production server on http://localhost:8080 . Please ensure you ran npm run build first before running this command.","title":"npm start"},{"location":"crowdsource_platform/#npm-run-startdocker","text":"It will start the production server on http://localhost:3000 for docker. Please ensure you ran npm run build:docker first before running this command.","title":"npm run start:docker"},{"location":"crowdsource_platform/#auto-validation","text":"Auto Validation feature validates and warns the users in case their inputs are detected to be different from what actual data should be. It is done by comparing the user input to a machine generated output and validated against a set threshold limit for every initiative. When Auto Validation is enabled for the application, user inputs during the validation are screened depending upon the threshold set for each language in their respective initiatives. For each initiatives, scores are calculated with user input and the machine generated output. If the scores do not pass the required threshold, the system displays a message on the application screen asking the user to double check their input. In case the users go ahead and submit their input despite the message, the response input is flagged and not validated further. Types of scores calculated for initiatives: ASR - WER (Word Error Rate) OCR - Levenstein method Parallel - BleuScore method The Text Initiative currently does not support auto validation feature.","title":"Auto Validation"},{"location":"crowdsource_platform/#cicd","text":"CircleCI is used for CI/CD. Unit tests are run continously for each commit Functional Tests are run continously for each commit and act as one if the quality gates before Production deployment Automated deployment to K8s for multiple environments Database schema changes are done continously and automatically Trunk based developement is followed","title":"CI/CD"},{"location":"crowdsource_platform/#infrastructure-as-code","text":"Infrastructure defined in code with Terraform and shell scripts Easily migrate to another AWS account Spin up new env easily","title":"Infrastructure as Code"},{"location":"crowdsource_platform/#getting-started","text":"To get started install the prerequisites and clone the repo to machine on which you wish to run the application.","title":"Getting Started"},{"location":"crowdsource_platform/#prerequisites","text":"Install node library using commands mentioned below. For any linux based operating system (preferred Ubuntu): sudo apt-get install nodejs For Mac-os: brew install node Windows user can follow installation steps on https://nodejs.org/en/#home-downloadhead Install or connect to a postgres database Get credentials from google developer console for google cloud storage access/ or aws cli for amazon s3 storage access.","title":"Prerequisites"},{"location":"crowdsource_platform/#installation","text":"Clone the repo using git clone https://github.com/Open-Speech-EkStep/crowdsource-dataplatform.git Go inside the directory cd crowdsource-dataplatform Install node requirements npm install","title":"Installation"},{"location":"crowdsource_platform/#usage","text":"","title":"Usage"},{"location":"crowdsource_platform/#common-configuration-steps","text":"","title":"Common configuration steps:"},{"location":"crowdsource_platform/#setting-credentials-for-google-cloud-bucket","text":"You can set credentials for Google cloud bucket by running the following command gcloud auth application-default login","title":"Setting credentials for Google cloud bucket"},{"location":"crowdsource_platform/#setting-credentials-for-aws-cloud-bucket","text":"You can set credentials for AWS cloud bucket by running the following command aws configure","title":"Setting credentials for AWS cloud bucket"},{"location":"crowdsource_platform/#bucket-configuration","text":"You can create a specific bucket to store the recorded samples on aws or gcp. And mention those in the environment variables.","title":"Bucket configuration"},{"location":"crowdsource_platform/#environment-file-configurations","text":"The following are the variables required to run the application, for running on local these can be added to a .env file DB_HOST: The host url where your postgres instance is running DB_USER: The username to access the db DB_NAME: The database name DEV_DB_NAME: The database name specific to dev environment DB_PASS: The database password BUCKET_NAME: The bucket name configured on aws or gcp ENCRYPTION_KEY: Key to run unit tests PORT: Port to run the application on","title":"Environment file configurations"},{"location":"crowdsource_platform/#running-services","text":"Make sure the google credentials are present in project root folder in credentials.json file. You can run the project using the command npm run To run application using a Google cloud bucket npm run gcp To run application using a AWS cloud bucket npm run aws","title":"Running services"},{"location":"crowdsource_platform/#database-migrations","text":"This package is used to do migrations. To create the current database structure in your postgres instance, run the following command: db-migrate up It would read the configurations from the path migations/config/migration_config.json Once can also run the migrate up command by setting an environment variable DATABASE_URL=postgresql://${DB_USER}:${DB_PASS}@${DB_HOST}/${DB_NAME} To add a new migration db-migrate create add-new-table Using the above command with the --sqlFile flag would create corresponding .sql files in which one can write sql commands to do the operation. To rollback the last migration, one can db-migrate down Documentation for the package can be found here","title":"Database migrations"},{"location":"crowdsource_platform/#testing","text":"Multiple types of tests are continously performed to make sure the application is in healthy state. Pyramid approach is followed with Unit tests at the base and Exploratory tests on top.","title":"Testing"},{"location":"crowdsource_platform/#unit-tests","text":"Unit tests can be run using below command npm test","title":"Unit Tests"},{"location":"crowdsource_platform/#functional-test","text":"Functional tests can be run using below command npm run functional_test -- --env ( test | dev )","title":"Functional Test"},{"location":"crowdsource_platform/#scalabiity-test","text":"Scalabiity tests performed to verify that the system is elastically scalable Below tests were performed Test Objective: Scalability Test - Validate elastic scalability Resource Configuration: Environment: Dev Pod resources: 0.25 CPU/ 250M RAM Horizontal Pod Autoscaler : Scaling Threshold - 10% CPU Utilization Min pods: 1 Max Pods: 10 Test configuration: Number of concurrent users: 1000 Total Requests : 15000 Expected: Pods should scale if load increases and CPU utilization goes beyond 10% and should scale down after 5 mins Actual : Pods were scaled up after the CPU utilization went past 10%. Time to scale to desired state was around 2-3 mins Outcome: PASSED As surge started, pods started spinning up","title":"Scalabiity Test"},{"location":"crowdsource_platform/#load-test","text":"Load testing is performed to verify the system is able to handle 5K concurrent users without much impact on latency Test Objective: Load Test - Validate if application can handle 5K concurrent users Date: 04/03/2021 Resource Configuration: Environment: Test Initial Pods: 3 Pod resources: 2 CPU/ 2GB RAM Horizontal Pod Autoscaler : Scaling Threshold - 40% CPU Utilization Min pods: 3 , Max Pods: 10 Database CPU : 4 Test configuration: Number of concurrent users: 20000 Requests per user : 3 Ramp up time: 10 sec Iterations: 3 Outcome: PASSED ELB stats: Database stats: Jmeter stats: Summary: - This test had 20000 users ramped up within 1 min (3 times). - The test was performed from a single machine so 20K concurrent users could scale in 1 min. - All the requests were served within initial resources, no scaling was triggered. - All three endpoints served response in around 2 sec on an average. - The system was able to handle upto 12K concurrent users. - There were some errors thrown by AWS Load balancer may be due to single IP requests. - Database could handle the load and no connection leak is observed","title":"Load Test"},{"location":"crowdsource_platform/#security","text":"Security first approach is taken while building this application. The OWASP top 10 are ingrained in the application security DNA. Please reach out to srajat@thoughtworks or heerabal@thoughtworks.com for more information around Security","title":"Security"},{"location":"crowdsource_platform/#running-cost-estimates","text":"Cloud : AWS Amazon RDS (4 CPU): $400 WAF: $30 EKS + Fargate: $75 + $225 = $300 ELB: $150 Others: $200 Total: ~ $1100-1200 per month","title":"Running cost estimates"},{"location":"crowdsource_platform/#architecture-decision-records","text":"Decision records are maintained HERE Cache above RDBMS","title":"Architecture Decision Records"},{"location":"crowdsource_platform/#contributing","text":"Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request","title":"Contributing"},{"location":"crowdsource_platform/#license","text":"Distributed under the [MIT] License. See LICENSE for more information.","title":"License"},{"location":"crowdsource_platform/#git-repository","text":"https://github.com/Open-Speech-EkStep/crowdsource-dataplatform/","title":"Git repository"},{"location":"crowdsource_platform/#contact","text":"Connect with community on Gitter Project Link: https://github.com/Open-Speech-EkStep/crowdsource-dataplatform/","title":"Contact"},{"location":"data_collection/","text":"Data Collection Pipeline \u00b6 Table of Contents \u00b6 Data Collection Pipeline Table of Contents About The Project Built With Summary Getting Started Prerequisites Installation Usage Common configuration steps: Setting credentials for Google cloud bucket Bucket configuration Metadata file configurations Youtube download configurations Youtube API configuration Web Crawl Configuration Adding new spider Running services Youtube spider in channel mode: Youtube spider in file mode: Bing Spider Urls Spider Selenium google crawler Selenium youtube crawler for file mode and api mode Contributing License Git Repository Contact Acknowledgements About The Project \u00b6 This is downloading framework that is extensible and allows the user to add new source without much code changes. For each new source user need to write a scrapy spider script and rest of downloading and meta file creation is handled by repective pipelines. And if required user can add their custom pipelines. This framework automatically transfer the downloaded data to a Google cloud bucket automatically. For more info on writing scrapy spider and pipeline one can refer to the documentation . Data Collection Pipeline\u2019s developer documentation is meant for its adopters, developers and contributors. The developer documentation helps you to get familiar with the bare necessities, giving you a quick and clean approach to get you up and running. If you are looking for ways to customize the workflow, or just breaking things down to build them back up, head to the reference section to dig into the mechanics of Data Collection Pipeline. Data Collection Pipeline is based on an open platform, you are free to use any programming language to extend or customize it but we prefer to use python to perform smart scraping. The Developer documentation provides you with a complete set of guidelines which you need to: Install Data Collection Pipeline Configure Data Collection Pipeline Customize Data Collection Pipeline Extend Data Collection Pipeline Contribute to Data Collection Pipeline Built With \u00b6 We have used scrapy as the base of this framework. * Scrapy Summary \u00b6 This summary mentions the key advantages and limitations of this smart crawler service. Youtube Crawler Key Points and Advantages: Get language relevant channels from YouTube and download videos from them.(70%-80% relevancy with language - based on Manual Analysis) Can fetch channels with Creative Commons video and download the videos in them as well.(70% relevancy with language) Can download using file mode(manually filled with video Ids) or channel mode. Youtube-dl can fetch N number of videos from a channel and download them. YouTube crawler downloads files at a rate of maximum of 2000 hours per day and minimum of 800 hours per day. Youtube crawler is more convenient and it\u2019s a main source of Creative Commons data that can be accessed easily. It can be deployed in cloud service called zyte used for scraping/crawling. License information of videos are available in metadata. Limitations: Youtube-api cannot return more than 500 videos per channel.(when using YOUTUBE_API mode in configuration) Youtube-api is restricted to 10000 tokens per day in free mode. 10000 tokens can be used to get license info of 10000 videos.(in any mode) 10000 tokens can be used to get 5000 channels.(in YOUTUBE_API mode) Youtube-dl can be used to get all videos freely.(in YOUTUBE_DL mode) Cannot fetch data from specific playlist. (Solution: Fetch videos Ids of a playlist using YouTube-dl and put them in a file and download in file mode.) Rare cases in which you might get Too many requests error from Youtube-DL. (Solution: Rerun the application with same sources.) Cannot download videos which require user information and private videos. Web Crawler Key Points and Advantages: Web crawler can download specific language audio but with around 50 - 60% relevance. Web crawler downloads files at a rate of at least 2000 hours per day. It is a faster means of downloading data. Creative Commons license of videos can be identified if available while crawling websites. Limitations: Web crawler is not finely tuned yet, so downloaded content might have low language relevance. It cannot be deployed in zyte service free accounts and can be only deployed in zyte service paid accounts where docker container creation can be customised. License information of videos in web crawler cannot be automatically identified but requires some manual intervention. Getting Started \u00b6 To get started install the prerequisites and clone the repo to machine on which you wish to run the framework. Prerequisites \u00b6 Install ffmpeg library using commands mentioned below. For any linux based operating system (preferred Ubuntu): sudo apt-get install ffmpeg For Mac-os: brew install ffmpeg Windows user can follow installation steps on https://www.ffmpeg.org Install Python Version = 3.6 Get credentials from google developer console for google cloud storage access. Installation \u00b6 Clone the repo using git clone https://github.com/Open-Speech-EkStep/data-acquisition-pipeline.git Go inside the directory cd data-acquisition-pipeline Install python requirements ``` pip install -r requirements.txt Install gcloud utils Download from: https://cloud.google.com/sdk/docs/install#linux > gcloud init Usage \u00b6 This framework allows the user to download the media file from a websource(youtube, xyz.com, etc) and creates the respective metadata file from the data that is extracted from the file.For using any added source or to add new source refer to steps below.It can also crawl internet for media of a specific language. For web crawling, refer to the web crawl configuration below. Common configuration steps: \u00b6 Setting credentials for Google cloud bucket \u00b6 You can set credentials for Google cloud bucket in the credentials.json add the credentials in given manner It can be found in the project root folder. { \"Credentials\" : { YOUR ACCOUNT CREDENTIAL KEYS }} Note: All configuration files can be found in the following path data-acquisition-pipeline/data_acquisition_framework/configs/ Bucket configuration \u00b6 Bucket configurations for data transfer in storage_config.json \"bucket\" : \"ekstepspeechrecognition-dev\" , Your bucket name \"channel_blob_path\" : \"scrapydump/refactor_test\" , Path to directory where downloaded files is to be stored \"archive_blob_path\" : \"archive\" , Folder name in which history of download is to be maintained \"channels_file_blob_path\" : \"channels\" , Folder name in which channels and its videos are saved \"scraped_data_blob_path\" : \"data_to_be_scraped\" Folder name in which CSV for youtube file mode is stored Note: 1 . The scraped_data_blob_path folder should be present inside the channel_blob_path folder. 2 . The CSV file used in file mode of youtube and its name must be same as source_name given above. 3 . ( only for datacollector_urls and datacollector_bing spiders ) To autoconfigure language parameter to channel_blob_path from web_crawler_config.json, use <language> in channel_blob_path. \"eg: for tamil : data/download/<language>/audio - this will replace <language> with tamil.\" 4 . The archive_blob_path and channels_file_blob_path are folders that will be autogenerated in bucket with the given name. Metadata file configurations \u00b6 Metadata file configurations in config.json mode: 'complete' This should not be changed audio_id: null If you want to give a custom audio id add here cleaned_duration: null If you know the cleaned duration of audio add here num_of_speakers: null Number of speaker present in audio language: Hindi Language of audio has_other_audio_signature: False If audio has multiple speaker in same file ( True/False ) type: 'audio' Type of media ( audio or video ) source: 'Demo_Source' Source name experiment_use: False If its for experimental use ( True/False ) utterances_files_list: null source_website: '' Source website url experiment_name: null Name of experiment if experiment_use is True mother_tongue: null Accent of language ( Bengali, Marathi, etc... ) age_group: null Age group of speaker in audio recorded_state: null State in which audio is recorded recorded_district: null District of state in which audio is recorded recorded_place: null Recording location recorded_date: null Recording date purpose: null Purpose of recording speaker_gender: null Gender of speaker speaker_name: null Name of speaker Note: 1 . If any of the field info is not available keep its value to null 2 . If speaker_name or speaker_gender is given then that same will be used for all the files in given source Youtube download configurations \u00b6 You can set download mode [file/channel] in youtube_pipeline_config.py mode = 'file' # [channel,file] In file mode you will store a csv file whose name must be same as source name in scraped_data_blob_path. csv must contain urls of youtube videos, speaker name and gender as three different columns. Urls is a must field. You can leave speaker name and gender blank if data is not available. Given below is the structure of csv. video_url,speaker_name,speaker_gender https://www.youtube.com/watch?v = K1vW_ZikA5o,Ram_Singh,male https://www.youtube.com/watch?v = o82HIOgozi8,John_Doe,male ... Common configurations in youtube_pipeline_config.py # Common configurations \"source_name\" : \"DEMO\" , This is the name of source you are downloading batch_num = 1 Number of videos to be downloaded as batches youtube_service_to_use = YoutubeService.YOUTUBE_DL This field is to choose which service to use for getting video information only_creative_commons = False Should Download only creative commons ( True, False ) Possible values for youtube_service_to_use: (YoutubeService.YOUTUBE_DL, YoutubeService.YOUTUBE_API) File mode configurations in youtube_pipeline_config.py # File Mode configurations file_speaker_gender_column = 'speaker_gender' Gender column name in csv file file_speaker_name_column = \"speaker_name\" Speaker name column name in csv file file_url_name_column = \"video_url\" Video url column name in csv file license_column = \"license\" Video license column name in csv file channel mode configuration in youtube_pipeline_config.py # Channel mode configurations channel_url_dict = {} Channel url dictionary ( This will download all the videos from the given channels with corresponding source names ) Note: 1 . In channel_url_dict, the keys must be the urls and values must be their channel names 2 . To get list of channels from youtube API, channel_url_dict must be empty Youtube API configuration \u00b6 Automated Youtube fetching configuration in youtube_api_config.json # Youtube API configurations \"language\" : \"hindi\" , Type of language for which search results are required. \"language_code\" : \"hi\" , Language code for the specified language. \"keywords\" : [ The search keywords to be given in youtube API query \"audio\" , \"speech\" , \"talk\" ] , \"words_to_ignore\" : [ The words that are to be ignored in youtube API query \"song\" , \"music\" ] , \"max_results\" : 20 Maximum number of channels or results that is required. Web Crawl Configuration \u00b6 web crawl configuration in web_crawl_config.json (Use this only for datacollector_bing and datacollector_urls spider) \"language\" : \"gujarati\" , Language to be crawled \"language_code\" : \"gu\" , Language code for the specified language. \"keywords\" : [ Keywords to query \"talks audio\" , \"audiobooks\" , \"speeches\" , ] , \"word_to_ignore\" : [ Words to ignore while crawling \"ieeexplore.ieee.org\" , \"dl.acm.org\" , \"www.microsoft.com\" ] , \"extensions_to_ignore\" : [ Formats/extensions to ignore while crawling \".jpeg\" , \"xlsx\" , \".xml\" ] , \"extensions_to_include\" : [ Formats/extensions to include while crawling \".mp3\" , \".wav\" , \".mp4\" , ] , \"pages\" : 1 , Number of pages to crawl \"depth\" : 1 , Nesting depth for each website \"continue_page\" : \"NO\" , Field to continue /resume crawling \"last_visited\" : 200 , Last visited results count \"enable_hours_restriction\" : \"YES\" , Restrict crawling based on hours of data collected \"max_hours\" : 1 Maximum hours to crawl Adding new spider \u00b6 As we already mentioned our framework is extensible for any new source. To add a new source user just need to write a spider for that source. To add a spider you can follow the scrapy documentation or you can check our sample spider. Running services \u00b6 Make sure the google credentials are present in project root folder in credentials.json file. Youtube spider in channel mode: \u00b6 In data_acqusition_framework/configs , do the following: Open config.json and change language and type to your respective use case. Open storage_config.json and change bucket and channel_blob_path to your respective gcp paths.(For more info on these fields, scroll above to Bucket configuration) Open youtube_pipeline_config.py and change mode to channel (eg: mode='channel') There are two ways to download videos of youtube channels: You can hardcode the channel url and channel name. You can use youtube-utils service(youtube-dl/youtube data api) to fetch channels and its respective videos information. To download by hardcoding the channel urls, do the following: Open data_acqusition_framework/configs/youtube_pipeline_config.py and do the following: Add the channel_urls and its names in channel_url_dict variable. eg. channel_url_dict = { \"https://www.youtube.com/channel/1\": \"channel_name_a\", \"https://www.youtube.com/channel/2\":\"channel_name_b\" } Set youtube_service_to_use variable value to either YoutubeService.YOUTUBE_DL or YoutubeService.YOUTUBE_API for collecting video info. If YoutubeService.YOUTUBE_API is chosen, then get APIKEY for youtube data api from google developer console and store it in a file called .youtube_api_key in project root folder. Generate .youtube_api_key from From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos from youtube for the given channels and download them to bucket. To download by using youtube-utils service, do the following: Open data_acqusition_framework/configs/youtube_pipeline_config.py and do the following: Assign channel_url_dict = {} (If not empty, will not work). Set youtube_service_to_use variable value to either YoutubeService.YOUTUBE_DL or YoutubeService.YOUTUBE_API for collecting video info. If YoutubeService.YOUTUBE_API is chosen, then get APIKEY for youtube data api from google developer console and store it in a file called .youtube_api_key in project root folder. Open data_acqusition_framework/configs/youtube_api_config.json and change the fields to your requirements.(For more info: check above in Youtube api configuration) From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos from youtube for the given channels and download them to bucket. Youtube spider in file mode: \u00b6 In data_acqusition_framework/configs , do the following: Open config.json and change language and type to your respective use case. Open storage_config.json and change bucket and channel_blob_path to your respective gcp paths.(For more info on these fields, scroll above to Bucket configuration) Open youtube_pipeline_config.py and do the following: change mode to file (eg: mode='file'). change source_name to your requirement so that videos get downloaded to that folder in google storage bucket. Next Steps: Create a file in the following format: eg. source_name.csv with content (license column is optional): Here source_name in source_name.csv is the name you gave in youtube_pipeline_config.py file. It should be the same. video_url,speaker_name,speaker_gender,license https://www.youtube.com/watch?v=K1vW_ZikA5o,Ram_Singh,male,Creative Commons https://www.youtube.com/watch?v=o82HIOgozi8,John_Doe,male,Standard Youtube ... Now to upload this file to google cloud storage do the following: Open the channel_blob_path folder that you gave in storage_config.json and create a folder there named data_to_be_scraped . Upload the file that you created with previous step to this folder. From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos mentioned in the file from youtube and download them to bucket. Bing Spider \u00b6 Configure data_acquisition_framework/configs/web_crawl_config.json for your requirements. Starting datacollector_bing spider with audio pipeline. From project root folder, run the following: scrapy crawl datacollector_bing Urls Spider \u00b6 Configure data_acquisition_framework/configs/web_crawl_config.json for your requirements. Starting datacollector_urls spider with audio pipeline. Make sure to put the urls to crawl in the data_acquisition_framework/urls.txt . From project root folder, run the following: scrapy crawl datacollector_urls Selenium google crawler \u00b6 It is capable of crawling search results of google for a given language and exporting them to urls.txt file. This urls.txt file can be used with datacollector_urls spider to crawl all the search results website and download the media along with their metadata. A specified Readme can be found in selenium_google_crawler folder. Readme for selenium google crawler Selenium youtube crawler for file mode and api mode \u00b6 It is capable of crawling youtube videos using youtube api or from a list of files with youtube video ids provided with channel name as filename. A specified Readme can be found in selenium_youtube_crawler folder. Readme for selenium youtube crawler Contributing \u00b6 Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request License \u00b6 Distributed under the [XYZ] License. See LICENSE for more information. Git Repository \u00b6 https://github.com/Open-Speech-EkStep/data-acquisition-pipeline Contact \u00b6 Connect with community on Gitter Acknowledgements \u00b6 Scrapy YouTube-dl TinyTag","title":"Data Collection Pipeine"},{"location":"data_collection/#data-collection-pipeline","text":"","title":"Data Collection Pipeline"},{"location":"data_collection/#table-of-contents","text":"Data Collection Pipeline Table of Contents About The Project Built With Summary Getting Started Prerequisites Installation Usage Common configuration steps: Setting credentials for Google cloud bucket Bucket configuration Metadata file configurations Youtube download configurations Youtube API configuration Web Crawl Configuration Adding new spider Running services Youtube spider in channel mode: Youtube spider in file mode: Bing Spider Urls Spider Selenium google crawler Selenium youtube crawler for file mode and api mode Contributing License Git Repository Contact Acknowledgements","title":"Table of Contents"},{"location":"data_collection/#about-the-project","text":"This is downloading framework that is extensible and allows the user to add new source without much code changes. For each new source user need to write a scrapy spider script and rest of downloading and meta file creation is handled by repective pipelines. And if required user can add their custom pipelines. This framework automatically transfer the downloaded data to a Google cloud bucket automatically. For more info on writing scrapy spider and pipeline one can refer to the documentation . Data Collection Pipeline\u2019s developer documentation is meant for its adopters, developers and contributors. The developer documentation helps you to get familiar with the bare necessities, giving you a quick and clean approach to get you up and running. If you are looking for ways to customize the workflow, or just breaking things down to build them back up, head to the reference section to dig into the mechanics of Data Collection Pipeline. Data Collection Pipeline is based on an open platform, you are free to use any programming language to extend or customize it but we prefer to use python to perform smart scraping. The Developer documentation provides you with a complete set of guidelines which you need to: Install Data Collection Pipeline Configure Data Collection Pipeline Customize Data Collection Pipeline Extend Data Collection Pipeline Contribute to Data Collection Pipeline","title":"About The Project"},{"location":"data_collection/#built-with","text":"We have used scrapy as the base of this framework. * Scrapy","title":"Built With"},{"location":"data_collection/#summary","text":"This summary mentions the key advantages and limitations of this smart crawler service. Youtube Crawler Key Points and Advantages: Get language relevant channels from YouTube and download videos from them.(70%-80% relevancy with language - based on Manual Analysis) Can fetch channels with Creative Commons video and download the videos in them as well.(70% relevancy with language) Can download using file mode(manually filled with video Ids) or channel mode. Youtube-dl can fetch N number of videos from a channel and download them. YouTube crawler downloads files at a rate of maximum of 2000 hours per day and minimum of 800 hours per day. Youtube crawler is more convenient and it\u2019s a main source of Creative Commons data that can be accessed easily. It can be deployed in cloud service called zyte used for scraping/crawling. License information of videos are available in metadata. Limitations: Youtube-api cannot return more than 500 videos per channel.(when using YOUTUBE_API mode in configuration) Youtube-api is restricted to 10000 tokens per day in free mode. 10000 tokens can be used to get license info of 10000 videos.(in any mode) 10000 tokens can be used to get 5000 channels.(in YOUTUBE_API mode) Youtube-dl can be used to get all videos freely.(in YOUTUBE_DL mode) Cannot fetch data from specific playlist. (Solution: Fetch videos Ids of a playlist using YouTube-dl and put them in a file and download in file mode.) Rare cases in which you might get Too many requests error from Youtube-DL. (Solution: Rerun the application with same sources.) Cannot download videos which require user information and private videos. Web Crawler Key Points and Advantages: Web crawler can download specific language audio but with around 50 - 60% relevance. Web crawler downloads files at a rate of at least 2000 hours per day. It is a faster means of downloading data. Creative Commons license of videos can be identified if available while crawling websites. Limitations: Web crawler is not finely tuned yet, so downloaded content might have low language relevance. It cannot be deployed in zyte service free accounts and can be only deployed in zyte service paid accounts where docker container creation can be customised. License information of videos in web crawler cannot be automatically identified but requires some manual intervention.","title":"Summary"},{"location":"data_collection/#getting-started","text":"To get started install the prerequisites and clone the repo to machine on which you wish to run the framework.","title":"Getting Started"},{"location":"data_collection/#prerequisites","text":"Install ffmpeg library using commands mentioned below. For any linux based operating system (preferred Ubuntu): sudo apt-get install ffmpeg For Mac-os: brew install ffmpeg Windows user can follow installation steps on https://www.ffmpeg.org Install Python Version = 3.6 Get credentials from google developer console for google cloud storage access.","title":"Prerequisites"},{"location":"data_collection/#installation","text":"Clone the repo using git clone https://github.com/Open-Speech-EkStep/data-acquisition-pipeline.git Go inside the directory cd data-acquisition-pipeline Install python requirements ``` pip install -r requirements.txt Install gcloud utils Download from: https://cloud.google.com/sdk/docs/install#linux > gcloud init","title":"Installation"},{"location":"data_collection/#usage","text":"This framework allows the user to download the media file from a websource(youtube, xyz.com, etc) and creates the respective metadata file from the data that is extracted from the file.For using any added source or to add new source refer to steps below.It can also crawl internet for media of a specific language. For web crawling, refer to the web crawl configuration below.","title":"Usage"},{"location":"data_collection/#common-configuration-steps","text":"","title":"Common configuration steps:"},{"location":"data_collection/#setting-credentials-for-google-cloud-bucket","text":"You can set credentials for Google cloud bucket in the credentials.json add the credentials in given manner It can be found in the project root folder. { \"Credentials\" : { YOUR ACCOUNT CREDENTIAL KEYS }} Note: All configuration files can be found in the following path data-acquisition-pipeline/data_acquisition_framework/configs/","title":"Setting credentials for Google cloud bucket"},{"location":"data_collection/#bucket-configuration","text":"Bucket configurations for data transfer in storage_config.json \"bucket\" : \"ekstepspeechrecognition-dev\" , Your bucket name \"channel_blob_path\" : \"scrapydump/refactor_test\" , Path to directory where downloaded files is to be stored \"archive_blob_path\" : \"archive\" , Folder name in which history of download is to be maintained \"channels_file_blob_path\" : \"channels\" , Folder name in which channels and its videos are saved \"scraped_data_blob_path\" : \"data_to_be_scraped\" Folder name in which CSV for youtube file mode is stored Note: 1 . The scraped_data_blob_path folder should be present inside the channel_blob_path folder. 2 . The CSV file used in file mode of youtube and its name must be same as source_name given above. 3 . ( only for datacollector_urls and datacollector_bing spiders ) To autoconfigure language parameter to channel_blob_path from web_crawler_config.json, use <language> in channel_blob_path. \"eg: for tamil : data/download/<language>/audio - this will replace <language> with tamil.\" 4 . The archive_blob_path and channels_file_blob_path are folders that will be autogenerated in bucket with the given name.","title":"Bucket configuration"},{"location":"data_collection/#metadata-file-configurations","text":"Metadata file configurations in config.json mode: 'complete' This should not be changed audio_id: null If you want to give a custom audio id add here cleaned_duration: null If you know the cleaned duration of audio add here num_of_speakers: null Number of speaker present in audio language: Hindi Language of audio has_other_audio_signature: False If audio has multiple speaker in same file ( True/False ) type: 'audio' Type of media ( audio or video ) source: 'Demo_Source' Source name experiment_use: False If its for experimental use ( True/False ) utterances_files_list: null source_website: '' Source website url experiment_name: null Name of experiment if experiment_use is True mother_tongue: null Accent of language ( Bengali, Marathi, etc... ) age_group: null Age group of speaker in audio recorded_state: null State in which audio is recorded recorded_district: null District of state in which audio is recorded recorded_place: null Recording location recorded_date: null Recording date purpose: null Purpose of recording speaker_gender: null Gender of speaker speaker_name: null Name of speaker Note: 1 . If any of the field info is not available keep its value to null 2 . If speaker_name or speaker_gender is given then that same will be used for all the files in given source","title":"Metadata file configurations"},{"location":"data_collection/#youtube-download-configurations","text":"You can set download mode [file/channel] in youtube_pipeline_config.py mode = 'file' # [channel,file] In file mode you will store a csv file whose name must be same as source name in scraped_data_blob_path. csv must contain urls of youtube videos, speaker name and gender as three different columns. Urls is a must field. You can leave speaker name and gender blank if data is not available. Given below is the structure of csv. video_url,speaker_name,speaker_gender https://www.youtube.com/watch?v = K1vW_ZikA5o,Ram_Singh,male https://www.youtube.com/watch?v = o82HIOgozi8,John_Doe,male ... Common configurations in youtube_pipeline_config.py # Common configurations \"source_name\" : \"DEMO\" , This is the name of source you are downloading batch_num = 1 Number of videos to be downloaded as batches youtube_service_to_use = YoutubeService.YOUTUBE_DL This field is to choose which service to use for getting video information only_creative_commons = False Should Download only creative commons ( True, False ) Possible values for youtube_service_to_use: (YoutubeService.YOUTUBE_DL, YoutubeService.YOUTUBE_API) File mode configurations in youtube_pipeline_config.py # File Mode configurations file_speaker_gender_column = 'speaker_gender' Gender column name in csv file file_speaker_name_column = \"speaker_name\" Speaker name column name in csv file file_url_name_column = \"video_url\" Video url column name in csv file license_column = \"license\" Video license column name in csv file channel mode configuration in youtube_pipeline_config.py # Channel mode configurations channel_url_dict = {} Channel url dictionary ( This will download all the videos from the given channels with corresponding source names ) Note: 1 . In channel_url_dict, the keys must be the urls and values must be their channel names 2 . To get list of channels from youtube API, channel_url_dict must be empty","title":"Youtube download configurations"},{"location":"data_collection/#youtube-api-configuration","text":"Automated Youtube fetching configuration in youtube_api_config.json # Youtube API configurations \"language\" : \"hindi\" , Type of language for which search results are required. \"language_code\" : \"hi\" , Language code for the specified language. \"keywords\" : [ The search keywords to be given in youtube API query \"audio\" , \"speech\" , \"talk\" ] , \"words_to_ignore\" : [ The words that are to be ignored in youtube API query \"song\" , \"music\" ] , \"max_results\" : 20 Maximum number of channels or results that is required.","title":"Youtube API configuration"},{"location":"data_collection/#web-crawl-configuration","text":"web crawl configuration in web_crawl_config.json (Use this only for datacollector_bing and datacollector_urls spider) \"language\" : \"gujarati\" , Language to be crawled \"language_code\" : \"gu\" , Language code for the specified language. \"keywords\" : [ Keywords to query \"talks audio\" , \"audiobooks\" , \"speeches\" , ] , \"word_to_ignore\" : [ Words to ignore while crawling \"ieeexplore.ieee.org\" , \"dl.acm.org\" , \"www.microsoft.com\" ] , \"extensions_to_ignore\" : [ Formats/extensions to ignore while crawling \".jpeg\" , \"xlsx\" , \".xml\" ] , \"extensions_to_include\" : [ Formats/extensions to include while crawling \".mp3\" , \".wav\" , \".mp4\" , ] , \"pages\" : 1 , Number of pages to crawl \"depth\" : 1 , Nesting depth for each website \"continue_page\" : \"NO\" , Field to continue /resume crawling \"last_visited\" : 200 , Last visited results count \"enable_hours_restriction\" : \"YES\" , Restrict crawling based on hours of data collected \"max_hours\" : 1 Maximum hours to crawl","title":"Web Crawl Configuration"},{"location":"data_collection/#adding-new-spider","text":"As we already mentioned our framework is extensible for any new source. To add a new source user just need to write a spider for that source. To add a spider you can follow the scrapy documentation or you can check our sample spider.","title":"Adding new spider"},{"location":"data_collection/#running-services","text":"Make sure the google credentials are present in project root folder in credentials.json file.","title":"Running services"},{"location":"data_collection/#youtube-spider-in-channel-mode","text":"In data_acqusition_framework/configs , do the following: Open config.json and change language and type to your respective use case. Open storage_config.json and change bucket and channel_blob_path to your respective gcp paths.(For more info on these fields, scroll above to Bucket configuration) Open youtube_pipeline_config.py and change mode to channel (eg: mode='channel') There are two ways to download videos of youtube channels: You can hardcode the channel url and channel name. You can use youtube-utils service(youtube-dl/youtube data api) to fetch channels and its respective videos information. To download by hardcoding the channel urls, do the following: Open data_acqusition_framework/configs/youtube_pipeline_config.py and do the following: Add the channel_urls and its names in channel_url_dict variable. eg. channel_url_dict = { \"https://www.youtube.com/channel/1\": \"channel_name_a\", \"https://www.youtube.com/channel/2\":\"channel_name_b\" } Set youtube_service_to_use variable value to either YoutubeService.YOUTUBE_DL or YoutubeService.YOUTUBE_API for collecting video info. If YoutubeService.YOUTUBE_API is chosen, then get APIKEY for youtube data api from google developer console and store it in a file called .youtube_api_key in project root folder. Generate .youtube_api_key from From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos from youtube for the given channels and download them to bucket. To download by using youtube-utils service, do the following: Open data_acqusition_framework/configs/youtube_pipeline_config.py and do the following: Assign channel_url_dict = {} (If not empty, will not work). Set youtube_service_to_use variable value to either YoutubeService.YOUTUBE_DL or YoutubeService.YOUTUBE_API for collecting video info. If YoutubeService.YOUTUBE_API is chosen, then get APIKEY for youtube data api from google developer console and store it in a file called .youtube_api_key in project root folder. Open data_acqusition_framework/configs/youtube_api_config.json and change the fields to your requirements.(For more info: check above in Youtube api configuration) From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos from youtube for the given channels and download them to bucket.","title":"Youtube spider in channel mode:"},{"location":"data_collection/#youtube-spider-in-file-mode","text":"In data_acqusition_framework/configs , do the following: Open config.json and change language and type to your respective use case. Open storage_config.json and change bucket and channel_blob_path to your respective gcp paths.(For more info on these fields, scroll above to Bucket configuration) Open youtube_pipeline_config.py and do the following: change mode to file (eg: mode='file'). change source_name to your requirement so that videos get downloaded to that folder in google storage bucket. Next Steps: Create a file in the following format: eg. source_name.csv with content (license column is optional): Here source_name in source_name.csv is the name you gave in youtube_pipeline_config.py file. It should be the same. video_url,speaker_name,speaker_gender,license https://www.youtube.com/watch?v=K1vW_ZikA5o,Ram_Singh,male,Creative Commons https://www.youtube.com/watch?v=o82HIOgozi8,John_Doe,male,Standard Youtube ... Now to upload this file to google cloud storage do the following: Open the channel_blob_path folder that you gave in storage_config.json and create a folder there named data_to_be_scraped . Upload the file that you created with previous step to this folder. From the project root folder, run the following command: scrapy crawl datacollector_youtube --set=ITEM_PIPELINES='{\"data_acquisition_framework.pipelines.youtube_api_pipeline.YoutubeApiPipeline\": 1}' This will start fetching the videos mentioned in the file from youtube and download them to bucket.","title":"Youtube spider in file mode:"},{"location":"data_collection/#bing-spider","text":"Configure data_acquisition_framework/configs/web_crawl_config.json for your requirements. Starting datacollector_bing spider with audio pipeline. From project root folder, run the following: scrapy crawl datacollector_bing","title":"Bing Spider"},{"location":"data_collection/#urls-spider","text":"Configure data_acquisition_framework/configs/web_crawl_config.json for your requirements. Starting datacollector_urls spider with audio pipeline. Make sure to put the urls to crawl in the data_acquisition_framework/urls.txt . From project root folder, run the following: scrapy crawl datacollector_urls","title":"Urls Spider"},{"location":"data_collection/#selenium-google-crawler","text":"It is capable of crawling search results of google for a given language and exporting them to urls.txt file. This urls.txt file can be used with datacollector_urls spider to crawl all the search results website and download the media along with their metadata. A specified Readme can be found in selenium_google_crawler folder. Readme for selenium google crawler","title":"Selenium google crawler"},{"location":"data_collection/#selenium-youtube-crawler-for-file-mode-and-api-mode","text":"It is capable of crawling youtube videos using youtube api or from a list of files with youtube video ids provided with channel name as filename. A specified Readme can be found in selenium_youtube_crawler folder. Readme for selenium youtube crawler","title":"Selenium youtube crawler for file mode and api mode"},{"location":"data_collection/#contributing","text":"Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request","title":"Contributing"},{"location":"data_collection/#license","text":"Distributed under the [XYZ] License. See LICENSE for more information.","title":"License"},{"location":"data_collection/#git-repository","text":"https://github.com/Open-Speech-EkStep/data-acquisition-pipeline","title":"Git Repository"},{"location":"data_collection/#contact","text":"Connect with community on Gitter","title":"Contact"},{"location":"data_collection/#acknowledgements","text":"Scrapy YouTube-dl TinyTag","title":"Acknowledgements"},{"location":"gender_identification/","text":"Gender Identification \u00b6 Table of Contents \u00b6 Gender Identification Table of Contents About The Project Working Embeddings Classification algorithm Hyperparameters Train data Test data Usage Parameters to change Commands for Inference About The Project \u00b6 Gender Identification is the task of classifying an audio into gender labels (we used 'male' and 'female'). We needed to identify whether the voice present in each utterance was male or female since it is a very important step while trying to balance your data among gender classes, for any downstream task. This also helps in maintaining diversity in data. Our intelligent data pipelines split each audio, based on Voice Activity Detection, as one of the starting steps. These shorter audio utterances can then used to train deep learning models. We assume that the utterances are short enough to have one speaker per utterance, since the splitting logic is using unvoiced segments as points to split an audio. We use embeddings (explained in the next section) to train a machine learning based classifier and achieve an accurracy of 96% on our test set. (Datasets and training explained later.) Working \u00b6 Embeddings \u00b6 To convert our audio utternaces into fixed length summary vectors, we use the Voice Encoder model proposed here , and implemented here for converting our audio utterances into fixed length embeddings. Voice Encoder is a speaker-discriminative model trained on a text-independent speaker verification task. Since these embeddings are able to summarise the characteristics of the voice spoken, they have been used for Gender Classification here , as shown in the diagram below. The above plot shows a clear boundary for gender between embeddings from distinct speakers present in LibriSpeech dataset, when projected in 2D using umap. Classification algorithm \u00b6 We use the Support vector machine classifier defined here to train for gender classification task, with embeddings from previous step and input, and their correct gender labels as target classes. Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection. The advantages of support vector machines are: Effective in high dimensional spaces. Our space is 256 dimensional. Still effective in cases where number of dimensions is greater than the number of samples. Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient. Hyperparameters \u00b6 Below is the description of each hyperparameter we used, with the value we used it it. kernel: 'rbf' RBF Kernel is popular because of its similarity to K-Nearest Neighborhood Algorithm. It has the advantages of K-NN and overcomes the space complexity problem as RBF Kernel Support Vector Machines just needs to store the support vectors during training and not the entire dataset. When training an SVM with the Radial Basis Function (RBF) kernel, two parameters must be considered: C and gamma. The parameter C, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly. gamma defines how much influence a single training example has. The larger gamma is, the closer other examples must be to be affected. We used the following values for C and gamma: - gamma: 0.01 - C: 100 Train data \u00b6 We used data for training as a combined set of audios in the following languages: Hindi, Tamil, Telugu, Kannada. Final train dataset had the following distribution: - male: 17.4 hours - female: 16.2 hours Test data \u00b6 We used data for training as a combined set of audios in the following languages: Hindi, Tamil, Telugu, Kannada, Marathi and Bengali. Test data had two additional languages, as compared to the train data: Marathi and Bengali. Final test dataset (balanced) had the following distribution: - male: 3.6 hours - female: 3.6 hours Usage \u00b6 Parameters to change \u00b6 model-path : set the path where you want to save the trained model. eg: 'path/to/trained/model/model.sav' file-mode : default=False; set True for single file inference file-path : path to .wav file if --file-mode = True csv-path : path to csv containing multiple audio file paths save-dir : default= current directory; else give path to save predictions.csv Commands for Inference \u00b6 For single file inference: python scripts/inference.py --model-path model/clf_svc.sav --file-mode True --file-path <filename>.wav For CSV mode inference: Create a csv containing multiple file paths python scripts/inference.py --model-path model/clf_svc.sav --csv-path <file_paths>.csv --save-dir <destination path>","title":"Gender Identification"},{"location":"gender_identification/#gender-identification","text":"","title":"Gender Identification"},{"location":"gender_identification/#table-of-contents","text":"Gender Identification Table of Contents About The Project Working Embeddings Classification algorithm Hyperparameters Train data Test data Usage Parameters to change Commands for Inference","title":"Table of Contents"},{"location":"gender_identification/#about-the-project","text":"Gender Identification is the task of classifying an audio into gender labels (we used 'male' and 'female'). We needed to identify whether the voice present in each utterance was male or female since it is a very important step while trying to balance your data among gender classes, for any downstream task. This also helps in maintaining diversity in data. Our intelligent data pipelines split each audio, based on Voice Activity Detection, as one of the starting steps. These shorter audio utterances can then used to train deep learning models. We assume that the utterances are short enough to have one speaker per utterance, since the splitting logic is using unvoiced segments as points to split an audio. We use embeddings (explained in the next section) to train a machine learning based classifier and achieve an accurracy of 96% on our test set. (Datasets and training explained later.)","title":"About The Project"},{"location":"gender_identification/#working","text":"","title":"Working"},{"location":"gender_identification/#embeddings","text":"To convert our audio utternaces into fixed length summary vectors, we use the Voice Encoder model proposed here , and implemented here for converting our audio utterances into fixed length embeddings. Voice Encoder is a speaker-discriminative model trained on a text-independent speaker verification task. Since these embeddings are able to summarise the characteristics of the voice spoken, they have been used for Gender Classification here , as shown in the diagram below. The above plot shows a clear boundary for gender between embeddings from distinct speakers present in LibriSpeech dataset, when projected in 2D using umap.","title":"Embeddings"},{"location":"gender_identification/#classification-algorithm","text":"We use the Support vector machine classifier defined here to train for gender classification task, with embeddings from previous step and input, and their correct gender labels as target classes. Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection. The advantages of support vector machines are: Effective in high dimensional spaces. Our space is 256 dimensional. Still effective in cases where number of dimensions is greater than the number of samples. Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.","title":"Classification algorithm"},{"location":"gender_identification/#hyperparameters","text":"Below is the description of each hyperparameter we used, with the value we used it it. kernel: 'rbf' RBF Kernel is popular because of its similarity to K-Nearest Neighborhood Algorithm. It has the advantages of K-NN and overcomes the space complexity problem as RBF Kernel Support Vector Machines just needs to store the support vectors during training and not the entire dataset. When training an SVM with the Radial Basis Function (RBF) kernel, two parameters must be considered: C and gamma. The parameter C, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low C makes the decision surface smooth, while a high C aims at classifying all training examples correctly. gamma defines how much influence a single training example has. The larger gamma is, the closer other examples must be to be affected. We used the following values for C and gamma: - gamma: 0.01 - C: 100","title":"Hyperparameters"},{"location":"gender_identification/#train-data","text":"We used data for training as a combined set of audios in the following languages: Hindi, Tamil, Telugu, Kannada. Final train dataset had the following distribution: - male: 17.4 hours - female: 16.2 hours","title":"Train data"},{"location":"gender_identification/#test-data","text":"We used data for training as a combined set of audios in the following languages: Hindi, Tamil, Telugu, Kannada, Marathi and Bengali. Test data had two additional languages, as compared to the train data: Marathi and Bengali. Final test dataset (balanced) had the following distribution: - male: 3.6 hours - female: 3.6 hours","title":"Test data"},{"location":"gender_identification/#usage","text":"","title":"Usage"},{"location":"gender_identification/#parameters-to-change","text":"model-path : set the path where you want to save the trained model. eg: 'path/to/trained/model/model.sav' file-mode : default=False; set True for single file inference file-path : path to .wav file if --file-mode = True csv-path : path to csv containing multiple audio file paths save-dir : default= current directory; else give path to save predictions.csv","title":"Parameters to change"},{"location":"gender_identification/#commands-for-inference","text":"For single file inference: python scripts/inference.py --model-path model/clf_svc.sav --file-mode True --file-path <filename>.wav For CSV mode inference: Create a csv containing multiple file paths python scripts/inference.py --model-path model/clf_svc.sav --csv-path <file_paths>.csv --save-dir <destination path>","title":"Commands for Inference"},{"location":"intelligent_data_pipelines/","text":"Intelligent Data Pipeline \u00b6 Table of Contents \u00b6 Intelligent Data Pipeline Table of Contents About The Project Getting Started Architecture Intelligent Data Pipeline - Jobs Audio Processor Audio Analysis Language identification Speaker identification Gender identification Audio Data Balancing Audio Validation Audio Transcription Installation Run on Kubernetes Using Composer Requirements Infra Setup CI/CD setup Audio Processing Config Description Config Steps to run Audio Analysis Config Config Steps to run Data Balancing Config config steps to run: Audio Transcription (with config): config: steps to run: Contributing License Git Repository Contact About The Project \u00b6 Intelligent Data Pipelines are built to create the audio data set that can be used for Speech Recognition deeplearning models. The aim is to allow easy, quick and fast dataset generation without doing manual work. It splits data into smallers utterences which are understood well by deeplearning models. The data is then cleansed based on 'Signal to Noise ratio'. The audio analysis is performed using pre trained models and clustering based on audio features (see Resemblyzer for more details). It leverages Kubernetes for parallel computing and below are the metrics we have acheived so far: Some stats for a language with 1000 hrs raw data Raw data 1000 hrs Time taken: 2-3 days Final Usable Data of Pretraining: 600 Final Usable Data of Fine Tuning: 400 Getting Started \u00b6 The developer documentation helps you to get familiar with the bare necessities, giving you a quick and clean approach to get you up and running. If you are looking for ways to customize the workflow, or just breaking things down to build them back up, head to the reference section to dig into the mechanics of Data Pipelines. To get started install the prerequisites and clone the repo to machine on which you wish to run the framework. Here is the code Architecture \u00b6 Intelligent Data Pipeline - Jobs \u00b6 Audio Processor \u00b6 Audio Processor job takes raw data generted from Data Collection Pipeline or it also consumes data that is generated by any other means. It converts raw data file to wav file, then it splits wav file into smaller audio chunks. Then 'Signal to Noise Ratio (SNR)' is generated for each audio chunk and then filtration is performed based on the 'Signal to Noise Ratio (SNR)'. The threshold can be configured through configurations file. It then adds audio metadata to the catalogue (PostgresDB). Audio Analysis \u00b6 Audio Analysis job takes processed and filtered audio chunks generated from Audio Processor job and performs three type of analysis: Language identification \u00b6 It predicts the language spoken in each audio chunk using a pre-trained language identification model for a language. It gives the confidence score of the language for each audio chunk. Please see this for more details. Speaker Clustering \u00b6 It estimates the total number of speakers in a particular source and maps speaker to audio chunks. That mapping of speaker to audio chunk is required for data balancing. Please see this for more details. Gender identification \u00b6 It predicts the speaker gender for each audio chunk using a pre-trained gender identification model. Please see this for more details. Audio Data Balancing \u00b6 The model training data requires data with proper gender ratio. Also, the data should be balanced based on speaker duration. It also provides capability to filter and choose data based on certain metadata filter criteria. Audio Validation \u00b6 The data that goes into model training should be of good quality. This job validates data that is not adhereing to quality standards required by the model. It generates csv reports that can be analysed by data scientists to further filter out the best data for model training. Audio Transcription \u00b6 For model fine-tuning, the paired audio data is required (audio with labeled text). This job generates text for each audio chunk using Google or Azure API's. The texts generated are further sanitized based on the rules defined for the language. Installation \u00b6 Clone the repo git clone git@github.com:Open-Speech-EkStep/audio-to-speech-pipeline.git Install python requirements pip install -r requirements.txt Run on Kubernetes \u00b6 Using Composer \u00b6 Requirements \u00b6 Terraform https://www.terraform.io/downloads.html gcloud https://cloud.google.com/sdk/docs/install kubectl https://kubernetes.io/docs/tasks/tools/install-kubectl-linux Infra Setup \u00b6 Clone the repo: git clone https://github.com/Open-Speech-EkStep/ekstep-deep-speech-infra.git Initialize terraform modules terraform init Select a workspace as per the environments(dev,test,prod). terraform workspace select <env_name> eg: terraform workspace select prod Configure variable \"project\" { descrip t io n = \"The name of the Google Cloud Project.\" de fault = \"<project-name>\" } variable \"composer_env_name\" { descrip t io n = \"The name of the Google composer_env_name.\" de fault = \"ekstepcomposer\" } variable \"script_path\" { descrip t io n = \"The path of the working dir.\" de fault = \"./modules/gcp-composer/\" } variable \"bucket_name\" { descrip t io n = \"The name of the gcp bucket\" de fault = \"<bucket-name>\" } variable \"database_version\" { descrip t io n = \"The name of the database_version.\" t ype = s tr i n g de fault = \"POSTGRES_11\" } variable \"database_instance_name\" { descrip t io n = \"The name of the database_instance.\" t ype = s tr i n g de fault = \"<db-instance-name>\" } variable \"db_region\" { descrip t io n = \"The name of the db region.\" t ype = s tr i n g de fault = \"us-central1\" } variable \"database1\" { descrip t io n = \"The name of the database1.\" t ype = s tr i n g de fault = \"speech_recognition_data_catalog-1\" } variable \"database2\" { descrip t io n = \"The name of the database2.\" t ype = s tr i n g de fault = \"speech_recognition_data_catalog-2\" } variable \"speechrecognition_service_account\" { descrip t io n = \"The name of the speechrecognition_service_account.\" t ype = s tr i n g de fault = \"service-account-1\" } variable \"circleci_service_account\" { descrip t io n = \"The name of the circleci_service_account.\" t ype = s tr i n g de fault = \"servacct-circleci\" } variable \"sql_instance_size\" { de fault = \"db-custom-2-7680\" t ype = s tr i n g descrip t io n = \"Size of Cloud SQL instances\" } variable \"sql_disk_type\" { de fault = \"PD_HDD\" t ype = s tr i n g descrip t io n = \"Cloud SQL instance disk type\" } variable \"sql_disk_size\" { de fault = \"20\" t ype = s tr i n g descrip t io n = \"Storage size in GB\" } Create Service account: terraform apply -target=module.service-accounts Create keys from console.cloud.google.com Set env variable export GOOGLE_APPLICATION_CREDENTIAL_SERVICE_ACC= </path/to/key.json> Run specific modules as per requirements. terraform apply -target = module.<module-name> eg: terraform apply -target = module.sql-database Run all modules at once. terraform apply Connect to Database from local: Setup proxy ./cloud_sql_proxy -dir = ./cloudsql -instances = <project-id>:<zone>:<db-instance-name> = tcp:5432 Create username and password from console. Then connect to localhost Whitelist composer worker IP in Database Network. CI/CD setup \u00b6 Once you pull code you have to configure some variable in your CircleCI . So that while deploying code image should easily push into google container registry. 1. GCP_PROJECT # Name of your GCP project 2. GOOGLE_AUTH # Service account key that is created using terraform 3. POSTGRES_DB # Database host ip that is created using terraform 4. POSTGRES_PASSWORD # Database password 5. POSTGRES_USER # Database user name 6. DB_INSTANCE # Database instance name Audio Processing Config \u00b6 Description \u00b6 Create a yaml file using following config format and configure paths and other parameters. Config \u00b6 config : common : db_configuration : db_name : '' db_pass : '' db_user : '' cloud_sql_connection_name : '<DB Host>' gcs_config : # master data bucket master_bucket : '<Name of the bucket>' audio_processor_config : # feat_language_identification should true if you want run language identification for a source feat_language_identification : False # language of the audio language : '' # path of the files on gcs which need to be processed # path eg: <bucket-name/data/audiotospeech/raw/download/downloaded/{language}/audio> remote_raw_audio_file_path : '' # after processing where we want to move raw data snr_done_folder_path : '' # <bucket-name/data/audiotospeech/raw/download/snr_done/{language}/audio> # path where the processed files need to be uploaded remote_processed_audio_file_path : '' # <bucket-name/data/audiotospeech/raw/download/catalogue/{language}/audio> # path where Duplicate files need to be uploaded based on checksum duplicate_audio_file_path : '' # <bucket-name/data/audiotospeech/raw/download/duplicate/{language}/audio> chunking_conversion_configuration : aggressiveness : '' # using for vad by default it's value is 2 the more the value that aggressive vad for chunking audio max_duration : '' # max duration is second if chunk is more than that vad will retry chunking with inc aggressiveness # SNR specific configurations snr_configuration : max_snr_threshold : '' # less than max_snr_threshold utterance will move to rejected folder. local_input_file_path : '' local_output_file_path : '' Steps to run: \u00b6 We have to configure sourcepathforsnr in airflow variable where our raw data is stored. Other variable that we need to configure is snrcatalogue in that we need to add our source(s) name which we want to process and following parameters: count: Count of files that we want to process in one trigger. format: The format of the raw audio file in bucket. language: Language of source(s). parallelism: Number of pods that will be up in one run. If parallelism is not defined then number of pod = count. ex:- \"snrcatalogue\" : { \"<source_name>\" : { \"count\" : 5 , \"format\" : \"mp3\" , \"language\" : \"telugu\" , \"parallelism\" : 2 }, \"<other_source_name>\" : { \"count\" : 5 , \"format\" : \"mp3\" , \"language\" : \"telugu\" , \"parallelism\" : 5 } } We have to also set audiofilelist with whatever source(s) we want to run with empty array that will store our file path ex:- \"audiofilelist\" : { \"<source_name>\" : [], \"<other_source_name>\" : [] } That will create a DAG with the source_name(s) now we can trigger that DAG, that will process given number(count) of file and upload processed file to remote_processed_audio_file_path that we mentioned in the config file. And move raw data from remote_raw_audio_file_path to snr_done_folder_path . Also, Database will be updated with the metadata which we created using CircleCI. Audio Analysis Config \u00b6 Config \u00b6 audio_analysis_config : analysis_options : gender_analysis : 1 # It should be 1 if you want run gender analysis for a source else it should be 0. speaker_analysis : 0 # It should be 1 if you want run speaker analysis for a source else it should be 0. # path where the processed files need to be uploaded remote_processed_audio_file_path : '' # <bucket-name/data/audiotospeech/raw/download/catalogued/{language}/audio> # path where the embeddings need to be uploaded path_for_embeddings : '' # <bucket-name/data/audiotospeech/raw/download/catalogued/{language}/embeddings/> min_cluster_size : 4 # It is least number of cluster for one speaker. partial_set_size : 15000 # Number of audio chunks to create embeddings for a given source. fit_noise_on_similarity : 0.77 min_samples : 2 Steps to run \u00b6 We have to configure audio_analysis_config in airflow variable using this json, we have to mention source name, language, parallelism and batch size. \"audio_analysis_config\" : { \"<source name>\" : { \"language\" : \"hindi\" , \"format\" : \"wav\" , \"parallelism\" : 5 , \"batch_size\" : 5000 } } That will create a audio_analysis DAG with name source_name_audio_embedding_analysis . Now, we can trigger that DAG and that will process given sources. It will create embeddings, processed files and upload them to path_for_embeddings and remote_processed_audio_file_path respectively that we have mentioned in config file. Also, Database will be updated with the metadata which we created using CircleCI. Data Balancing Config \u00b6 config \u00b6 data_tagger_config : # path of to the folder in the master bucket where the data tagger will move the data to landing_directory_path : '' #'<bucket_name>/data/audiotospeech/raw/download/catalogued/{language}/audio' # path of to the folder in the master bucket from where the data tagger will pick up the data that needs to be moved source_directory_path : '' #'<bucket_name>/data/audiotospeech/raw/landing/{language}/audio' Steps to run: \u00b6 We need to configure data_filter_config airflow variable for each source. We provide 2 modes of filtration file mode and filter mode . Only, one mode can be used at a time. To use filter mode, \"file_mode\": \"n\" . If \"file_mode\": \"y\" , then snr filter, duration filter etc. won't work. data_set : select data set type from 'train' and 'test'. file_mode : It should be 'y' if you want to use file_mode for a source else it should be 'n'. This mode can be used when we need to filter out some specific files that we found after analysis by providing path of the CSV file in file_path parameter. file_path : path of the CSV file We have multiple filters: by_snr : filter based on SNR value. \"lte\" means lower than and \"gte\" means greater than by_duration :total duration from a given source. by_speaker : we can configure how much data per speaker we want. by_utterance_duration : we can required duration of utterance. exclude_audio_ids : we can pass a list of audio_ids that we want to skip. exclude_speaker_ids : we can pass a list of speaker_ids that we want to skip. with_randomness : It is a boolean value if it's true it will pickup random data from DB. \"data_filter_config\" : { \"test_source1\" : { \"language\" : \"hindi\" , \"file_mode\" : \"n\" , \"data_set\" : \"train\" , \"file_path\" : \"data/audiotospeech/raw/download/duplicate/test_source1.csv\" , \"filter\" : { \"by_snr\" : { \"lte\" : 75 , \"gte\" : 15 }, \"by_duration\" : 2 , \"with_randomness\" : \"true\" } }, \"test_source2\" : { \"language\" : \"hindi\" , \"file_mode\" : \"n\" , \"data_set\" : \"train\" , \"file_path\" : \"data/audiotospeech/raw/download/duplicate/test_source2.csv\" , \"filter\" : { \"by_speaker\" : { \"lte_per_speaker_duration\" : 60 , \"gte_per_speaker_duration\" : 0 , \"with_threshold\" : 0 }, \"by_duration\" : 2 } } } After configuring all values, one DAG will created data_marker_pipeline we can trigger that DAG. This DAG will filter out all audio chunks on the basis of the given criteria's and it will pick audio chunks from source_directory_path . After filtration, audio chunks will be moved to landing_directory_path . Audio Transcription (with config): \u00b6 config: \u00b6 config: common: db_configuration: db_name: '' db_pass: '' db_user: '' cloud_sql_connection_name: '<DB host>' gcs_config: # master data bucket master_bucket: '<bucket name>' azure_transcription_client: speech_key: '<key of the api>' service_region: 'centralindia' # service region google_transcription_client: bucket: '<bucket name>' language: 'hi-IN' # It is BCP-47 language tag with this we call STT api. sample_rate: 16000 # Sample rate of audio utterance audio_channel_count: 1 #The number of channels in the input audio data audio_transcription_config: # defaults to hi-IN language: 'hi-IN' # language # audio_language it's used for sanitization rule whichever language you choose you need to add a rule class for the same. # You can use reference of hindi sanitization # sanitization rule eg: empty transcription, strip, char etc audio_language: 'kannada' # Bucket bath of wav file remote_clean_audio_file_path: '<bucketname>/data/audiotospeech/raw/landing/{language}/audio' # path where the processed files need to be uploaded remote_stt_audio_file_path: '<bucketname>/data/audiotospeech/integration/processed/{language}/audio' Steps to run: \u00b6 We have to configure sttsourcepath in airflow variable where our filtered audio chunks are stored. Other variable is sourceinfo in that we update our source(s) which we want to process through Speech-to-Text API (STT). count : Count of files that we want to process in one trigger. stt : STT API we want to use for transcription generation. We have support for Google & Azure STT API and you can add rapper as well for other API's. language : Language of source(s). data_set : Category of data \"train\" or \"test\". ex: \"sourceinfo\" : { \"<source_name>\" : { \"count\" : 5 , \"stt\" : \"google\" \"language\" : \"telugu\" , \"data_set\" : \"train\" }, \"<source2_name>\" : { \"count\" : 5 , \"stt\" : \"google\" \"language\" : \"telugu\" , \"data_set\" : \"train\" } } We have to also set audioidsforstt and with source(s) we want to run with empty array that will store audio_id. ex: \"audioidsforstt\" : { \"<source_name>\" : [], \"<source2_name>\" : [] } Also, configure integrationprocessedpath variable with the path of folder where we want move transcribed data. integrationprocessedpath : '' That will create a DAG with the source_name now we can trigger that DAG. And that will process given number(count) of audio chunks and upload processed files to remote_stt_audio_file_path that we mentioned in config file. Also, it will move raw data from remote_clean_audio_file_path to integrationprocessedpath and database will be updated with the metadata which we created using CircleCI. Contributing \u00b6 Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request We follow conventional commits License \u00b6 Distributed under the [MIT] License. See LICENSE for more information. Git Repository \u00b6 https://github.com/Open-Speech-EkStep/audio-to-speech-pipeline Contact \u00b6 Connect with community on Gitter","title":"Intelligent Data Pipeline"},{"location":"intelligent_data_pipelines/#intelligent-data-pipeline","text":"","title":"Intelligent Data Pipeline"},{"location":"intelligent_data_pipelines/#table-of-contents","text":"Intelligent Data Pipeline Table of Contents About The Project Getting Started Architecture Intelligent Data Pipeline - Jobs Audio Processor Audio Analysis Language identification Speaker identification Gender identification Audio Data Balancing Audio Validation Audio Transcription Installation Run on Kubernetes Using Composer Requirements Infra Setup CI/CD setup Audio Processing Config Description Config Steps to run Audio Analysis Config Config Steps to run Data Balancing Config config steps to run: Audio Transcription (with config): config: steps to run: Contributing License Git Repository Contact","title":"Table of Contents"},{"location":"intelligent_data_pipelines/#about-the-project","text":"Intelligent Data Pipelines are built to create the audio data set that can be used for Speech Recognition deeplearning models. The aim is to allow easy, quick and fast dataset generation without doing manual work. It splits data into smallers utterences which are understood well by deeplearning models. The data is then cleansed based on 'Signal to Noise ratio'. The audio analysis is performed using pre trained models and clustering based on audio features (see Resemblyzer for more details). It leverages Kubernetes for parallel computing and below are the metrics we have acheived so far: Some stats for a language with 1000 hrs raw data Raw data 1000 hrs Time taken: 2-3 days Final Usable Data of Pretraining: 600 Final Usable Data of Fine Tuning: 400","title":"About The Project"},{"location":"intelligent_data_pipelines/#getting-started","text":"The developer documentation helps you to get familiar with the bare necessities, giving you a quick and clean approach to get you up and running. If you are looking for ways to customize the workflow, or just breaking things down to build them back up, head to the reference section to dig into the mechanics of Data Pipelines. To get started install the prerequisites and clone the repo to machine on which you wish to run the framework. Here is the code","title":"Getting Started"},{"location":"intelligent_data_pipelines/#architecture","text":"","title":"Architecture"},{"location":"intelligent_data_pipelines/#intelligent-data-pipeline-jobs","text":"","title":"Intelligent Data Pipeline - Jobs"},{"location":"intelligent_data_pipelines/#audio-processor","text":"Audio Processor job takes raw data generted from Data Collection Pipeline or it also consumes data that is generated by any other means. It converts raw data file to wav file, then it splits wav file into smaller audio chunks. Then 'Signal to Noise Ratio (SNR)' is generated for each audio chunk and then filtration is performed based on the 'Signal to Noise Ratio (SNR)'. The threshold can be configured through configurations file. It then adds audio metadata to the catalogue (PostgresDB).","title":"Audio Processor"},{"location":"intelligent_data_pipelines/#audio-analysis","text":"Audio Analysis job takes processed and filtered audio chunks generated from Audio Processor job and performs three type of analysis:","title":"Audio Analysis"},{"location":"intelligent_data_pipelines/#language-identification","text":"It predicts the language spoken in each audio chunk using a pre-trained language identification model for a language. It gives the confidence score of the language for each audio chunk. Please see this for more details.","title":"Language identification"},{"location":"intelligent_data_pipelines/#speaker-clustering","text":"It estimates the total number of speakers in a particular source and maps speaker to audio chunks. That mapping of speaker to audio chunk is required for data balancing. Please see this for more details.","title":"Speaker Clustering"},{"location":"intelligent_data_pipelines/#gender-identification","text":"It predicts the speaker gender for each audio chunk using a pre-trained gender identification model. Please see this for more details.","title":"Gender identification"},{"location":"intelligent_data_pipelines/#audio-data-balancing","text":"The model training data requires data with proper gender ratio. Also, the data should be balanced based on speaker duration. It also provides capability to filter and choose data based on certain metadata filter criteria.","title":"Audio Data Balancing"},{"location":"intelligent_data_pipelines/#audio-validation","text":"The data that goes into model training should be of good quality. This job validates data that is not adhereing to quality standards required by the model. It generates csv reports that can be analysed by data scientists to further filter out the best data for model training.","title":"Audio Validation"},{"location":"intelligent_data_pipelines/#audio-transcription","text":"For model fine-tuning, the paired audio data is required (audio with labeled text). This job generates text for each audio chunk using Google or Azure API's. The texts generated are further sanitized based on the rules defined for the language.","title":"Audio Transcription"},{"location":"intelligent_data_pipelines/#installation","text":"Clone the repo git clone git@github.com:Open-Speech-EkStep/audio-to-speech-pipeline.git Install python requirements pip install -r requirements.txt","title":"Installation"},{"location":"intelligent_data_pipelines/#run-on-kubernetes","text":"","title":"Run on Kubernetes"},{"location":"intelligent_data_pipelines/#using-composer","text":"","title":"Using Composer"},{"location":"intelligent_data_pipelines/#requirements","text":"Terraform https://www.terraform.io/downloads.html gcloud https://cloud.google.com/sdk/docs/install kubectl https://kubernetes.io/docs/tasks/tools/install-kubectl-linux","title":"Requirements"},{"location":"intelligent_data_pipelines/#infra-setup","text":"Clone the repo: git clone https://github.com/Open-Speech-EkStep/ekstep-deep-speech-infra.git Initialize terraform modules terraform init Select a workspace as per the environments(dev,test,prod). terraform workspace select <env_name> eg: terraform workspace select prod Configure variable \"project\" { descrip t io n = \"The name of the Google Cloud Project.\" de fault = \"<project-name>\" } variable \"composer_env_name\" { descrip t io n = \"The name of the Google composer_env_name.\" de fault = \"ekstepcomposer\" } variable \"script_path\" { descrip t io n = \"The path of the working dir.\" de fault = \"./modules/gcp-composer/\" } variable \"bucket_name\" { descrip t io n = \"The name of the gcp bucket\" de fault = \"<bucket-name>\" } variable \"database_version\" { descrip t io n = \"The name of the database_version.\" t ype = s tr i n g de fault = \"POSTGRES_11\" } variable \"database_instance_name\" { descrip t io n = \"The name of the database_instance.\" t ype = s tr i n g de fault = \"<db-instance-name>\" } variable \"db_region\" { descrip t io n = \"The name of the db region.\" t ype = s tr i n g de fault = \"us-central1\" } variable \"database1\" { descrip t io n = \"The name of the database1.\" t ype = s tr i n g de fault = \"speech_recognition_data_catalog-1\" } variable \"database2\" { descrip t io n = \"The name of the database2.\" t ype = s tr i n g de fault = \"speech_recognition_data_catalog-2\" } variable \"speechrecognition_service_account\" { descrip t io n = \"The name of the speechrecognition_service_account.\" t ype = s tr i n g de fault = \"service-account-1\" } variable \"circleci_service_account\" { descrip t io n = \"The name of the circleci_service_account.\" t ype = s tr i n g de fault = \"servacct-circleci\" } variable \"sql_instance_size\" { de fault = \"db-custom-2-7680\" t ype = s tr i n g descrip t io n = \"Size of Cloud SQL instances\" } variable \"sql_disk_type\" { de fault = \"PD_HDD\" t ype = s tr i n g descrip t io n = \"Cloud SQL instance disk type\" } variable \"sql_disk_size\" { de fault = \"20\" t ype = s tr i n g descrip t io n = \"Storage size in GB\" } Create Service account: terraform apply -target=module.service-accounts Create keys from console.cloud.google.com Set env variable export GOOGLE_APPLICATION_CREDENTIAL_SERVICE_ACC= </path/to/key.json> Run specific modules as per requirements. terraform apply -target = module.<module-name> eg: terraform apply -target = module.sql-database Run all modules at once. terraform apply Connect to Database from local: Setup proxy ./cloud_sql_proxy -dir = ./cloudsql -instances = <project-id>:<zone>:<db-instance-name> = tcp:5432 Create username and password from console. Then connect to localhost Whitelist composer worker IP in Database Network.","title":"Infra Setup"},{"location":"intelligent_data_pipelines/#cicd-setup","text":"Once you pull code you have to configure some variable in your CircleCI . So that while deploying code image should easily push into google container registry. 1. GCP_PROJECT # Name of your GCP project 2. GOOGLE_AUTH # Service account key that is created using terraform 3. POSTGRES_DB # Database host ip that is created using terraform 4. POSTGRES_PASSWORD # Database password 5. POSTGRES_USER # Database user name 6. DB_INSTANCE # Database instance name","title":"CI/CD setup"},{"location":"intelligent_data_pipelines/#audio-processing-config","text":"","title":"Audio Processing Config"},{"location":"intelligent_data_pipelines/#description","text":"Create a yaml file using following config format and configure paths and other parameters.","title":"Description"},{"location":"intelligent_data_pipelines/#config","text":"config : common : db_configuration : db_name : '' db_pass : '' db_user : '' cloud_sql_connection_name : '<DB Host>' gcs_config : # master data bucket master_bucket : '<Name of the bucket>' audio_processor_config : # feat_language_identification should true if you want run language identification for a source feat_language_identification : False # language of the audio language : '' # path of the files on gcs which need to be processed # path eg: <bucket-name/data/audiotospeech/raw/download/downloaded/{language}/audio> remote_raw_audio_file_path : '' # after processing where we want to move raw data snr_done_folder_path : '' # <bucket-name/data/audiotospeech/raw/download/snr_done/{language}/audio> # path where the processed files need to be uploaded remote_processed_audio_file_path : '' # <bucket-name/data/audiotospeech/raw/download/catalogue/{language}/audio> # path where Duplicate files need to be uploaded based on checksum duplicate_audio_file_path : '' # <bucket-name/data/audiotospeech/raw/download/duplicate/{language}/audio> chunking_conversion_configuration : aggressiveness : '' # using for vad by default it's value is 2 the more the value that aggressive vad for chunking audio max_duration : '' # max duration is second if chunk is more than that vad will retry chunking with inc aggressiveness # SNR specific configurations snr_configuration : max_snr_threshold : '' # less than max_snr_threshold utterance will move to rejected folder. local_input_file_path : '' local_output_file_path : ''","title":"Config"},{"location":"intelligent_data_pipelines/#steps-to-run","text":"We have to configure sourcepathforsnr in airflow variable where our raw data is stored. Other variable that we need to configure is snrcatalogue in that we need to add our source(s) name which we want to process and following parameters: count: Count of files that we want to process in one trigger. format: The format of the raw audio file in bucket. language: Language of source(s). parallelism: Number of pods that will be up in one run. If parallelism is not defined then number of pod = count. ex:- \"snrcatalogue\" : { \"<source_name>\" : { \"count\" : 5 , \"format\" : \"mp3\" , \"language\" : \"telugu\" , \"parallelism\" : 2 }, \"<other_source_name>\" : { \"count\" : 5 , \"format\" : \"mp3\" , \"language\" : \"telugu\" , \"parallelism\" : 5 } } We have to also set audiofilelist with whatever source(s) we want to run with empty array that will store our file path ex:- \"audiofilelist\" : { \"<source_name>\" : [], \"<other_source_name>\" : [] } That will create a DAG with the source_name(s) now we can trigger that DAG, that will process given number(count) of file and upload processed file to remote_processed_audio_file_path that we mentioned in the config file. And move raw data from remote_raw_audio_file_path to snr_done_folder_path . Also, Database will be updated with the metadata which we created using CircleCI.","title":"Steps to run:"},{"location":"intelligent_data_pipelines/#audio-analysis-config","text":"","title":"Audio Analysis Config"},{"location":"intelligent_data_pipelines/#config_1","text":"audio_analysis_config : analysis_options : gender_analysis : 1 # It should be 1 if you want run gender analysis for a source else it should be 0. speaker_analysis : 0 # It should be 1 if you want run speaker analysis for a source else it should be 0. # path where the processed files need to be uploaded remote_processed_audio_file_path : '' # <bucket-name/data/audiotospeech/raw/download/catalogued/{language}/audio> # path where the embeddings need to be uploaded path_for_embeddings : '' # <bucket-name/data/audiotospeech/raw/download/catalogued/{language}/embeddings/> min_cluster_size : 4 # It is least number of cluster for one speaker. partial_set_size : 15000 # Number of audio chunks to create embeddings for a given source. fit_noise_on_similarity : 0.77 min_samples : 2","title":"Config"},{"location":"intelligent_data_pipelines/#steps-to-run_1","text":"We have to configure audio_analysis_config in airflow variable using this json, we have to mention source name, language, parallelism and batch size. \"audio_analysis_config\" : { \"<source name>\" : { \"language\" : \"hindi\" , \"format\" : \"wav\" , \"parallelism\" : 5 , \"batch_size\" : 5000 } } That will create a audio_analysis DAG with name source_name_audio_embedding_analysis . Now, we can trigger that DAG and that will process given sources. It will create embeddings, processed files and upload them to path_for_embeddings and remote_processed_audio_file_path respectively that we have mentioned in config file. Also, Database will be updated with the metadata which we created using CircleCI.","title":"Steps to run"},{"location":"intelligent_data_pipelines/#data-balancing-config","text":"","title":"Data Balancing Config"},{"location":"intelligent_data_pipelines/#config_2","text":"data_tagger_config : # path of to the folder in the master bucket where the data tagger will move the data to landing_directory_path : '' #'<bucket_name>/data/audiotospeech/raw/download/catalogued/{language}/audio' # path of to the folder in the master bucket from where the data tagger will pick up the data that needs to be moved source_directory_path : '' #'<bucket_name>/data/audiotospeech/raw/landing/{language}/audio'","title":"config"},{"location":"intelligent_data_pipelines/#steps-to-run_2","text":"We need to configure data_filter_config airflow variable for each source. We provide 2 modes of filtration file mode and filter mode . Only, one mode can be used at a time. To use filter mode, \"file_mode\": \"n\" . If \"file_mode\": \"y\" , then snr filter, duration filter etc. won't work. data_set : select data set type from 'train' and 'test'. file_mode : It should be 'y' if you want to use file_mode for a source else it should be 'n'. This mode can be used when we need to filter out some specific files that we found after analysis by providing path of the CSV file in file_path parameter. file_path : path of the CSV file We have multiple filters: by_snr : filter based on SNR value. \"lte\" means lower than and \"gte\" means greater than by_duration :total duration from a given source. by_speaker : we can configure how much data per speaker we want. by_utterance_duration : we can required duration of utterance. exclude_audio_ids : we can pass a list of audio_ids that we want to skip. exclude_speaker_ids : we can pass a list of speaker_ids that we want to skip. with_randomness : It is a boolean value if it's true it will pickup random data from DB. \"data_filter_config\" : { \"test_source1\" : { \"language\" : \"hindi\" , \"file_mode\" : \"n\" , \"data_set\" : \"train\" , \"file_path\" : \"data/audiotospeech/raw/download/duplicate/test_source1.csv\" , \"filter\" : { \"by_snr\" : { \"lte\" : 75 , \"gte\" : 15 }, \"by_duration\" : 2 , \"with_randomness\" : \"true\" } }, \"test_source2\" : { \"language\" : \"hindi\" , \"file_mode\" : \"n\" , \"data_set\" : \"train\" , \"file_path\" : \"data/audiotospeech/raw/download/duplicate/test_source2.csv\" , \"filter\" : { \"by_speaker\" : { \"lte_per_speaker_duration\" : 60 , \"gte_per_speaker_duration\" : 0 , \"with_threshold\" : 0 }, \"by_duration\" : 2 } } } After configuring all values, one DAG will created data_marker_pipeline we can trigger that DAG. This DAG will filter out all audio chunks on the basis of the given criteria's and it will pick audio chunks from source_directory_path . After filtration, audio chunks will be moved to landing_directory_path .","title":"Steps to run:"},{"location":"intelligent_data_pipelines/#audio-transcription-with-config","text":"","title":"Audio Transcription (with config):"},{"location":"intelligent_data_pipelines/#config_3","text":"config: common: db_configuration: db_name: '' db_pass: '' db_user: '' cloud_sql_connection_name: '<DB host>' gcs_config: # master data bucket master_bucket: '<bucket name>' azure_transcription_client: speech_key: '<key of the api>' service_region: 'centralindia' # service region google_transcription_client: bucket: '<bucket name>' language: 'hi-IN' # It is BCP-47 language tag with this we call STT api. sample_rate: 16000 # Sample rate of audio utterance audio_channel_count: 1 #The number of channels in the input audio data audio_transcription_config: # defaults to hi-IN language: 'hi-IN' # language # audio_language it's used for sanitization rule whichever language you choose you need to add a rule class for the same. # You can use reference of hindi sanitization # sanitization rule eg: empty transcription, strip, char etc audio_language: 'kannada' # Bucket bath of wav file remote_clean_audio_file_path: '<bucketname>/data/audiotospeech/raw/landing/{language}/audio' # path where the processed files need to be uploaded remote_stt_audio_file_path: '<bucketname>/data/audiotospeech/integration/processed/{language}/audio'","title":"config:"},{"location":"intelligent_data_pipelines/#steps-to-run_3","text":"We have to configure sttsourcepath in airflow variable where our filtered audio chunks are stored. Other variable is sourceinfo in that we update our source(s) which we want to process through Speech-to-Text API (STT). count : Count of files that we want to process in one trigger. stt : STT API we want to use for transcription generation. We have support for Google & Azure STT API and you can add rapper as well for other API's. language : Language of source(s). data_set : Category of data \"train\" or \"test\". ex: \"sourceinfo\" : { \"<source_name>\" : { \"count\" : 5 , \"stt\" : \"google\" \"language\" : \"telugu\" , \"data_set\" : \"train\" }, \"<source2_name>\" : { \"count\" : 5 , \"stt\" : \"google\" \"language\" : \"telugu\" , \"data_set\" : \"train\" } } We have to also set audioidsforstt and with source(s) we want to run with empty array that will store audio_id. ex: \"audioidsforstt\" : { \"<source_name>\" : [], \"<source2_name>\" : [] } Also, configure integrationprocessedpath variable with the path of folder where we want move transcribed data. integrationprocessedpath : '' That will create a DAG with the source_name now we can trigger that DAG. And that will process given number(count) of audio chunks and upload processed files to remote_stt_audio_file_path that we mentioned in config file. Also, it will move raw data from remote_clean_audio_file_path to integrationprocessedpath and database will be updated with the metadata which we created using CircleCI.","title":"Steps to run:"},{"location":"intelligent_data_pipelines/#contributing","text":"Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request We follow conventional commits","title":"Contributing"},{"location":"intelligent_data_pipelines/#license","text":"Distributed under the [MIT] License. See LICENSE for more information.","title":"License"},{"location":"intelligent_data_pipelines/#git-repository","text":"https://github.com/Open-Speech-EkStep/audio-to-speech-pipeline","title":"Git Repository"},{"location":"intelligent_data_pipelines/#contact","text":"Connect with community on Gitter","title":"Contact"},{"location":"language_identification/","text":"Language Identification \u00b6 Table of Contents \u00b6 Language Identification Table of Contents About The Project Working Training Train data Results Usage Preparing the Data Training the Model Inference About The Project \u00b6 Language Identification works for classifying the audio utterances into different classes. This repository can work for 2 or more classes depending on the requirement. This utility is a part of our Intelligent Data Pipelines. For all the data crawled, we can't always be a 100% sure that it only belongs to the language we want to train our Speech Recognition model on. Thus we train a language identification model on a different, limited set of data, to aid us in finding (or eliminating) files that have high content of foreign language. Since we also pass the datasets used for training LID through our pipelines, we can assume that VAD has split all original audios into small chunks with minimum overlap between languages, for training.Another assumption is that the training data comes from a wide variety of environments. We train a ResNet18 model on the spectrograms extracted from audio utterances. During inference, we return a confidence score value for each class (provided by user during training) for an input audio. Working \u00b6 Training \u00b6 We extract spectrograms from audio utterances and use them to train a CNN based classifier: ResNet18, which is 18 layers deep. We don't use the pretrained version, and treat Language Identification as an image classification problem. Users can decide upon the number of classes they want to train on. We follow a one vs other approach where main language is the one we want to classify against. Outputs for each audio are confidence scores for different labels. Train data \u00b6 We trained a Tamil vs others classification model, with 30-40 hours data for each of the classes- Tamil and Other. Validation data consisted of roughly 14 hours of data, balanced between the classes. Usage \u00b6 Preparing the Data \u00b6 Keep separate audio folders for different classes as well as the train and valid sets of each. The audio files should be present in .wav format. To prepare the data, edit the data paths in file data/create_manifest.py. To run the file: python create_manifest . py This creates the train and valid csv files in the data/ directory. Training the Model \u00b6 Edit the train_config.yml file for the training parameters. Give the file path for train and valid csv's created while preparing the data. To start the training, run: python train . py Inference \u00b6 Edit the language_map.yml to map the labels(0,1, etc) with the languege names or codes('hi','en', etc) To infer, edit inference.py file and provide the best_checkpoint path and audio file name. Parameters: model_path : Path to best_checkpoint.pt audio_path : Audio file path Run the file: python inference . py This runs on a single audio file.","title":"Language Identification"},{"location":"language_identification/#language-identification","text":"","title":"Language Identification"},{"location":"language_identification/#table-of-contents","text":"Language Identification Table of Contents About The Project Working Training Train data Results Usage Preparing the Data Training the Model Inference","title":"Table of Contents"},{"location":"language_identification/#about-the-project","text":"Language Identification works for classifying the audio utterances into different classes. This repository can work for 2 or more classes depending on the requirement. This utility is a part of our Intelligent Data Pipelines. For all the data crawled, we can't always be a 100% sure that it only belongs to the language we want to train our Speech Recognition model on. Thus we train a language identification model on a different, limited set of data, to aid us in finding (or eliminating) files that have high content of foreign language. Since we also pass the datasets used for training LID through our pipelines, we can assume that VAD has split all original audios into small chunks with minimum overlap between languages, for training.Another assumption is that the training data comes from a wide variety of environments. We train a ResNet18 model on the spectrograms extracted from audio utterances. During inference, we return a confidence score value for each class (provided by user during training) for an input audio.","title":"About The Project"},{"location":"language_identification/#working","text":"","title":"Working"},{"location":"language_identification/#training","text":"We extract spectrograms from audio utterances and use them to train a CNN based classifier: ResNet18, which is 18 layers deep. We don't use the pretrained version, and treat Language Identification as an image classification problem. Users can decide upon the number of classes they want to train on. We follow a one vs other approach where main language is the one we want to classify against. Outputs for each audio are confidence scores for different labels.","title":"Training"},{"location":"language_identification/#train-data","text":"We trained a Tamil vs others classification model, with 30-40 hours data for each of the classes- Tamil and Other. Validation data consisted of roughly 14 hours of data, balanced between the classes.","title":"Train data"},{"location":"language_identification/#usage","text":"","title":"Usage"},{"location":"language_identification/#preparing-the-data","text":"Keep separate audio folders for different classes as well as the train and valid sets of each. The audio files should be present in .wav format. To prepare the data, edit the data paths in file data/create_manifest.py. To run the file: python create_manifest . py This creates the train and valid csv files in the data/ directory.","title":"Preparing the Data"},{"location":"language_identification/#training-the-model","text":"Edit the train_config.yml file for the training parameters. Give the file path for train and valid csv's created while preparing the data. To start the training, run: python train . py","title":"Training the Model"},{"location":"language_identification/#inference","text":"Edit the language_map.yml to map the labels(0,1, etc) with the languege names or codes('hi','en', etc) To infer, edit inference.py file and provide the best_checkpoint path and audio file name. Parameters: model_path : Path to best_checkpoint.pt audio_path : Audio file path Run the file: python inference . py This runs on a single audio file.","title":"Inference"},{"location":"model_training/","text":"Pretrained Models \u00b6 We are releasing pretrained models in various Indic Languages. Please head over to this repo . Table of contents \u00b6 * Installation and Setup * Directory Structure * Data Description * Usage * For Pretraining * For Finetuning * For Inference * For Single File Inference * License Installation and Setup \u00b6 git clone https://github.com/Open-Speech-EkStep/vakyansh-wav2vec2-experimentation.git conda create --name <env_name> python=3.7 conda activate <env_name> cd vakyansh-wav2vec2-experimentation ### Packages pip install packaging soundfile swifter pip install -r requirements.txt pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html ### For fairseq setup(fairseq should be installed outside vakyansh-wav2vec2-experimentation repo) cd .. git clone -b ekstep-wav2vec2 https://github.com/Open-Speech-EkStep/fairseq.git cd fairseq pip install -e . ### install other libraries ### For Kenlm, openblas cd .. sudo apt-get install liblzma-dev libbz2-dev libzstd-dev libsndfile1-dev libopenblas-dev libfftw3-dev libgflags-dev libgoogle-glog-dev sudo apt install build-essential cmake libboost-system-dev libboost-thread-dev libboost-program-options-dev libboost-test-dev libeigen3-dev zlib1g-dev libbz2-dev liblzma-dev git clone https://github.com/kpu/kenlm.git cd kenlm mkdir -p build && cd build cmake .. make -j 16 cd .. export KENLM_ROOT_DIR=$PWD export USE_CUDA=0 ## for cpu cd .. ### wav2letter git clone -b v0.2 https://github.com/facebookresearch/wav2letter.git cd wav2letter git checkout b1d1f89f586120a978a4666cffd45c55f0a2e564 cd bindings/python pip install -e . Directory Structure \u00b6 root-directory . |-- ./checkpoints | |-- ./checkpoints/custom_model | | `-- ./checkpoints/custom_model/ | |-- ./checkpoints/finetuning | | `-- ./checkpoints/finetuning/ | `-- ./checkpoints/pretraining | `-- ./checkpoints/pretraining/ |-- ./data | |-- ./data/finetuning | | `-- ./data/finetuning/ | |-- ./data/inference | | `-- ./data/inference/ | |-- ./data/pretraining | | `-- ./data/pretraining/ | `-- ./data/processed | `-- ./data/processed/ |-- ./lm | `-- ./lm/ |-- ./logs | |-- ./logs/finetuning | | `-- ./logs/finetuning/ | `-- ./logs/pretraining | `-- ./logs/pretraining/ |-- ./notebooks | `-- ./notebooks/ |-- ./results | `-- ./results/ |-- ./scripts | |-- ./scripts/data | | `-- ./scripts/data/ | |-- ./scripts/parse_yaml.sh | |-- ./scripts/finetuning | | |-- ./scripts/finetuning/start_finetuning.sh | | |-- ./scripts/finetuning/prepare_data.sh | | `-- ./scripts/finetuning/README.md | |-- ./scripts/lm | | |-- ./scripts/lm/run_lm_pipeline.sh | | `-- ./scripts/lm/README.md | |-- ./scripts/pretraining | | |-- ./scripts/pretraining/start_pretraining_base.sh | | |-- ./scripts/pretraining/start_pretraining_large.sh | | |-- ./scripts/pretraining/prepare_data.sh | | `-- ./scripts/pretraining/README.md | `-- ./scripts/inference | |-- ./scripts/inference/infer.sh | |-- ./scripts/inference/prepare_data.sh | |-- ./scripts/inference/generate_custom_model.sh | |-- ./scripts/inference/single_file_inference.sh | `-- ./scripts/inference/README.md |-- ./config | |-- ./config/finetuning.yaml | |-- ./config/pretraining_base.yaml | |-- ./config/pretraining_large.yaml | `-- ./config/README.md |-- ./requirements.txt |-- ./utils | |-- ./utils/analysis | | `-- ./utils/analysis/generate_wav_report_from_tsv.py | |-- ./utils/prep_scripts | | |-- ./utils/prep_scripts/dict_and_lexicon_maker.py | | |-- ./utils/prep_scripts/labels.py | | `-- ./utils/prep_scripts/manifest.py | |-- ./utils/wer | | |-- ./utils/wer/wer.py | | `-- ./utils/wer/wer_wav2vec.py | |-- ./utils/inference | | |-- ./utils/inference/generate_custom_model.py | | `-- ./utils/inference/single_file_inference.py | `-- ./utils/lm | |-- ./utils/lm/concatenate_text.py | |-- ./utils/lm/make_lexicon_lst.py | |-- ./utils/lm/generate_lm.py | |-- ./utils/lm/clean_text.py | `-- ./utils/lm/remove_duplicate_lines.py `-- ./README.md Data Description \u00b6 For Audio Files. Sample Rate [Hz] = 16000 Channels = 'mono' Bit Rate [kbit/s] = 256 Precision [bits] = 16 Audio length should be less than 30 seconds otherwise it will be ignored during data preparation After scripts/finetuning/prepare_data.sh is run, analysis will be generated which can be used to tune min/max_sample_size in the config files For Text Files Corresponding text file of each audio file must be on the same directory as its audio Text file should not contain any punctuation characters Check dict.ltr.txt file generated after prepare_data so that it does not contain any foreign language character For Language Model Character set of text used for language model should be same as character set used for training Sample code for cleaning text file for english language is given here clean_text.py Sample code for removing duplicate line from text file is given here remove_duplicate_lines.py Usage \u00b6 For Pretraining \u00b6 Edit the path to data in the scripts/pretraining/prepare_data.sh file. To prepare the data: $ cd scripts/pretraining $ bash prepare_data.sh Edit the config/pretraining_base.yaml or config/pretraining_large.yaml for different parameter configurations.Check the required paths and values in start_pretraining_base.sh or start_pretraining_large.sh. Refer to config README To start run: $ bash start_pretraining_base.sh Refer this for pretraining parameters. For Finetuning \u00b6 Edit the path to data in the scripts/finetuning/prepare_data.sh file. To prepare the data: $ cd scripts/finetuning $ bash prepare_data.sh Edit the config/finetuning.yaml for different parameter configurations.Check the required paths and values in start_finetuning.sh. Refer to config README To start run: $ bash start_finetuning.sh Refer this for finetuning parameters. For Inference \u00b6 Edit the path to data in the scripts/inference/prepare_data.sh file. To prepare the test data run: $ cd scripts/inference/ $ bash prepare_data.sh Edit the infer.sh file for required paths. To start inference run: $ bash infer.sh Refer this for inference parameters. For Single File Inference \u00b6 To generate custom model, run: $ cd scripts/inference $ bash generate_custom_model.sh To infer for single file, change path in single_file_inference.sh. Then run: $ bash single_file_inference.sh For generating LM \u00b6 Edit the run_lm_pipeline.sh variables as required, then run: $ cd scripts/lm $ bash run_lm_pipeline.sh Refer this for LM pipeline. License \u00b6 fairseq(-py) is MIT-licensed. The license applies to the pre-trained models as well.","title":"Model Training Pipeline"},{"location":"model_training/#pretrained-models","text":"We are releasing pretrained models in various Indic Languages. Please head over to this repo .","title":"Pretrained Models"},{"location":"model_training/#table-of-contents","text":"* Installation and Setup * Directory Structure * Data Description * Usage * For Pretraining * For Finetuning * For Inference * For Single File Inference * License","title":"Table of contents"},{"location":"model_training/#installation-and-setup","text":"git clone https://github.com/Open-Speech-EkStep/vakyansh-wav2vec2-experimentation.git conda create --name <env_name> python=3.7 conda activate <env_name> cd vakyansh-wav2vec2-experimentation ### Packages pip install packaging soundfile swifter pip install -r requirements.txt pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html ### For fairseq setup(fairseq should be installed outside vakyansh-wav2vec2-experimentation repo) cd .. git clone -b ekstep-wav2vec2 https://github.com/Open-Speech-EkStep/fairseq.git cd fairseq pip install -e . ### install other libraries ### For Kenlm, openblas cd .. sudo apt-get install liblzma-dev libbz2-dev libzstd-dev libsndfile1-dev libopenblas-dev libfftw3-dev libgflags-dev libgoogle-glog-dev sudo apt install build-essential cmake libboost-system-dev libboost-thread-dev libboost-program-options-dev libboost-test-dev libeigen3-dev zlib1g-dev libbz2-dev liblzma-dev git clone https://github.com/kpu/kenlm.git cd kenlm mkdir -p build && cd build cmake .. make -j 16 cd .. export KENLM_ROOT_DIR=$PWD export USE_CUDA=0 ## for cpu cd .. ### wav2letter git clone -b v0.2 https://github.com/facebookresearch/wav2letter.git cd wav2letter git checkout b1d1f89f586120a978a4666cffd45c55f0a2e564 cd bindings/python pip install -e .","title":"Installation and Setup"},{"location":"model_training/#directory-structure","text":"root-directory . |-- ./checkpoints | |-- ./checkpoints/custom_model | | `-- ./checkpoints/custom_model/ | |-- ./checkpoints/finetuning | | `-- ./checkpoints/finetuning/ | `-- ./checkpoints/pretraining | `-- ./checkpoints/pretraining/ |-- ./data | |-- ./data/finetuning | | `-- ./data/finetuning/ | |-- ./data/inference | | `-- ./data/inference/ | |-- ./data/pretraining | | `-- ./data/pretraining/ | `-- ./data/processed | `-- ./data/processed/ |-- ./lm | `-- ./lm/ |-- ./logs | |-- ./logs/finetuning | | `-- ./logs/finetuning/ | `-- ./logs/pretraining | `-- ./logs/pretraining/ |-- ./notebooks | `-- ./notebooks/ |-- ./results | `-- ./results/ |-- ./scripts | |-- ./scripts/data | | `-- ./scripts/data/ | |-- ./scripts/parse_yaml.sh | |-- ./scripts/finetuning | | |-- ./scripts/finetuning/start_finetuning.sh | | |-- ./scripts/finetuning/prepare_data.sh | | `-- ./scripts/finetuning/README.md | |-- ./scripts/lm | | |-- ./scripts/lm/run_lm_pipeline.sh | | `-- ./scripts/lm/README.md | |-- ./scripts/pretraining | | |-- ./scripts/pretraining/start_pretraining_base.sh | | |-- ./scripts/pretraining/start_pretraining_large.sh | | |-- ./scripts/pretraining/prepare_data.sh | | `-- ./scripts/pretraining/README.md | `-- ./scripts/inference | |-- ./scripts/inference/infer.sh | |-- ./scripts/inference/prepare_data.sh | |-- ./scripts/inference/generate_custom_model.sh | |-- ./scripts/inference/single_file_inference.sh | `-- ./scripts/inference/README.md |-- ./config | |-- ./config/finetuning.yaml | |-- ./config/pretraining_base.yaml | |-- ./config/pretraining_large.yaml | `-- ./config/README.md |-- ./requirements.txt |-- ./utils | |-- ./utils/analysis | | `-- ./utils/analysis/generate_wav_report_from_tsv.py | |-- ./utils/prep_scripts | | |-- ./utils/prep_scripts/dict_and_lexicon_maker.py | | |-- ./utils/prep_scripts/labels.py | | `-- ./utils/prep_scripts/manifest.py | |-- ./utils/wer | | |-- ./utils/wer/wer.py | | `-- ./utils/wer/wer_wav2vec.py | |-- ./utils/inference | | |-- ./utils/inference/generate_custom_model.py | | `-- ./utils/inference/single_file_inference.py | `-- ./utils/lm | |-- ./utils/lm/concatenate_text.py | |-- ./utils/lm/make_lexicon_lst.py | |-- ./utils/lm/generate_lm.py | |-- ./utils/lm/clean_text.py | `-- ./utils/lm/remove_duplicate_lines.py `-- ./README.md","title":"Directory Structure"},{"location":"model_training/#data-description","text":"For Audio Files. Sample Rate [Hz] = 16000 Channels = 'mono' Bit Rate [kbit/s] = 256 Precision [bits] = 16 Audio length should be less than 30 seconds otherwise it will be ignored during data preparation After scripts/finetuning/prepare_data.sh is run, analysis will be generated which can be used to tune min/max_sample_size in the config files For Text Files Corresponding text file of each audio file must be on the same directory as its audio Text file should not contain any punctuation characters Check dict.ltr.txt file generated after prepare_data so that it does not contain any foreign language character For Language Model Character set of text used for language model should be same as character set used for training Sample code for cleaning text file for english language is given here clean_text.py Sample code for removing duplicate line from text file is given here remove_duplicate_lines.py","title":"Data Description"},{"location":"model_training/#usage","text":"","title":"Usage"},{"location":"model_training/#for-pretraining","text":"Edit the path to data in the scripts/pretraining/prepare_data.sh file. To prepare the data: $ cd scripts/pretraining $ bash prepare_data.sh Edit the config/pretraining_base.yaml or config/pretraining_large.yaml for different parameter configurations.Check the required paths and values in start_pretraining_base.sh or start_pretraining_large.sh. Refer to config README To start run: $ bash start_pretraining_base.sh Refer this for pretraining parameters.","title":"For Pretraining"},{"location":"model_training/#for-finetuning","text":"Edit the path to data in the scripts/finetuning/prepare_data.sh file. To prepare the data: $ cd scripts/finetuning $ bash prepare_data.sh Edit the config/finetuning.yaml for different parameter configurations.Check the required paths and values in start_finetuning.sh. Refer to config README To start run: $ bash start_finetuning.sh Refer this for finetuning parameters.","title":"For Finetuning"},{"location":"model_training/#for-inference","text":"Edit the path to data in the scripts/inference/prepare_data.sh file. To prepare the test data run: $ cd scripts/inference/ $ bash prepare_data.sh Edit the infer.sh file for required paths. To start inference run: $ bash infer.sh Refer this for inference parameters.","title":"For Inference"},{"location":"model_training/#for-single-file-inference","text":"To generate custom model, run: $ cd scripts/inference $ bash generate_custom_model.sh To infer for single file, change path in single_file_inference.sh. Then run: $ bash single_file_inference.sh","title":"For Single File Inference"},{"location":"model_training/#for-generating-lm","text":"Edit the run_lm_pipeline.sh variables as required, then run: $ cd scripts/lm $ bash run_lm_pipeline.sh Refer this for LM pipeline.","title":"For generating LM"},{"location":"model_training/#license","text":"fairseq(-py) is MIT-licensed. The license applies to the pre-trained models as well.","title":"License"},{"location":"speaker_clustering/","text":"Speaker Clustering \u00b6 Table of Contents \u00b6 Speaker Clustering Table of Contents About The Project Working Embeddings Clustering algorithm Repetitive merging Splitting Fitting Noise points Hyperparameters About The Project \u00b6 Speaker Clustering, or identification of speakers in the wild is mainly useful for audio sources with no mapping between audios and a speaker label/name. It is the task of identifying the unique speakers in a set of audio recordings (each belonging to exactly one speaker) without knowing who and how many speakers are present in the entire data. Our intelligent data pipelines split each audio, based on Voice Activity Detection, as one of the starting steps. These shorter audio utterances are then used to train deep learning models. We assume that the utterances are short enough to have one speaker per utterance, since the splitting logic is using unvoiced segments as points to split an audio. Working \u00b6 This documentation will help you understand the various steps involved and take you through the hyperparameters, so you can tune them to achieve the best possible results. Embeddings \u00b6 We use the Voice Encoder model proposed here , and implemented here for converting our audio utterances into fixed length embeddings. System overview for training Voice encoder. Different colours indicate utterances/embeddings from different speakers. Voice Encoder is a speaker-discriminative model trained on a text-independent speaker verification task. Thus it allows us to derive a high-level representation of the voice present in an audio. An embedding is a 256 dimensional vector capable of summarizing the characteristics of the voice spoken. The data used to train the model contained 1.8k speakers from LibriSpeech-other, Voxceleb, Vox celeb2; making a final of more than 1000 hrs of data in English. Since our experiment sources for audios were in Hindi, we did a small experiment to determine whether embeddings on Hindi data using this pretrained model were able to separate speakers. The resulting dist plot is presented below. x-axis is the Cosine Similarity. A distplot showing separation between embeddings belonging to different speakers, based on Cosine similarity. Clustering algorithm \u00b6 Embeddings for a source are passed as a matrix for clustering in this step. If the number of embeddings are greater than the parameter partial_set_size , they are divided into multiple partial sets of this size. This step is done to reduce the computational cost of calculating cosine distances and other matrix operations during clustering. We use Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) as our core clustering algorithm on each of these partial sets. This step also classifies some points as noise points - meaning they couldn't be used up in any clusters formed for this partial set. We keep a record of all these noise points for fitting later. Repetitive merging \u00b6 We found in our experiments that some of the speakers had their clusters distributed as separate ones - even in one partial set. This step helps in allowing such clusters to merge. Merging is based on cosine similarity (94-96% similar clusters are merged repetitively). Also, clusters for the same speaker but from different partial sets also get merged in this step. Initial EOM clustering and repetitive merging of clusters. Splitting \u00b6 For some sources in our experiments, big clusters usually contained very high diversity of speakers. Splitting is done on such \"bigger\" clusters to make sure we have high cluster purity. A cluster is called big if it has more than 3 times the average number of points across all clusters. We also tried splitting a large cluster containing audios from one speaker only and it was retained as is, meaning this step should not affect large clusters belonging to one speaker only. Cluster splitting is achieved by using Leaf HDBSCAN clustering on the big clusters. This allows for more more fine grained clustering. More details on this follow in the next section. Repetitive merging is applied again after splitting to allow clusters with high cosine simialrities to be merged again, if any. Splitting big clusters and repetitive merging of clusters. Fitting Noise points \u00b6 All the noise points - points which could not be put into a cluster, are allowed to merge with a clusters if they have a cosine similairy of >= 80% with a noise point. This parameter is also configurable. Hyperparameters \u00b6 min_cluster_size: the smallest size grouping that you wish to consider a cluster min_cluster_size can be increased if the source is expected to have large number of utterances for a less number of speakers. min_samples: number of points required in the neighborhood of a point to be considered a core point smallest value = 1, max value = min_cluster_size smaller values of min_samples have an effect of lowering the number of noise points in our experiments, around 30% of points across a source were being classified as noise before fitting. You can try with min_samples=1 for least possible noise points classification. partial_set_size: number of utterances to treat as one set for clustering. We used partial_set_size =11122 as this number represents around 20 hours of data on an average for us, and was computationally less demanding. fit_noise_on_similarity: cosine similarity between a noise point and a cluster, at which the point can be fit to a cluster. default = 0.80, meaning points with cosine similarity >=0.80 with a cluster's mean embedding will be fit to that cluster. values of fit_noise_on_similarity lesser than 0.80 can have an effect of decresing cluster purity. cluster_selection_method: can either be \u2018eom\u2019 or \u2018leaf\u2019 'eom' or Excess of Mass is the default way of HDBSCAN's working. 'leaf' will select leaf nodes from the tree, producing many small homogeneous clusters. Allowing for fine-grained clusters. This is used while splitting clusters into smaller ones.","title":"Speaker Clustering"},{"location":"speaker_clustering/#speaker-clustering","text":"","title":"Speaker Clustering"},{"location":"speaker_clustering/#table-of-contents","text":"Speaker Clustering Table of Contents About The Project Working Embeddings Clustering algorithm Repetitive merging Splitting Fitting Noise points Hyperparameters","title":"Table of Contents"},{"location":"speaker_clustering/#about-the-project","text":"Speaker Clustering, or identification of speakers in the wild is mainly useful for audio sources with no mapping between audios and a speaker label/name. It is the task of identifying the unique speakers in a set of audio recordings (each belonging to exactly one speaker) without knowing who and how many speakers are present in the entire data. Our intelligent data pipelines split each audio, based on Voice Activity Detection, as one of the starting steps. These shorter audio utterances are then used to train deep learning models. We assume that the utterances are short enough to have one speaker per utterance, since the splitting logic is using unvoiced segments as points to split an audio.","title":"About The Project"},{"location":"speaker_clustering/#working","text":"This documentation will help you understand the various steps involved and take you through the hyperparameters, so you can tune them to achieve the best possible results.","title":"Working"},{"location":"speaker_clustering/#embeddings","text":"We use the Voice Encoder model proposed here , and implemented here for converting our audio utterances into fixed length embeddings. System overview for training Voice encoder. Different colours indicate utterances/embeddings from different speakers. Voice Encoder is a speaker-discriminative model trained on a text-independent speaker verification task. Thus it allows us to derive a high-level representation of the voice present in an audio. An embedding is a 256 dimensional vector capable of summarizing the characteristics of the voice spoken. The data used to train the model contained 1.8k speakers from LibriSpeech-other, Voxceleb, Vox celeb2; making a final of more than 1000 hrs of data in English. Since our experiment sources for audios were in Hindi, we did a small experiment to determine whether embeddings on Hindi data using this pretrained model were able to separate speakers. The resulting dist plot is presented below. x-axis is the Cosine Similarity. A distplot showing separation between embeddings belonging to different speakers, based on Cosine similarity.","title":"Embeddings"},{"location":"speaker_clustering/#clustering-algorithm","text":"Embeddings for a source are passed as a matrix for clustering in this step. If the number of embeddings are greater than the parameter partial_set_size , they are divided into multiple partial sets of this size. This step is done to reduce the computational cost of calculating cosine distances and other matrix operations during clustering. We use Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) as our core clustering algorithm on each of these partial sets. This step also classifies some points as noise points - meaning they couldn't be used up in any clusters formed for this partial set. We keep a record of all these noise points for fitting later.","title":"Clustering algorithm"},{"location":"speaker_clustering/#repetitive-merging","text":"We found in our experiments that some of the speakers had their clusters distributed as separate ones - even in one partial set. This step helps in allowing such clusters to merge. Merging is based on cosine similarity (94-96% similar clusters are merged repetitively). Also, clusters for the same speaker but from different partial sets also get merged in this step. Initial EOM clustering and repetitive merging of clusters.","title":"Repetitive merging"},{"location":"speaker_clustering/#splitting","text":"For some sources in our experiments, big clusters usually contained very high diversity of speakers. Splitting is done on such \"bigger\" clusters to make sure we have high cluster purity. A cluster is called big if it has more than 3 times the average number of points across all clusters. We also tried splitting a large cluster containing audios from one speaker only and it was retained as is, meaning this step should not affect large clusters belonging to one speaker only. Cluster splitting is achieved by using Leaf HDBSCAN clustering on the big clusters. This allows for more more fine grained clustering. More details on this follow in the next section. Repetitive merging is applied again after splitting to allow clusters with high cosine simialrities to be merged again, if any. Splitting big clusters and repetitive merging of clusters.","title":"Splitting"},{"location":"speaker_clustering/#fitting-noise-points","text":"All the noise points - points which could not be put into a cluster, are allowed to merge with a clusters if they have a cosine similairy of >= 80% with a noise point. This parameter is also configurable.","title":"Fitting Noise points"},{"location":"speaker_clustering/#hyperparameters","text":"min_cluster_size: the smallest size grouping that you wish to consider a cluster min_cluster_size can be increased if the source is expected to have large number of utterances for a less number of speakers. min_samples: number of points required in the neighborhood of a point to be considered a core point smallest value = 1, max value = min_cluster_size smaller values of min_samples have an effect of lowering the number of noise points in our experiments, around 30% of points across a source were being classified as noise before fitting. You can try with min_samples=1 for least possible noise points classification. partial_set_size: number of utterances to treat as one set for clustering. We used partial_set_size =11122 as this number represents around 20 hours of data on an average for us, and was computationally less demanding. fit_noise_on_similarity: cosine similarity between a noise point and a cluster, at which the point can be fit to a cluster. default = 0.80, meaning points with cosine similarity >=0.80 with a cluster's mean embedding will be fit to that cluster. values of fit_noise_on_similarity lesser than 0.80 can have an effect of decresing cluster purity. cluster_selection_method: can either be \u2018eom\u2019 or \u2018leaf\u2019 'eom' or Excess of Mass is the default way of HDBSCAN's working. 'leaf' will select leaf nodes from the tree, producing many small homogeneous clusters. Allowing for fine-grained clusters. This is used while splitting clusters into smaller ones.","title":"Hyperparameters"},{"location":"tts_model_api/","text":"Text to Speech model API \u00b6 About The Project \u00b6 Text To Speech (TTS), also known as Speech Synthesis, is a process where text is converted into a human-sounding voice. TTS has been a popular choice for developers and business users alike when building IVR (Interactive Voice Response) solutions and other voice applications, as it accelerates time to production without having to record audio files with human voices. Using recorded files requires recording each message with a human voice, whereas TTS prompts can be dynamically generated from raw text. Our TTS service can enable us to generate life-like speech synthesis in both male and female voices for an array of Indic languages like Hindi, Tamil,Malayalam, Kannada and many more. API enable us to provide the following features: Support for Indic only languages. No software Installation required. Cloud based robust API. Two gender voices for every language. Fast and easy integration. Cross-platform capability. The Developer documentation provides you with a complete set of guidelines which you need to get started with: Architecture overview API reference Client Code reference Setup and getting started guide Extend this project Contribute to this project Architecture Overview \u00b6 We built this REST API using Fast API that can run in any python enabled environment. We have made prebuilt docker images available to run them without dealing with code built or deploy directly on Kubernetes. The above diagram represents our internal deployment using Kubernetes. We use envoy as Load Balancer and reverse proxy to forward requests to a specific instance. REST API request takes text and config as input and returns Audio bytes. Response Audio bytes encoded as base64. Bytes can directly be embedded in a web page using an HTML Audio tag. API reference \u00b6 Our API has predictable resource-oriented URLs, accepts form-encoded request bodies, returns JSON-encoded responses, and uses standard HTTP response codes, and verbs. Base URL http://<hostname.domain-name>/tts/v1/<language_code> Request Attributes Method: POST Headers: [{ \"key\": \"Content-Type\", \"value\": \"application/json\", \"description\": \"\", \"type\": \"text\", \"enabled\": true }] Body: Schema for TTS request and response defined at https://github.com/ULCA-IN/ulca/blob/master/specs/model-schema.yml Example Request Body - { \"input\": [ { \"source\": \"\u092d\u093e\u0930\u0924 \u092e\u0947\u0930\u093e \u0926\u0947\u0936 \u0939\u0948|\" } ], \"config\": { \"gender\": \"female\", \"language\": { \"sourceLanguage\": \"hi\" } } } The attributes input and config are the mandatory attributes for the request to process. The child attribute for input is source which should hold the text to synthesize. The child attributes for config are gender and sourceLanguage . Responses | Code | Description | |------|--------------------------------------| | 200 | On successful completion of the job. | Response Attributes Body: The child attribute \"audioContent\" of attribute \"audio\" would provide the audio bytes of the synthesized speech. The response also contains Audio Format and sampling as part of the response schema. Example: { \"audio\": [ { \"audioContent\": \"UklGRiS4AgBXQVZFZm10\" } ], \"config\": { \"language\": { \"sourceLanguage\": \"hi\" }, \"audioFormat\": \"wav\", \"encoding\": \"base64\", \"samplingRate\": 22050 } } Errors Our API uses HTTP response codes to indicate the success or failure of an API request. | 200 | OK | Everything worked as expected. | | 400 | Bad Request | The request was unacceptable, often due to missing a required parameter. | | 401 | Unauthorized | No valid API key provided. | | 402 | Request Failed | The parameters were valid but the request failed. | | 403 | Forbidden | The API key doesn't have permissions to perform the request. | | 404 | Not Found | The requested resource doesn't exist. | | 409 | Conflict | The request conflicts with another request (perhaps due to using the same idempotent key). | | 429 | Too Many Requests | Too many requests hit the API too quickly. We recommend an exponential backoff of your requests. | | 50X | Server Errors | Something went wrong on Stripe's end. (These are rare.) | Implementing the api from local using Docker Our API server can be hosted from any environment which supports Docker containers as we have pre-built images packaging the API and dependencies. Our latest container images can be pulled directly from gcr.io/ekstepspeechrecognition/text_to_speech_open_api:<version tag> Pre-requisites : Download all tts models in the local path using gsutil -m cp -r gs://vakyaansh-open-models/tts_models <local path>/tts_models/ Download all transliteration models into local using gsutil -m cp -r gs://vakyaansh-open-models/translit_models <local path>/translit_models/ Prepare the model_dict.json and place it in /tts_models/ The model_dict.json sample : { \"hi\" : { \"male_glow\" : \"hindi/male/glow_tts\", \"male_hifi\" : \"hindi/male/hifi_tts\", \"female_glow\" : \"hindi/female/glow_tts\", \"female_hifi\" : \"hindi/female/hifi_tts\" } } Sample docker run command: docker run -itd -p 5000:5000 --gpus all --env languages='[\"hi\",\"ml\"]' -v <local path>/tts_models/:/opt/text_to_speech_open_api/deployed_models/ -v <local path>/translit_models/:/opt/text_to_speech_open_api/vakyansh-tts/src/glow_tts/tts_infer/translit_models/ gcr.io/ekstepspeechrecognition/text_to_speech_open_api:2.1.4 Once the docker container is up and running, the api can be tested from localhost on port 5000. Contributing \u00b6 Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request License \u00b6 Distributed under the [MIT] License. See LICENSE for more information. Git repository \u00b6 https://github.com/Open-Speech-EkStep/text-to-speech-open-api Contact \u00b6 Connect with community on Gitter Project Link: https://github.com/Open-Speech-EkStep/text-to-speech-open-api","title":"Text To Speech Model API"},{"location":"tts_model_api/#text-to-speech-model-api","text":"","title":"Text to Speech model API"},{"location":"tts_model_api/#about-the-project","text":"Text To Speech (TTS), also known as Speech Synthesis, is a process where text is converted into a human-sounding voice. TTS has been a popular choice for developers and business users alike when building IVR (Interactive Voice Response) solutions and other voice applications, as it accelerates time to production without having to record audio files with human voices. Using recorded files requires recording each message with a human voice, whereas TTS prompts can be dynamically generated from raw text. Our TTS service can enable us to generate life-like speech synthesis in both male and female voices for an array of Indic languages like Hindi, Tamil,Malayalam, Kannada and many more. API enable us to provide the following features: Support for Indic only languages. No software Installation required. Cloud based robust API. Two gender voices for every language. Fast and easy integration. Cross-platform capability. The Developer documentation provides you with a complete set of guidelines which you need to get started with: Architecture overview API reference Client Code reference Setup and getting started guide Extend this project Contribute to this project","title":"About The Project"},{"location":"tts_model_api/#architecture-overview","text":"We built this REST API using Fast API that can run in any python enabled environment. We have made prebuilt docker images available to run them without dealing with code built or deploy directly on Kubernetes. The above diagram represents our internal deployment using Kubernetes. We use envoy as Load Balancer and reverse proxy to forward requests to a specific instance. REST API request takes text and config as input and returns Audio bytes. Response Audio bytes encoded as base64. Bytes can directly be embedded in a web page using an HTML Audio tag.","title":"Architecture Overview"},{"location":"tts_model_api/#api-reference","text":"Our API has predictable resource-oriented URLs, accepts form-encoded request bodies, returns JSON-encoded responses, and uses standard HTTP response codes, and verbs. Base URL http://<hostname.domain-name>/tts/v1/<language_code> Request Attributes Method: POST Headers: [{ \"key\": \"Content-Type\", \"value\": \"application/json\", \"description\": \"\", \"type\": \"text\", \"enabled\": true }] Body: Schema for TTS request and response defined at https://github.com/ULCA-IN/ulca/blob/master/specs/model-schema.yml Example Request Body - { \"input\": [ { \"source\": \"\u092d\u093e\u0930\u0924 \u092e\u0947\u0930\u093e \u0926\u0947\u0936 \u0939\u0948|\" } ], \"config\": { \"gender\": \"female\", \"language\": { \"sourceLanguage\": \"hi\" } } } The attributes input and config are the mandatory attributes for the request to process. The child attribute for input is source which should hold the text to synthesize. The child attributes for config are gender and sourceLanguage . Responses | Code | Description | |------|--------------------------------------| | 200 | On successful completion of the job. | Response Attributes Body: The child attribute \"audioContent\" of attribute \"audio\" would provide the audio bytes of the synthesized speech. The response also contains Audio Format and sampling as part of the response schema. Example: { \"audio\": [ { \"audioContent\": \"UklGRiS4AgBXQVZFZm10\" } ], \"config\": { \"language\": { \"sourceLanguage\": \"hi\" }, \"audioFormat\": \"wav\", \"encoding\": \"base64\", \"samplingRate\": 22050 } } Errors Our API uses HTTP response codes to indicate the success or failure of an API request. | 200 | OK | Everything worked as expected. | | 400 | Bad Request | The request was unacceptable, often due to missing a required parameter. | | 401 | Unauthorized | No valid API key provided. | | 402 | Request Failed | The parameters were valid but the request failed. | | 403 | Forbidden | The API key doesn't have permissions to perform the request. | | 404 | Not Found | The requested resource doesn't exist. | | 409 | Conflict | The request conflicts with another request (perhaps due to using the same idempotent key). | | 429 | Too Many Requests | Too many requests hit the API too quickly. We recommend an exponential backoff of your requests. | | 50X | Server Errors | Something went wrong on Stripe's end. (These are rare.) | Implementing the api from local using Docker Our API server can be hosted from any environment which supports Docker containers as we have pre-built images packaging the API and dependencies. Our latest container images can be pulled directly from gcr.io/ekstepspeechrecognition/text_to_speech_open_api:<version tag> Pre-requisites : Download all tts models in the local path using gsutil -m cp -r gs://vakyaansh-open-models/tts_models <local path>/tts_models/ Download all transliteration models into local using gsutil -m cp -r gs://vakyaansh-open-models/translit_models <local path>/translit_models/ Prepare the model_dict.json and place it in /tts_models/ The model_dict.json sample : { \"hi\" : { \"male_glow\" : \"hindi/male/glow_tts\", \"male_hifi\" : \"hindi/male/hifi_tts\", \"female_glow\" : \"hindi/female/glow_tts\", \"female_hifi\" : \"hindi/female/hifi_tts\" } } Sample docker run command: docker run -itd -p 5000:5000 --gpus all --env languages='[\"hi\",\"ml\"]' -v <local path>/tts_models/:/opt/text_to_speech_open_api/deployed_models/ -v <local path>/translit_models/:/opt/text_to_speech_open_api/vakyansh-tts/src/glow_tts/tts_infer/translit_models/ gcr.io/ekstepspeechrecognition/text_to_speech_open_api:2.1.4 Once the docker container is up and running, the api can be tested from localhost on port 5000.","title":"API reference"},{"location":"tts_model_api/#contributing","text":"Contributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are greatly appreciated . Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'Add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request","title":"Contributing"},{"location":"tts_model_api/#license","text":"Distributed under the [MIT] License. See LICENSE for more information.","title":"License"},{"location":"tts_model_api/#git-repository","text":"https://github.com/Open-Speech-EkStep/text-to-speech-open-api","title":"Git repository"},{"location":"tts_model_api/#contact","text":"Connect with community on Gitter Project Link: https://github.com/Open-Speech-EkStep/text-to-speech-open-api","title":"Contact"},{"location":"tts_model_training/","text":"vakyansh-tts \u00b6 Models \u00b6 Our open-sourced TTS models for Indic languages are present in this repo . Components \u00b6 There are two models at work that convert your text to an audio. First of all, we train a glow-TTS text-to-mel model to convert text to mel spectrogram. This mel spectrogram is then passed as input to a mel-to-wav model (HiFi-GAN) which converts it to an audio. Text to Mel: We use Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search proposed here . You can find the original source code implemented by the authors here . Mel to Wav: We use HiFi-GAN: a GAN-based model capable of generating high fidelity speech efficiently proposed here . You can find the original source code implemented by the authors here . Training logs \u00b6 Language : Hindi Data used for training : Monolingual Hindi male corpus (4.5 hrs) from IndicCorp . Sample Rate : 22050 Hz Glow-TTS trained for : 100 epochs Hifi-GAN trained for : 200k steps Tensorboard Logs \u00b6 Logs for Glow-TTS training Logs for Hifi GAN training 1. Installation and Setup for training \u00b6 Clone repo git clone https://github.com/Open-Speech-EkStep/vakyansh-tts Note: If you're interested in training a multi-speaker glow-tts for Text to Mel conversion, you can clone the multispeaker branch from here or: git clone https://github.com/Open-Speech-EkStep/vakyansh-tts -b multispeaker Build conda virtual environment cd ./vakyansh-tts conda create --name <env_name> python=3.7 conda activate <env_name> pip install -r requirements.txt Install apex ; commit: 37cdaf4 for Mixed-precision training Note: this is needed for glow-tts training cd .. git clone https://github.com/NVIDIA/apex cd apex git checkout 37cdaf4 pip install -v --disable-pip-version-check --no-cache-dir ./ cd ../vakyansh-tts Build Monotonic Alignment Search Code (Cython) bash install.sh 2. Data Resampling \u00b6 The data format should have a folder containing all the .wav files for glow-tts and a text file containing filenames with their sentences. Directory structure: langauge_folder_name language_folder_name |-- ./wav/*.wav |-- ./text_file_name.txt The format for text_file_name.txt (Text file is only needed for glow-tts training) ( audio1.wav \"Sentence1.\" ) ( audio2.wav \"Sentence2.\" ) To resample the .wav files to 22050 sample rate, change the following parameters in the vakyansh-tts/scripts/data/resample.sh input_wav_path : absolute path to wav file folder in vakyansh_tts/data/ output_wav_path : absolute path to vakyansh_tts/data/resampled_wav_folder_name output_sample_rate : 22050 (or any other desired sample rate) To run: cd scripts/data/ bash resample.sh 3. Spectogram Training (glow-tts) \u00b6 3.1 Data Preparation \u00b6 To prepare the data edit the vakyansh-tts/scripts/glow/prepare_data.sh file and change the following parameters input_text_path : absolute path to vakyansh_tts/data/text_file_name.txt input_wav_path : absolute path to vakyansh_tts/data/resampled_wav_folder_name gender : female or male voice To run: cd scripts/glow/ bash prepare_data.sh 3.2 Training glow-tts \u00b6 To start the spectogram-training edit the vakyansh-tts/scripts/glow/train_glow.sh file and change the following parameter: gender : female or male voice Make sure that the gender is same as that of the prepare_data.sh file To start the training, run: cd scripts/glow/ bash train_glow.sh 4. Vocoder Training (hifi-gan) \u00b6 4.1 Data Preparation \u00b6 To prepare the data edit the vakyansh-tts/scripts/hifi/prepare_data.sh file and change the following parameters input_wav_path : absolute path to vakyansh_tts/data/resampled_wav_folder_name gender : female or male voice To run: cd scripts/hifi/ bash prepare_data.sh 4.2 Training hifi-gan \u00b6 To start the spectogram-training edit the vakyansh-tts/scripts/hifi/train_hifi.sh file and change the following parameter: gender : female or male voice Make sure that the gender is same as that of the prepare_data.sh file To start the training, run: cd scripts/hifi/ bash train_hifi.sh 5. Inference \u00b6 5.1 Using Gradio \u00b6 To use the gradio link edit the following parameters in the vakyansh-tts/scripts/inference/gradio.sh file: gender : female or male voice device : cpu or cuda lang : langauge code To run: cd scripts/inference/ bash gradio.sh 5.2 Using fast API \u00b6 To use the fast api link edit the parameters in the vakyansh-tts/scripts/inference/api.sh file similar to section 5.1 To run: cd scripts/inference/ bash api.sh 5.3 Direct Inference using text \u00b6 To infer, edit the parameters in the vakyansh-tts/scripts/inference/infer.sh file similar to section 5.1 and set the text to the text variable To run: cd scripts/inference/ bash infer.sh To configure other parameters there is a version that runs the advanced inference as well. Additional Parameters: noise_scale : can vary from 0 to 1 for noise factor length_scale : can vary from 0 to 2 for changing the speed of the generated audio transliteration : whether to switch on/off transliteration. 1: ON, 0: OFF number_conversion : whether to switch on/off number to words conversion. 1: ON, 0: OFF split_sentences : whether to switch on/off splitting of sentences. 1: ON, 0: OFF To run: cd scripts/inference/ bash advanced_infer.sh 5.4 Installation of tts_infer package \u00b6 In tts_infer package, we currently have two components: 1. Transliteration (AI4bharat's open sourced models) (Languages supported: {'hi', 'gu', 'mr', 'bn', 'te', 'ta', 'kn', 'pa', 'gom', 'mai', 'ml', 'sd', 'si', 'ur'} ) 2. Num to Word (Languages supported: {'en', 'hi', 'gu', 'mr', 'bn', 'te', 'ta', 'kn', 'or', 'pa'} ) git clone https://github.com/Open-Speech-EkStep/vakyansh-tts cd vakyansh-tts bash install.sh python setup.py bdist_wheel pip install -e . cd tts_infer gsutil -m cp -r gs://vakyaansh-open-models/translit_models . Usage: Refer to example file in tts_infer/ from tts_infer.tts import TextToMel, MelToWav from tts_infer.transliterate import XlitEngine from tts_infer.num_to_word_on_sent import normalize_nums import re from scipy.io.wavfile import write text_to_mel = TextToMel(glow_model_dir='/path/to/glow-tts/checkpoint/dir', device='cuda') mel_to_wav = MelToWav(hifi_model_dir='/path/to/hifi/checkpoint/dir', device='cuda') def translit(text, lang): reg = re.compile(r'[a-zA-Z]') engine = XlitEngine(lang) words = [engine.translit_word(word, topk=1)[lang][0] if reg.match(word) else word for word in text.split()] updated_sent = ' '.join(words) return updated_sent def run_tts(text, lang): text = text.replace('\u0964', '.') # only for hindi models text_num_to_word = normalize_nums(text, lang) # converting numbers to words in lang text_num_to_word_and_transliterated = translit(text_num_to_word, lang) # transliterating english words to lang mel = text_to_mel.generate_mel(text_num_to_word_and_transliterated) audio, sr = mel_to_wav.generate_wav(mel) write(filename='temp.wav', rate=sr, data=audio) # for saving wav file, if needed return (sr, audio)","title":"Text To Speech Model training"},{"location":"tts_model_training/#vakyansh-tts","text":"","title":"vakyansh-tts"},{"location":"tts_model_training/#models","text":"Our open-sourced TTS models for Indic languages are present in this repo .","title":"Models"},{"location":"tts_model_training/#components","text":"There are two models at work that convert your text to an audio. First of all, we train a glow-TTS text-to-mel model to convert text to mel spectrogram. This mel spectrogram is then passed as input to a mel-to-wav model (HiFi-GAN) which converts it to an audio. Text to Mel: We use Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search proposed here . You can find the original source code implemented by the authors here . Mel to Wav: We use HiFi-GAN: a GAN-based model capable of generating high fidelity speech efficiently proposed here . You can find the original source code implemented by the authors here .","title":"Components"},{"location":"tts_model_training/#training-logs","text":"Language : Hindi Data used for training : Monolingual Hindi male corpus (4.5 hrs) from IndicCorp . Sample Rate : 22050 Hz Glow-TTS trained for : 100 epochs Hifi-GAN trained for : 200k steps","title":"Training logs"},{"location":"tts_model_training/#tensorboard-logs","text":"Logs for Glow-TTS training Logs for Hifi GAN training","title":"Tensorboard Logs"},{"location":"tts_model_training/#1-installation-and-setup-for-training","text":"Clone repo git clone https://github.com/Open-Speech-EkStep/vakyansh-tts Note: If you're interested in training a multi-speaker glow-tts for Text to Mel conversion, you can clone the multispeaker branch from here or: git clone https://github.com/Open-Speech-EkStep/vakyansh-tts -b multispeaker Build conda virtual environment cd ./vakyansh-tts conda create --name <env_name> python=3.7 conda activate <env_name> pip install -r requirements.txt Install apex ; commit: 37cdaf4 for Mixed-precision training Note: this is needed for glow-tts training cd .. git clone https://github.com/NVIDIA/apex cd apex git checkout 37cdaf4 pip install -v --disable-pip-version-check --no-cache-dir ./ cd ../vakyansh-tts Build Monotonic Alignment Search Code (Cython) bash install.sh","title":"1. Installation and Setup for training"},{"location":"tts_model_training/#2-data-resampling","text":"The data format should have a folder containing all the .wav files for glow-tts and a text file containing filenames with their sentences. Directory structure: langauge_folder_name language_folder_name |-- ./wav/*.wav |-- ./text_file_name.txt The format for text_file_name.txt (Text file is only needed for glow-tts training) ( audio1.wav \"Sentence1.\" ) ( audio2.wav \"Sentence2.\" ) To resample the .wav files to 22050 sample rate, change the following parameters in the vakyansh-tts/scripts/data/resample.sh input_wav_path : absolute path to wav file folder in vakyansh_tts/data/ output_wav_path : absolute path to vakyansh_tts/data/resampled_wav_folder_name output_sample_rate : 22050 (or any other desired sample rate) To run: cd scripts/data/ bash resample.sh","title":"2. Data Resampling"},{"location":"tts_model_training/#3-spectogram-training-glow-tts","text":"","title":"3. Spectogram Training (glow-tts)"},{"location":"tts_model_training/#31-data-preparation","text":"To prepare the data edit the vakyansh-tts/scripts/glow/prepare_data.sh file and change the following parameters input_text_path : absolute path to vakyansh_tts/data/text_file_name.txt input_wav_path : absolute path to vakyansh_tts/data/resampled_wav_folder_name gender : female or male voice To run: cd scripts/glow/ bash prepare_data.sh","title":"3.1 Data Preparation"},{"location":"tts_model_training/#32-training-glow-tts","text":"To start the spectogram-training edit the vakyansh-tts/scripts/glow/train_glow.sh file and change the following parameter: gender : female or male voice Make sure that the gender is same as that of the prepare_data.sh file To start the training, run: cd scripts/glow/ bash train_glow.sh","title":"3.2 Training glow-tts"},{"location":"tts_model_training/#4-vocoder-training-hifi-gan","text":"","title":"4. Vocoder Training (hifi-gan)"},{"location":"tts_model_training/#41-data-preparation","text":"To prepare the data edit the vakyansh-tts/scripts/hifi/prepare_data.sh file and change the following parameters input_wav_path : absolute path to vakyansh_tts/data/resampled_wav_folder_name gender : female or male voice To run: cd scripts/hifi/ bash prepare_data.sh","title":"4.1 Data Preparation"},{"location":"tts_model_training/#42-training-hifi-gan","text":"To start the spectogram-training edit the vakyansh-tts/scripts/hifi/train_hifi.sh file and change the following parameter: gender : female or male voice Make sure that the gender is same as that of the prepare_data.sh file To start the training, run: cd scripts/hifi/ bash train_hifi.sh","title":"4.2 Training hifi-gan"},{"location":"tts_model_training/#5-inference","text":"","title":"5. Inference"},{"location":"tts_model_training/#51-using-gradio","text":"To use the gradio link edit the following parameters in the vakyansh-tts/scripts/inference/gradio.sh file: gender : female or male voice device : cpu or cuda lang : langauge code To run: cd scripts/inference/ bash gradio.sh","title":"5.1 Using Gradio"},{"location":"tts_model_training/#52-using-fast-api","text":"To use the fast api link edit the parameters in the vakyansh-tts/scripts/inference/api.sh file similar to section 5.1 To run: cd scripts/inference/ bash api.sh","title":"5.2 Using fast API"},{"location":"tts_model_training/#53-direct-inference-using-text","text":"To infer, edit the parameters in the vakyansh-tts/scripts/inference/infer.sh file similar to section 5.1 and set the text to the text variable To run: cd scripts/inference/ bash infer.sh To configure other parameters there is a version that runs the advanced inference as well. Additional Parameters: noise_scale : can vary from 0 to 1 for noise factor length_scale : can vary from 0 to 2 for changing the speed of the generated audio transliteration : whether to switch on/off transliteration. 1: ON, 0: OFF number_conversion : whether to switch on/off number to words conversion. 1: ON, 0: OFF split_sentences : whether to switch on/off splitting of sentences. 1: ON, 0: OFF To run: cd scripts/inference/ bash advanced_infer.sh","title":"5.3 Direct Inference using text"},{"location":"tts_model_training/#54-installation-of-tts_infer-package","text":"In tts_infer package, we currently have two components: 1. Transliteration (AI4bharat's open sourced models) (Languages supported: {'hi', 'gu', 'mr', 'bn', 'te', 'ta', 'kn', 'pa', 'gom', 'mai', 'ml', 'sd', 'si', 'ur'} ) 2. Num to Word (Languages supported: {'en', 'hi', 'gu', 'mr', 'bn', 'te', 'ta', 'kn', 'or', 'pa'} ) git clone https://github.com/Open-Speech-EkStep/vakyansh-tts cd vakyansh-tts bash install.sh python setup.py bdist_wheel pip install -e . cd tts_infer gsutil -m cp -r gs://vakyaansh-open-models/translit_models . Usage: Refer to example file in tts_infer/ from tts_infer.tts import TextToMel, MelToWav from tts_infer.transliterate import XlitEngine from tts_infer.num_to_word_on_sent import normalize_nums import re from scipy.io.wavfile import write text_to_mel = TextToMel(glow_model_dir='/path/to/glow-tts/checkpoint/dir', device='cuda') mel_to_wav = MelToWav(hifi_model_dir='/path/to/hifi/checkpoint/dir', device='cuda') def translit(text, lang): reg = re.compile(r'[a-zA-Z]') engine = XlitEngine(lang) words = [engine.translit_word(word, topk=1)[lang][0] if reg.match(word) else word for word in text.split()] updated_sent = ' '.join(words) return updated_sent def run_tts(text, lang): text = text.replace('\u0964', '.') # only for hindi models text_num_to_word = normalize_nums(text, lang) # converting numbers to words in lang text_num_to_word_and_transliterated = translit(text_num_to_word, lang) # transliterating english words to lang mel = text_to_mel.generate_mel(text_num_to_word_and_transliterated) audio, sr = mel_to_wav.generate_wav(mel) write(filename='temp.wav', rate=sr, data=audio) # for saving wav file, if needed return (sr, audio)","title":"5.4 Installation of tts_infer package"}]}